{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Kubernetes Documentation Overview Kubernetes is an open-source system for automating the deployment, scaling, and management of containerized applications. It provides a platform for managing distributed systems, making it easier to build and run resilient applications. Kubernetes abstracts away the underlying infrastructure, allowing developers to focus on their applications rather than the complexities of managing servers and clusters. The primary technology stack for Kubernetes is Go (Golang), with build scripts and configurations potentially involving shell scripting, Makefiles, and Docker. The repository is organized into directories such as cmd/ for main executables, pkg/ for reusable library code, api/ for API definitions, and docs/ for documentation. Kubernetes features a control plane and worker nodes architecture. Key components include the kube-apiserver (API exposure), kube-controller-manager (core controllers), kube-scheduler (pod scheduling), kubelet (node agent), kube-proxy (network proxy), kubectl (CLI), and client-go (Go client library). It utilizes architectural patterns like control loops, an API-centric design, and a client-server model. Quick Start New to Kubernetes? Start here: API Server - Learn about the core API component. Kubelet - Understand the node agent. Kubectl - Get started with the command-line interface. Client-go - Explore the Go client library for programmatic interaction. Component Documentation Control Plane Components API Server - Exposes the Kubernetes API. Controller Manager - Runs core Kubernetes controllers. Scheduler - Selects nodes for Pods. Node Components Kubelet - The primary node agent that runs Pods on nodes. Kube-proxy - Implements the Kubernetes Service concept on each node. Command-Line Tools & Libraries Kubectl - The command-line interface for interacting with Kubernetes. Client-go - The official Go client library for Kubernetes. Documentation generated on Saturday, January 24, 2026","title":"Kubernetes Documentation"},{"location":"index.html#kubernetes-documentation","text":"","title":"Kubernetes Documentation"},{"location":"index.html#overview","text":"Kubernetes is an open-source system for automating the deployment, scaling, and management of containerized applications. It provides a platform for managing distributed systems, making it easier to build and run resilient applications. Kubernetes abstracts away the underlying infrastructure, allowing developers to focus on their applications rather than the complexities of managing servers and clusters. The primary technology stack for Kubernetes is Go (Golang), with build scripts and configurations potentially involving shell scripting, Makefiles, and Docker. The repository is organized into directories such as cmd/ for main executables, pkg/ for reusable library code, api/ for API definitions, and docs/ for documentation. Kubernetes features a control plane and worker nodes architecture. Key components include the kube-apiserver (API exposure), kube-controller-manager (core controllers), kube-scheduler (pod scheduling), kubelet (node agent), kube-proxy (network proxy), kubectl (CLI), and client-go (Go client library). It utilizes architectural patterns like control loops, an API-centric design, and a client-server model.","title":"Overview"},{"location":"index.html#quick-start","text":"New to Kubernetes? Start here: API Server - Learn about the core API component. Kubelet - Understand the node agent. Kubectl - Get started with the command-line interface. Client-go - Explore the Go client library for programmatic interaction.","title":"Quick Start"},{"location":"index.html#component-documentation","text":"","title":"Component Documentation"},{"location":"index.html#control-plane-components","text":"API Server - Exposes the Kubernetes API. Controller Manager - Runs core Kubernetes controllers. Scheduler - Selects nodes for Pods.","title":"Control Plane Components"},{"location":"index.html#node-components","text":"Kubelet - The primary node agent that runs Pods on nodes. Kube-proxy - Implements the Kubernetes Service concept on each node.","title":"Node Components"},{"location":"index.html#command-line-tools-libraries","text":"Kubectl - The command-line interface for interacting with Kubernetes. Client-go - The official Go client library for Kubernetes. Documentation generated on Saturday, January 24, 2026","title":"Command-Line Tools &amp; Libraries"},{"location":"overview.html","text":"Kubernetes Repository Overview This repository contains the source code for Kubernetes, an open-source system for automating deployment, scaling, and management of containerized applications. Technology Stack The primary technology stack is Go (Golang). Build scripts and configuration may involve shell scripting, Makefiles, and potentially Docker for containerization. Directory Structure cmd/ : Contains the main executable programs for Kubernetes components (e.g., kube-apiserver , kubelet , kubectl ). pkg/ : Holds reusable library code and internal packages used by the Kubernetes components. This is where most of the core logic resides. api/ : Likely contains API definitions and related code. build/ : Contains build scripts and related tooling. hack/ : Contains various helper scripts used for development, testing, and maintenance tasks. staging/ : May contain code that is intended to be moved to the main pkg/ directory or external libraries. test/ : Contains end-to-end tests and other testing utilities. vendor/ : Contains vendored dependencies managed by Go modules. docs/ : Documentation files. plugin/ : Likely related to plugin mechanisms within Kubernetes. cluster/ : Potentially contains code for setting up or managing Kubernetes clusters. Major Components The following are the major components identified: kube-apiserver : The Kubernetes API server, which exposes the Kubernetes API. kube-controller-manager : The controller manager, which runs core Kubernetes controllers. kube-scheduler : The scheduler, which selects a node for newly created Pods. kubelet : The primary Kubernetes node agent, responsible for running Pods on nodes. kube-proxy : A network proxy that runs on each node, implementing part of the Kubernetes Service concept. kubectl : The command-line interface for interacting with the Kubernetes API server. client-go : The official Go client library for Kubernetes. Architectural Patterns Kubernetes employs a distributed systems architecture with a control plane and worker nodes. Key patterns include: Control Loop : Controllers continuously watch and reconcile the state of the cluster. API-Centric Design : All interactions occur through the kube-apiserver . Client-Server Model : kubectl and other clients interact with the kube-apiserver . Agent-Based Architecture : kubelet acts as an agent on each worker node. Modularity : The separation into cmd/ and pkg/ promotes modularity.","title":"Overview"},{"location":"overview.html#kubernetes-repository-overview","text":"This repository contains the source code for Kubernetes, an open-source system for automating deployment, scaling, and management of containerized applications.","title":"Kubernetes Repository Overview"},{"location":"overview.html#technology-stack","text":"The primary technology stack is Go (Golang). Build scripts and configuration may involve shell scripting, Makefiles, and potentially Docker for containerization.","title":"Technology Stack"},{"location":"overview.html#directory-structure","text":"cmd/ : Contains the main executable programs for Kubernetes components (e.g., kube-apiserver , kubelet , kubectl ). pkg/ : Holds reusable library code and internal packages used by the Kubernetes components. This is where most of the core logic resides. api/ : Likely contains API definitions and related code. build/ : Contains build scripts and related tooling. hack/ : Contains various helper scripts used for development, testing, and maintenance tasks. staging/ : May contain code that is intended to be moved to the main pkg/ directory or external libraries. test/ : Contains end-to-end tests and other testing utilities. vendor/ : Contains vendored dependencies managed by Go modules. docs/ : Documentation files. plugin/ : Likely related to plugin mechanisms within Kubernetes. cluster/ : Potentially contains code for setting up or managing Kubernetes clusters.","title":"Directory Structure"},{"location":"overview.html#major-components","text":"The following are the major components identified: kube-apiserver : The Kubernetes API server, which exposes the Kubernetes API. kube-controller-manager : The controller manager, which runs core Kubernetes controllers. kube-scheduler : The scheduler, which selects a node for newly created Pods. kubelet : The primary Kubernetes node agent, responsible for running Pods on nodes. kube-proxy : A network proxy that runs on each node, implementing part of the Kubernetes Service concept. kubectl : The command-line interface for interacting with the Kubernetes API server. client-go : The official Go client library for Kubernetes.","title":"Major Components"},{"location":"overview.html#architectural-patterns","text":"Kubernetes employs a distributed systems architecture with a control plane and worker nodes. Key patterns include: Control Loop : Controllers continuously watch and reconcile the state of the cluster. API-Centric Design : All interactions occur through the kube-apiserver . Client-Server Model : kubectl and other clients interact with the kube-apiserver . Agent-Based Architecture : kubelet acts as an agent on each worker node. Modularity : The separation into cmd/ and pkg/ promotes modularity.","title":"Architectural Patterns"},{"location":"docs/index.html","text":"Component Documentation This directory contains detailed documentation for each major component. Components Client Go Files: api_reference, architecture, configuration Kube Apiserver Files: api_reference, architecture, configuration Kube Controller Manager The kube-controller-manager is a core component of Kubernetes that runs controllers. These controllers watch the state of the cluster and make chan... Files: api_reference, architecture, configuration, index Kube Proxy Kube-proxy is a crucial network proxy that runs on each node in a Kubernetes cluster. Its primary responsibility is to handle network rules on node... Files: api_reference, architecture, configuration, index Kube Scheduler The Kubernetes Scheduler ( kube-scheduler ) is a control plane component that watches for newly created Pods that have no Node assigned and selects... Files: api_reference, architecture, configuration, index Kubectl The Kubernetes command-line interface, kubectl, is the primary tool for interacting with a Kubernetes cluster. It allows users to perform a wide ra... Files: api_reference, architecture, configuration, index Kubelet Files: 7 components documented","title":"Component Documentation"},{"location":"docs/index.html#component-documentation","text":"This directory contains detailed documentation for each major component.","title":"Component Documentation"},{"location":"docs/index.html#components","text":"","title":"Components"},{"location":"docs/index.html#client-go","text":"Files: api_reference, architecture, configuration","title":"Client Go"},{"location":"docs/index.html#kube-apiserver","text":"Files: api_reference, architecture, configuration","title":"Kube Apiserver"},{"location":"docs/index.html#kube-controller-manager","text":"The kube-controller-manager is a core component of Kubernetes that runs controllers. These controllers watch the state of the cluster and make chan... Files: api_reference, architecture, configuration, index","title":"Kube Controller Manager"},{"location":"docs/index.html#kube-proxy","text":"Kube-proxy is a crucial network proxy that runs on each node in a Kubernetes cluster. Its primary responsibility is to handle network rules on node... Files: api_reference, architecture, configuration, index","title":"Kube Proxy"},{"location":"docs/index.html#kube-scheduler","text":"The Kubernetes Scheduler ( kube-scheduler ) is a control plane component that watches for newly created Pods that have no Node assigned and selects... Files: api_reference, architecture, configuration, index","title":"Kube Scheduler"},{"location":"docs/index.html#kubectl","text":"The Kubernetes command-line interface, kubectl, is the primary tool for interacting with a Kubernetes cluster. It allows users to perform a wide ra... Files: api_reference, architecture, configuration, index","title":"Kubectl"},{"location":"docs/index.html#kubelet","text":"Files: 7 components documented","title":"Kubelet"},{"location":"docs/client-go/api_reference.html","text":"Client-go API Reference The client-go library provides a robust API for interacting with the Kubernetes cluster. It offers both strongly-typed clients for specific API groups and versions, as well as a dynamic client for more flexible interactions. Core Concepts Clientset : The primary entry point for interacting with the Kubernetes API using strongly-typed clients. It groups clients by API version (e.g., v1 , apps/v1 ). API Groups : Kubernetes resources are organized into API groups (e.g., core, apps, batch, custom.metrics.k8s.io). client-go provides clients for each relevant group. Resource Clients : Within each API group and version, there are clients for specific resources (e.g., Pods, Deployments, Services). Dynamic Client : Allows you to interact with any Kubernetes resource, including Custom Resource Definitions (CRDs), without needing pre-generated client code. This is useful for generic tooling or when dealing with resources whose types are not known at compile time. Clientset Usage The kubernetes.Clientset provides access to all the core Kubernetes API groups. 1. Creating a Clientset You typically create a clientset after loading the Kubernetes configuration. package main import ( \"fmt\" \"path/filepath\" \"k8s.io/client-go/kubernetes\" \"k8s.io/client-go/tools/clientcmd\" \"k8s.io/client-go/rest\" \"os\" ) func main() { var config *rest.Config var err error // Check if running inside a cluster (in-cluster config) if _, err := rest.InClusterConfig(); err == nil { // Load kubeconfig from default location kubeconfigPath := filepath.Join(os.Getenv(\"HOME\"), \".kube\", \"config\") config, err = clientcmd.BuildConfigFromFlags(\"\", kubeconfigPath) if err != nil { panic(fmt.Errorf(\"error building kubeconfig: %v\", err)) } } else { // Use in-cluster config config, err = rest.InClusterConfig() if err != nil { panic(fmt.Errorf(\"error building in-cluster config: %v\", err)) } } // Create a Clientset clientset, err := kubernetes.NewForConfig(config) if err != nil { panic(fmt.Errorf(\"error creating clientset: %v\", err)) } fmt.Println(\"Successfully created Kubernetes clientset.\") // Now you can use clientset to interact with the API } 2. Interacting with CoreV1 API (Pods) The CoreV1() method on the clientset provides access to Pods, Services, Namespaces, etc. package main import ( \"context\" \"fmt\" \"k8s.io/client-go/kubernetes\" \"k8s.io/client-go/rest\" metav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\" ) func main() { // Assume config is already loaded and clientset is created // config, err := rest.InClusterConfig() ... // clientset, err := kubernetes.NewForConfig(config) ... // Placeholder for actual config and clientset creation var clientset kubernetes.Interface // This would be initialized properly // List all pods in a specific namespace namespace := \"default\" pods, err := clientset.CoreV1().Pods(namespace).List(context.TODO(), metav1.ListOptions{}) if err != nil { panic(fmt.Errorf(\"error listing pods: %v\", err)) } fmt.Printf(\"Pods in namespace '%s':\\n\", namespace) for _, pod := range pods.Items { fmt.Printf(\"- %s (Status: %s)\\n\", pod.Name, pod.Status.Phase) } // Get a specific pod podName := \"my-pod-name\" // Replace with an actual pod name pod, err := clientset.CoreV1().Pods(namespace).Get(context.TODO(), podName, metav1.GetOptions{}) if err != nil { panic(fmt.Errorf(\"error getting pod %s/%s: %v\", namespace, podName, err)) } fmt.Printf(\"\\nDetails for Pod '%s/%s':\\n\", pod.Namespace, pod.Name) fmt.Printf(\" - Image: %s\\n\", pod.Spec.Containers[0].Image) // Assumes at least one container } 3. Interacting with AppsV1 API (Deployments) The AppsV1() method provides access to Deployments, StatefulSets, DaemonSets, etc. package main import ( \"context\" \"fmt\" \"k8s.io/client-go/kubernetes\" \"k8s.io/client-go/rest\" metav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\" ) func main() { // Assume config is loaded and clientset is created // config, err := rest.InClusterConfig() ... // clientset, err := kubernetes.NewForConfig(config) ... // Placeholder for actual config and clientset creation var clientset kubernetes.Interface // This would be initialized properly namespace := \"default\" deploymentName := \"my-deployment\" // Replace with an actual deployment name // Get a specific deployment deployment, err := clientset.AppsV1().Deployments(namespace).Get(context.TODO(), deploymentName, metav1.GetOptions{}) if err != nil { panic(fmt.Errorf(\"error getting deployment %s/%s: %v\", namespace, deploymentName, err)) } fmt.Printf(\"Deployment '%s/%s':\\n\", deployment.Namespace, deployment.Name) fmt.Printf(\" - Replicas: %d desired, %d available\\n\", *deployment.Spec.Replicas, deployment.Status.AvailableReplicas) fmt.Printf(\" - Strategy: %s\\n\", deployment.Spec.Strategy.Type) // List all deployments in a namespace deployments, err := clientset.AppsV1().Deployments(namespace).List(context.TODO(), metav1.ListOptions{}) if err != nil { panic(fmt.Errorf(\"error listing deployments: %v\", err)) } fmt.Printf(\"\\nFound %d deployments in namespace '%s'\\n\", len(deployments.Items), namespace) } Dynamic Client Usage The dynamic client is useful when you need to interact with resources that are not known at compile time, such as Custom Resources (CRDs). 1. Creating a Dynamic Client package main import ( \"fmt\" \"k8s.io/client-go/dynamic\" \"k8s.io/client-go/rest\" ) func main() { // Assume config is loaded // config, err := rest.InClusterConfig() ... // Placeholder for actual config creation var config *rest.Config // This would be initialized properly // Create a dynamic client dynamicClient, err := dynamic.NewForConfig(config) if err != nil { panic(fmt.Errorf(\"error creating dynamic client: %v\", err)) } fmt.Println(\"Successfully created dynamic client.\") // You can now use dynamicClient to interact with any Kubernetes resource } 2. Accessing a Custom Resource (CRD) This example assumes you have a CRD named myresources.stable.example.com and an instance of it. package main import ( \"context\" \"fmt\" \"k8s.io/client-go/dynamic\" metav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\" \"k8s.io/apimachinery/pkg/runtime/schema\" ) func main() { // Assume dynamicClient is created // dynamicClient, err := dynamic.NewForConfig(config) ... // Placeholder for actual dynamic client creation var dynamicClient dynamic.Interface // This would be initialized properly // Define the Group, Version, and Resource gvr := schema.GroupVersionResource{ Group: \"stable.example.com\", Version: \"v1\", Resource: \"myresources\", // Plural name of the resource } namespace := \"default\" resourceName := \"my-custom-resource-instance\" // Name of your CR instance // Get the Unstructured object unstructuredObj, err := dynamicClient.Resource(gvr).Namespace(namespace).Get(context.TODO(), resourceName, metav1.GetOptions{}) if err != nil { panic(fmt.Errorf(\"error getting custom resource %s: %v\", resourceName, err)) } fmt.Printf(\"Successfully retrieved custom resource '%s/%s'\\n\", namespace, resourceName) fmt.Printf(\" - Name: %s\\n\", unstructuredObj.GetName()) fmt.Printf(\" - Kind: %s\\n\", unstructuredObj.GetKind()) // Access spec fields (example) // Note: This requires knowledge of the CRD's schema spec, ok := unstructuredObj.Object[\"spec\"].(map[string]interface{}) if ok { if value, found := spec[\"someField\"]; found { fmt.Printf(\" - Spec.someField: %v\\n\", value) } } } Common API Operations The following table outlines common operations available through the clientset and dynamic client. Operation Clientset Example (CoreV1 Pods) Dynamic Client Example (CRD) Description List clientset.CoreV1().Pods(ns).List(ctx, opts) dynamicClient.Resource(gvr).Namespace(ns).List(ctx, opts) Retrieves a list of resources. Get clientset.CoreV1().Pods(ns).Get(ctx, name, opts) dynamicClient.Resource(gvr).Namespace(ns).Get(ctx, name, opts) Retrieves a single resource by name. Create clientset.CoreV1().Pods(ns).Create(ctx, pod, opts) dynamicClient.Resource(gvr).Namespace(ns).Create(ctx, obj, opts) Creates a new resource. Update clientset.CoreV1().Pods(ns).Update(ctx, pod, opts) dynamicClient.Resource(gvr).Namespace(ns).Update(ctx, obj, opts) Updates an existing resource. Patch clientset.CoreV1().Pods(ns).Patch(ctx, name, pt, data, opts) dynamicClient.Resource(gvr).Namespace(ns).Patch(ctx, name, pt, data, opts) Partially updates an existing resource. Delete clientset.CoreV1().Pods(ns).Delete(ctx, name, opts) dynamicClient.Resource(gvr).Namespace(ns).Delete(ctx, name, opts) Deletes a resource by name. Watch clientset.CoreV1().Pods(ns).Watch(ctx, opts) dynamicClient.Resource(gvr).Namespace(ns).Watch(ctx, opts) Establishes a watch stream for resource changes. (Used by Informers) Note : ctx is context.Context , opts are metav1.ListOptions , metav1.GetOptions , etc., ns is the namespace, gvr is schema.GroupVersionResource , pt is types.PatchType . Further Information Kubernetes API Conventions client-go dynamic client example","title":"Client-go API Reference"},{"location":"docs/client-go/api_reference.html#client-go-api-reference","text":"The client-go library provides a robust API for interacting with the Kubernetes cluster. It offers both strongly-typed clients for specific API groups and versions, as well as a dynamic client for more flexible interactions.","title":"Client-go API Reference"},{"location":"docs/client-go/api_reference.html#core-concepts","text":"Clientset : The primary entry point for interacting with the Kubernetes API using strongly-typed clients. It groups clients by API version (e.g., v1 , apps/v1 ). API Groups : Kubernetes resources are organized into API groups (e.g., core, apps, batch, custom.metrics.k8s.io). client-go provides clients for each relevant group. Resource Clients : Within each API group and version, there are clients for specific resources (e.g., Pods, Deployments, Services). Dynamic Client : Allows you to interact with any Kubernetes resource, including Custom Resource Definitions (CRDs), without needing pre-generated client code. This is useful for generic tooling or when dealing with resources whose types are not known at compile time.","title":"Core Concepts"},{"location":"docs/client-go/api_reference.html#clientset-usage","text":"The kubernetes.Clientset provides access to all the core Kubernetes API groups.","title":"Clientset Usage"},{"location":"docs/client-go/api_reference.html#1-creating-a-clientset","text":"You typically create a clientset after loading the Kubernetes configuration. package main import ( \"fmt\" \"path/filepath\" \"k8s.io/client-go/kubernetes\" \"k8s.io/client-go/tools/clientcmd\" \"k8s.io/client-go/rest\" \"os\" ) func main() { var config *rest.Config var err error // Check if running inside a cluster (in-cluster config) if _, err := rest.InClusterConfig(); err == nil { // Load kubeconfig from default location kubeconfigPath := filepath.Join(os.Getenv(\"HOME\"), \".kube\", \"config\") config, err = clientcmd.BuildConfigFromFlags(\"\", kubeconfigPath) if err != nil { panic(fmt.Errorf(\"error building kubeconfig: %v\", err)) } } else { // Use in-cluster config config, err = rest.InClusterConfig() if err != nil { panic(fmt.Errorf(\"error building in-cluster config: %v\", err)) } } // Create a Clientset clientset, err := kubernetes.NewForConfig(config) if err != nil { panic(fmt.Errorf(\"error creating clientset: %v\", err)) } fmt.Println(\"Successfully created Kubernetes clientset.\") // Now you can use clientset to interact with the API }","title":"1. Creating a Clientset"},{"location":"docs/client-go/api_reference.html#2-interacting-with-corev1-api-pods","text":"The CoreV1() method on the clientset provides access to Pods, Services, Namespaces, etc. package main import ( \"context\" \"fmt\" \"k8s.io/client-go/kubernetes\" \"k8s.io/client-go/rest\" metav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\" ) func main() { // Assume config is already loaded and clientset is created // config, err := rest.InClusterConfig() ... // clientset, err := kubernetes.NewForConfig(config) ... // Placeholder for actual config and clientset creation var clientset kubernetes.Interface // This would be initialized properly // List all pods in a specific namespace namespace := \"default\" pods, err := clientset.CoreV1().Pods(namespace).List(context.TODO(), metav1.ListOptions{}) if err != nil { panic(fmt.Errorf(\"error listing pods: %v\", err)) } fmt.Printf(\"Pods in namespace '%s':\\n\", namespace) for _, pod := range pods.Items { fmt.Printf(\"- %s (Status: %s)\\n\", pod.Name, pod.Status.Phase) } // Get a specific pod podName := \"my-pod-name\" // Replace with an actual pod name pod, err := clientset.CoreV1().Pods(namespace).Get(context.TODO(), podName, metav1.GetOptions{}) if err != nil { panic(fmt.Errorf(\"error getting pod %s/%s: %v\", namespace, podName, err)) } fmt.Printf(\"\\nDetails for Pod '%s/%s':\\n\", pod.Namespace, pod.Name) fmt.Printf(\" - Image: %s\\n\", pod.Spec.Containers[0].Image) // Assumes at least one container }","title":"2. Interacting with CoreV1 API (Pods)"},{"location":"docs/client-go/api_reference.html#3-interacting-with-appsv1-api-deployments","text":"The AppsV1() method provides access to Deployments, StatefulSets, DaemonSets, etc. package main import ( \"context\" \"fmt\" \"k8s.io/client-go/kubernetes\" \"k8s.io/client-go/rest\" metav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\" ) func main() { // Assume config is loaded and clientset is created // config, err := rest.InClusterConfig() ... // clientset, err := kubernetes.NewForConfig(config) ... // Placeholder for actual config and clientset creation var clientset kubernetes.Interface // This would be initialized properly namespace := \"default\" deploymentName := \"my-deployment\" // Replace with an actual deployment name // Get a specific deployment deployment, err := clientset.AppsV1().Deployments(namespace).Get(context.TODO(), deploymentName, metav1.GetOptions{}) if err != nil { panic(fmt.Errorf(\"error getting deployment %s/%s: %v\", namespace, deploymentName, err)) } fmt.Printf(\"Deployment '%s/%s':\\n\", deployment.Namespace, deployment.Name) fmt.Printf(\" - Replicas: %d desired, %d available\\n\", *deployment.Spec.Replicas, deployment.Status.AvailableReplicas) fmt.Printf(\" - Strategy: %s\\n\", deployment.Spec.Strategy.Type) // List all deployments in a namespace deployments, err := clientset.AppsV1().Deployments(namespace).List(context.TODO(), metav1.ListOptions{}) if err != nil { panic(fmt.Errorf(\"error listing deployments: %v\", err)) } fmt.Printf(\"\\nFound %d deployments in namespace '%s'\\n\", len(deployments.Items), namespace) }","title":"3. Interacting with AppsV1 API (Deployments)"},{"location":"docs/client-go/api_reference.html#dynamic-client-usage","text":"The dynamic client is useful when you need to interact with resources that are not known at compile time, such as Custom Resources (CRDs).","title":"Dynamic Client Usage"},{"location":"docs/client-go/api_reference.html#1-creating-a-dynamic-client","text":"package main import ( \"fmt\" \"k8s.io/client-go/dynamic\" \"k8s.io/client-go/rest\" ) func main() { // Assume config is loaded // config, err := rest.InClusterConfig() ... // Placeholder for actual config creation var config *rest.Config // This would be initialized properly // Create a dynamic client dynamicClient, err := dynamic.NewForConfig(config) if err != nil { panic(fmt.Errorf(\"error creating dynamic client: %v\", err)) } fmt.Println(\"Successfully created dynamic client.\") // You can now use dynamicClient to interact with any Kubernetes resource }","title":"1. Creating a Dynamic Client"},{"location":"docs/client-go/api_reference.html#2-accessing-a-custom-resource-crd","text":"This example assumes you have a CRD named myresources.stable.example.com and an instance of it. package main import ( \"context\" \"fmt\" \"k8s.io/client-go/dynamic\" metav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\" \"k8s.io/apimachinery/pkg/runtime/schema\" ) func main() { // Assume dynamicClient is created // dynamicClient, err := dynamic.NewForConfig(config) ... // Placeholder for actual dynamic client creation var dynamicClient dynamic.Interface // This would be initialized properly // Define the Group, Version, and Resource gvr := schema.GroupVersionResource{ Group: \"stable.example.com\", Version: \"v1\", Resource: \"myresources\", // Plural name of the resource } namespace := \"default\" resourceName := \"my-custom-resource-instance\" // Name of your CR instance // Get the Unstructured object unstructuredObj, err := dynamicClient.Resource(gvr).Namespace(namespace).Get(context.TODO(), resourceName, metav1.GetOptions{}) if err != nil { panic(fmt.Errorf(\"error getting custom resource %s: %v\", resourceName, err)) } fmt.Printf(\"Successfully retrieved custom resource '%s/%s'\\n\", namespace, resourceName) fmt.Printf(\" - Name: %s\\n\", unstructuredObj.GetName()) fmt.Printf(\" - Kind: %s\\n\", unstructuredObj.GetKind()) // Access spec fields (example) // Note: This requires knowledge of the CRD's schema spec, ok := unstructuredObj.Object[\"spec\"].(map[string]interface{}) if ok { if value, found := spec[\"someField\"]; found { fmt.Printf(\" - Spec.someField: %v\\n\", value) } } }","title":"2. Accessing a Custom Resource (CRD)"},{"location":"docs/client-go/api_reference.html#common-api-operations","text":"The following table outlines common operations available through the clientset and dynamic client. Operation Clientset Example (CoreV1 Pods) Dynamic Client Example (CRD) Description List clientset.CoreV1().Pods(ns).List(ctx, opts) dynamicClient.Resource(gvr).Namespace(ns).List(ctx, opts) Retrieves a list of resources. Get clientset.CoreV1().Pods(ns).Get(ctx, name, opts) dynamicClient.Resource(gvr).Namespace(ns).Get(ctx, name, opts) Retrieves a single resource by name. Create clientset.CoreV1().Pods(ns).Create(ctx, pod, opts) dynamicClient.Resource(gvr).Namespace(ns).Create(ctx, obj, opts) Creates a new resource. Update clientset.CoreV1().Pods(ns).Update(ctx, pod, opts) dynamicClient.Resource(gvr).Namespace(ns).Update(ctx, obj, opts) Updates an existing resource. Patch clientset.CoreV1().Pods(ns).Patch(ctx, name, pt, data, opts) dynamicClient.Resource(gvr).Namespace(ns).Patch(ctx, name, pt, data, opts) Partially updates an existing resource. Delete clientset.CoreV1().Pods(ns).Delete(ctx, name, opts) dynamicClient.Resource(gvr).Namespace(ns).Delete(ctx, name, opts) Deletes a resource by name. Watch clientset.CoreV1().Pods(ns).Watch(ctx, opts) dynamicClient.Resource(gvr).Namespace(ns).Watch(ctx, opts) Establishes a watch stream for resource changes. (Used by Informers) Note : ctx is context.Context , opts are metav1.ListOptions , metav1.GetOptions , etc., ns is the namespace, gvr is schema.GroupVersionResource , pt is types.PatchType .","title":"Common API Operations"},{"location":"docs/client-go/api_reference.html#further-information","text":"Kubernetes API Conventions client-go dynamic client example","title":"Further Information"},{"location":"docs/client-go/architecture.html","text":"Client-go Architecture System Context The client-go library acts as a crucial bridge between applications (like controllers, operators, or custom dashboards) and the Kubernetes API server. It abstracts away the complexities of direct HTTP communication, object serialization/deserialization, and efficient resource watching. graph LR subgraph Kubernetes Cluster A[API Server] B(etcd) A <--> B end subgraph Application/Controller C[client-go Library] D[Your Application Logic] C --> D end D -- Uses --> C C -- Interacts with --> A Core Components and Data Flow The client-go library is composed of several interconnected components that work together to provide a seamless experience for interacting with the Kubernetes API. graph TD subgraph client-go Library A[Config] --> B(Clientset); B --> C{REST Client}; C -- HTTP Request/Response --> D[Kubernetes API Server]; B --> E(Informer Factory); E --> F(Informer); F -- Watch --> D; F -- Cache Update --> G(Shared Cache); F -- Event --> H(Event Handler); H --> I(Workqueue); I -- Process Item --> J[Application/Controller Logic]; G -- Read --> K(Lister); K -- Provides Object --> J; C -- For --> E; end D -- API Events --> F; J -- CRUD Operations --> B; style A fill:#f9f,stroke:#333,stroke-width:2px style G fill:#ccf,stroke:#333,stroke-width:2px style J fill:#cfc,stroke:#333,stroke-width:2px Component Breakdown: Config ( rest.Config ) : Holds all the configuration needed to connect to the Kubernetes API server (API server URL, authentication credentials, etc.). Typically loaded from a kubeconfig file or from in-cluster service account tokens. Source: staging/src/k8s.io/client-go/rest/config.go Clientset ( kubernetes.Clientset ) : The primary entry point for typed API interactions. Provides convenient access to different API groups (e.g., CoreV1 , AppsV1 ). Used for direct API calls (Create, Get, Update, Delete, List, Watch). Source: staging/src/k8s.io/client-go/kubernetes/clientset.go REST Client ( rest.RESTClient ) : The underlying client responsible for making raw HTTP requests to the Kubernetes API server. Handles serialization (to JSON/YAML) and deserialization. Manages authentication and transport layers. Source: staging/src/k8s.io/client-go/rest/client.go Informer Factory ( informers.SharedInformerFactory ) : Manages the lifecycle of Informers . Ensures that only one informer for a given resource type exists within a given scope (e.g., a namespace or cluster-wide). Optimizes resource usage by sharing RESTClient and cache synchronization. Source: staging/src/k8s.io/client-go/informers/factory.go Informer ( cache.SharedInformer ) : Watches a specific Kubernetes resource type (e.g., Pods, Deployments) via the Watch endpoint of the API server. Populates and maintains a local, in-memory cache ( SharedCache ). Triggers event handlers when resources are added, updated, or deleted. Sources: staging/src/k8s.io/client-go/informers/... Shared Cache ( cache.Store ) : An optimized, thread-safe in-memory store that holds the latest state of Kubernetes objects observed by an Informer. Provides efficient key-value storage and retrieval. Source: staging/src/k8s.io/client-go/tools/cache/store.go Event Handler ( cache.ResourceEventHandlerFuncs ) : Callback functions defined by the application that are invoked by the Informer when resource events occur. These handlers typically add keys to a Workqueue . Source: staging/src/k8s.io/client-go/tools/cache/shared_informer.go Workqueue ( util.WorkQueue ) : A thread-safe queue used by controllers to manage the processing of events. Ensures that each resource change is processed reliably and efficiently, handling retries and de-duplication. Source: staging/src/k8s.io/client-go/util/workqueue/type.go Lister ( listers.GenericLister ) : Provides convenient, read-only access to objects stored in the SharedCache . Allows applications to quickly retrieve objects by name or other criteria without hitting the API server directly. Sources: staging/src/k8s.io/client-go/listers/... Data Flow: Watching a Resource An application creates a Config and a Clientset . It instantiates a SharedInformerFactory . Using the factory, it gets an Informer for a specific resource (e.g., Pods). The Informer starts a Watch on the Kubernetes API Server . As the API Server sends events (Add, Update, Delete), the Informer updates its Shared Cache . For each event, the Informer calls the registered Event Handler . The Event Handler typically adds the resource's key to a Workqueue . A controller's processing loop continuously fetches keys from the Workqueue . For each key, the controller uses a Lister to get the latest object from the Shared Cache and performs the necessary reconciliation logic. Code Example: Informer and Workqueue Integration This example demonstrates how an informer's event handler adds items to a workqueue for processing. graph LR A[Informer Detected Change] --> B(Event Handler); B --> C{Add Key to Workqueue}; C --> D[Workqueue]; D --> E[Controller]; E -- Get Item --> D; E -- Use Lister --> F[Shared Cache]; E -- Process --> G[Reconciliation Logic]; package main import ( \"context\" \"fmt\" \"time\" v1 \"k8s.io/api/core/v1\" metav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\" \"k8s.io/client-go/informers\" \"k8s.io/client-go/kubernetes\" \"k8s.io/client-go/tools/cache\" \"k8s.io/client-go/util/workqueue\" \"k8s.io/client-go/rest\" ) func main() { config, err := rest.InClusterConfig() if err != nil { panic(err.Error()) } clientset, err := kubernetes.NewForConfig(config) if err != nil { panic(err.Error()) } informerFactory := informers.NewSharedInformerFactory(clientset, time.Minute*5) podInformer := informerFactory.Core().V1().Pods().Informer() podLister := informerFactory.Core().V1().Pods().Lister() // Lister for cache access // Workqueue for processing Pod events queue := workqueue.NewNamed(\"pod-processor\") podInformer.AddEventHandler(cache.ResourceEventHandlerFuncs{ AddFunc: func(obj interface{}) { pod := obj.(*v1.Pod) key, err := cache.MetaNamespaceKeyFunc(pod) if err != nil { fmt.Printf(\"Error getting key for pod %s/%s: %v\\n\", pod.Namespace, pod.Name, err) return } queue.Add(key) // Add pod key to the workqueue fmt.Printf(\"Added Pod %s/%s to workqueue\\n\", pod.Namespace, pod.Name) }, UpdateFunc: func(oldObj, newObj interface{}) { newPod := newObj.(*v1.Pod) key, err := cache.MetaNamespaceKeyFunc(newPod) if err != nil { fmt.Printf(\"Error getting key for pod %s/%s: %v\\n\", newPod.Namespace, newPod.Name, err) return } queue.Add(key) // Add pod key to the workqueue fmt.Printf(\"Added Pod %s/%s to workqueue for update\\n\", newPod.Namespace, newPod.Name) }, DeleteFunc: func(obj interface{}) { pod := obj.(*v1.Pod) key, err := cache.MetaNamespaceKeyFunc(pod) if err != nil { fmt.Printf(\"Error getting key for pod %s/%s: %v\\n\", pod.Namespace, pod.Name, err) return } queue.Add(key) // Add pod key to the workqueue fmt.Printf(\"Added Pod %s/%s to workqueue for deletion\\n\", pod.Namespace, pod.Name) }, }) // Controller loop simulation ctx, cancel := context.WithCancel(context.Background()) defer cancel() go func() { for { select { case <-ctx.Done(): return default: obj, shutdown := queue.Get() if shutdown { return } key := obj.(string) fmt.Printf(\"Processing key: %s\\n\", key) // Use Lister to get the object from cache // This is a simplified example; real controllers often parse the key // to get namespace and name for the Lister. // For simplicity, we'll assume key is \"namespace/name\" parts := strings.Split(key, \"/\") if len(parts) != 2 { fmt.Printf(\"Invalid key format: %s\\n\", key) queue.Forget(key) continue } namespace, name := parts[0], parts[1] pod, err := podLister.Pods(namespace).Get(name) if err != nil { fmt.Printf(\"Error getting pod %s from cache: %v\\n\", key, err) // If the pod is not found, it might have been deleted. // Decide whether to requeue or forget. queue.Forget(key) continue } // --- Your reconciliation logic here --- fmt.Printf(\"Reconciling Pod: %s/%s, Status: %v\\n\", pod.Namespace, pod.Name, pod.Status.Phase) // Example: Check if Pod needs specific labels, update if necessary via clientset.CoreV1().Pods(namespace).Update(...) queue.Done(key) // Mark item as processed queue.Forget(key) // Remove item from queue if processing was successful } } }() go informerFactory.Start(ctx.Done()) if !cache.WaitForCacheSync(ctx.Done(), podInformer.HasSynced) { panic(\"Failed to sync informer cache\") } fmt.Println(\"Client-go architecture example started. Watching Pods...\") time.Sleep(30 * time.Second) // Run for a duration cancel() fmt.Println(\"Shutting down.\") } // Helper function to split key (assuming format \"namespace/name\") func splitKey(key string) (string, string, error) { parts := strings.Split(key, \"/\") if len(parts) != 2 { return \"\", \"\", fmt.Errorf(\"invalid key format: %s\", key) } return parts[0], parts[1], nil } Key Interactions Clientset <-> REST Client : The Clientset uses the REST Client to perform CRUD operations on Kubernetes resources. Informer <-> API Server : The Informer establishes a Watch connection to the API Server to receive resource change events. Informer <-> Shared Cache : The Informer continuously updates the Shared Cache with the latest resource states. Event Handler -> Workqueue : Event Handlers enqueue resource keys into the Workqueue upon detecting changes. Workqueue -> Controller Logic : Controller logic dequeues keys from the Workqueue to trigger reconciliation. Lister -> Shared Cache : Listers provide efficient read access to the data within the Shared Cache. Controller Logic -> Clientset : The controller uses the Clientset to perform actions (e.g., create, update, delete resources) based on its desired state. This architecture emphasizes efficiency through caching and event-driven processing, making it suitable for managing the dynamic state of a Kubernetes cluster.","title":"Client-go Architecture"},{"location":"docs/client-go/architecture.html#client-go-architecture","text":"","title":"Client-go Architecture"},{"location":"docs/client-go/architecture.html#system-context","text":"The client-go library acts as a crucial bridge between applications (like controllers, operators, or custom dashboards) and the Kubernetes API server. It abstracts away the complexities of direct HTTP communication, object serialization/deserialization, and efficient resource watching. graph LR subgraph Kubernetes Cluster A[API Server] B(etcd) A <--> B end subgraph Application/Controller C[client-go Library] D[Your Application Logic] C --> D end D -- Uses --> C C -- Interacts with --> A","title":"System Context"},{"location":"docs/client-go/architecture.html#core-components-and-data-flow","text":"The client-go library is composed of several interconnected components that work together to provide a seamless experience for interacting with the Kubernetes API. graph TD subgraph client-go Library A[Config] --> B(Clientset); B --> C{REST Client}; C -- HTTP Request/Response --> D[Kubernetes API Server]; B --> E(Informer Factory); E --> F(Informer); F -- Watch --> D; F -- Cache Update --> G(Shared Cache); F -- Event --> H(Event Handler); H --> I(Workqueue); I -- Process Item --> J[Application/Controller Logic]; G -- Read --> K(Lister); K -- Provides Object --> J; C -- For --> E; end D -- API Events --> F; J -- CRUD Operations --> B; style A fill:#f9f,stroke:#333,stroke-width:2px style G fill:#ccf,stroke:#333,stroke-width:2px style J fill:#cfc,stroke:#333,stroke-width:2px","title":"Core Components and Data Flow"},{"location":"docs/client-go/architecture.html#component-breakdown","text":"Config ( rest.Config ) : Holds all the configuration needed to connect to the Kubernetes API server (API server URL, authentication credentials, etc.). Typically loaded from a kubeconfig file or from in-cluster service account tokens. Source: staging/src/k8s.io/client-go/rest/config.go Clientset ( kubernetes.Clientset ) : The primary entry point for typed API interactions. Provides convenient access to different API groups (e.g., CoreV1 , AppsV1 ). Used for direct API calls (Create, Get, Update, Delete, List, Watch). Source: staging/src/k8s.io/client-go/kubernetes/clientset.go REST Client ( rest.RESTClient ) : The underlying client responsible for making raw HTTP requests to the Kubernetes API server. Handles serialization (to JSON/YAML) and deserialization. Manages authentication and transport layers. Source: staging/src/k8s.io/client-go/rest/client.go Informer Factory ( informers.SharedInformerFactory ) : Manages the lifecycle of Informers . Ensures that only one informer for a given resource type exists within a given scope (e.g., a namespace or cluster-wide). Optimizes resource usage by sharing RESTClient and cache synchronization. Source: staging/src/k8s.io/client-go/informers/factory.go Informer ( cache.SharedInformer ) : Watches a specific Kubernetes resource type (e.g., Pods, Deployments) via the Watch endpoint of the API server. Populates and maintains a local, in-memory cache ( SharedCache ). Triggers event handlers when resources are added, updated, or deleted. Sources: staging/src/k8s.io/client-go/informers/... Shared Cache ( cache.Store ) : An optimized, thread-safe in-memory store that holds the latest state of Kubernetes objects observed by an Informer. Provides efficient key-value storage and retrieval. Source: staging/src/k8s.io/client-go/tools/cache/store.go Event Handler ( cache.ResourceEventHandlerFuncs ) : Callback functions defined by the application that are invoked by the Informer when resource events occur. These handlers typically add keys to a Workqueue . Source: staging/src/k8s.io/client-go/tools/cache/shared_informer.go Workqueue ( util.WorkQueue ) : A thread-safe queue used by controllers to manage the processing of events. Ensures that each resource change is processed reliably and efficiently, handling retries and de-duplication. Source: staging/src/k8s.io/client-go/util/workqueue/type.go Lister ( listers.GenericLister ) : Provides convenient, read-only access to objects stored in the SharedCache . Allows applications to quickly retrieve objects by name or other criteria without hitting the API server directly. Sources: staging/src/k8s.io/client-go/listers/...","title":"Component Breakdown:"},{"location":"docs/client-go/architecture.html#data-flow-watching-a-resource","text":"An application creates a Config and a Clientset . It instantiates a SharedInformerFactory . Using the factory, it gets an Informer for a specific resource (e.g., Pods). The Informer starts a Watch on the Kubernetes API Server . As the API Server sends events (Add, Update, Delete), the Informer updates its Shared Cache . For each event, the Informer calls the registered Event Handler . The Event Handler typically adds the resource's key to a Workqueue . A controller's processing loop continuously fetches keys from the Workqueue . For each key, the controller uses a Lister to get the latest object from the Shared Cache and performs the necessary reconciliation logic.","title":"Data Flow: Watching a Resource"},{"location":"docs/client-go/architecture.html#code-example-informer-and-workqueue-integration","text":"This example demonstrates how an informer's event handler adds items to a workqueue for processing. graph LR A[Informer Detected Change] --> B(Event Handler); B --> C{Add Key to Workqueue}; C --> D[Workqueue]; D --> E[Controller]; E -- Get Item --> D; E -- Use Lister --> F[Shared Cache]; E -- Process --> G[Reconciliation Logic]; package main import ( \"context\" \"fmt\" \"time\" v1 \"k8s.io/api/core/v1\" metav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\" \"k8s.io/client-go/informers\" \"k8s.io/client-go/kubernetes\" \"k8s.io/client-go/tools/cache\" \"k8s.io/client-go/util/workqueue\" \"k8s.io/client-go/rest\" ) func main() { config, err := rest.InClusterConfig() if err != nil { panic(err.Error()) } clientset, err := kubernetes.NewForConfig(config) if err != nil { panic(err.Error()) } informerFactory := informers.NewSharedInformerFactory(clientset, time.Minute*5) podInformer := informerFactory.Core().V1().Pods().Informer() podLister := informerFactory.Core().V1().Pods().Lister() // Lister for cache access // Workqueue for processing Pod events queue := workqueue.NewNamed(\"pod-processor\") podInformer.AddEventHandler(cache.ResourceEventHandlerFuncs{ AddFunc: func(obj interface{}) { pod := obj.(*v1.Pod) key, err := cache.MetaNamespaceKeyFunc(pod) if err != nil { fmt.Printf(\"Error getting key for pod %s/%s: %v\\n\", pod.Namespace, pod.Name, err) return } queue.Add(key) // Add pod key to the workqueue fmt.Printf(\"Added Pod %s/%s to workqueue\\n\", pod.Namespace, pod.Name) }, UpdateFunc: func(oldObj, newObj interface{}) { newPod := newObj.(*v1.Pod) key, err := cache.MetaNamespaceKeyFunc(newPod) if err != nil { fmt.Printf(\"Error getting key for pod %s/%s: %v\\n\", newPod.Namespace, newPod.Name, err) return } queue.Add(key) // Add pod key to the workqueue fmt.Printf(\"Added Pod %s/%s to workqueue for update\\n\", newPod.Namespace, newPod.Name) }, DeleteFunc: func(obj interface{}) { pod := obj.(*v1.Pod) key, err := cache.MetaNamespaceKeyFunc(pod) if err != nil { fmt.Printf(\"Error getting key for pod %s/%s: %v\\n\", pod.Namespace, pod.Name, err) return } queue.Add(key) // Add pod key to the workqueue fmt.Printf(\"Added Pod %s/%s to workqueue for deletion\\n\", pod.Namespace, pod.Name) }, }) // Controller loop simulation ctx, cancel := context.WithCancel(context.Background()) defer cancel() go func() { for { select { case <-ctx.Done(): return default: obj, shutdown := queue.Get() if shutdown { return } key := obj.(string) fmt.Printf(\"Processing key: %s\\n\", key) // Use Lister to get the object from cache // This is a simplified example; real controllers often parse the key // to get namespace and name for the Lister. // For simplicity, we'll assume key is \"namespace/name\" parts := strings.Split(key, \"/\") if len(parts) != 2 { fmt.Printf(\"Invalid key format: %s\\n\", key) queue.Forget(key) continue } namespace, name := parts[0], parts[1] pod, err := podLister.Pods(namespace).Get(name) if err != nil { fmt.Printf(\"Error getting pod %s from cache: %v\\n\", key, err) // If the pod is not found, it might have been deleted. // Decide whether to requeue or forget. queue.Forget(key) continue } // --- Your reconciliation logic here --- fmt.Printf(\"Reconciling Pod: %s/%s, Status: %v\\n\", pod.Namespace, pod.Name, pod.Status.Phase) // Example: Check if Pod needs specific labels, update if necessary via clientset.CoreV1().Pods(namespace).Update(...) queue.Done(key) // Mark item as processed queue.Forget(key) // Remove item from queue if processing was successful } } }() go informerFactory.Start(ctx.Done()) if !cache.WaitForCacheSync(ctx.Done(), podInformer.HasSynced) { panic(\"Failed to sync informer cache\") } fmt.Println(\"Client-go architecture example started. Watching Pods...\") time.Sleep(30 * time.Second) // Run for a duration cancel() fmt.Println(\"Shutting down.\") } // Helper function to split key (assuming format \"namespace/name\") func splitKey(key string) (string, string, error) { parts := strings.Split(key, \"/\") if len(parts) != 2 { return \"\", \"\", fmt.Errorf(\"invalid key format: %s\", key) } return parts[0], parts[1], nil }","title":"Code Example: Informer and Workqueue Integration"},{"location":"docs/client-go/architecture.html#key-interactions","text":"Clientset <-> REST Client : The Clientset uses the REST Client to perform CRUD operations on Kubernetes resources. Informer <-> API Server : The Informer establishes a Watch connection to the API Server to receive resource change events. Informer <-> Shared Cache : The Informer continuously updates the Shared Cache with the latest resource states. Event Handler -> Workqueue : Event Handlers enqueue resource keys into the Workqueue upon detecting changes. Workqueue -> Controller Logic : Controller logic dequeues keys from the Workqueue to trigger reconciliation. Lister -> Shared Cache : Listers provide efficient read access to the data within the Shared Cache. Controller Logic -> Clientset : The controller uses the Clientset to perform actions (e.g., create, update, delete resources) based on its desired state. This architecture emphasizes efficiency through caching and event-driven processing, making it suitable for managing the dynamic state of a Kubernetes cluster.","title":"Key Interactions"},{"location":"docs/client-go/configuration.html","text":"Client-go Configuration Configuration for client-go primarily revolves around establishing a connection to the Kubernetes API server. This involves specifying the API server's address, authentication credentials, and other transport-related settings. The primary mechanism for this is the rest.Config struct. rest.Config The rest.Config struct holds all the necessary information to connect to and authenticate with a Kubernetes cluster. // Example structure of rest.Config (simplified) type Config struct { Host string // The URL for the API Server Username string // Basic Auth Username Password string // Basic Auth Password TLSClientConfig TLSConfig // TLS client configuration BearerToken string // Token for Bearer Token authentication Impersonate ImpersonateConfig // Impersonation settings // ... other fields for timeouts, caching, transport, etc. } type TLSConfig struct { Insecure bool // If true, skip TLS certificate verification CertFile string // Path to the client certificate KeyFile string // Path to the client private key CAFile string // Path to the CA certificate bundle } Loading Configuration client-go provides helper functions to load configuration in different environments: 1. In-Cluster Configuration When your application runs inside a Kubernetes cluster (e.g., as a Pod), it can automatically use the service account token mounted into the Pod. package main import ( \"fmt\" \"k8s.io/client-go/rest\" ) func main() { config, err := rest.InClusterConfig() if err != nil { // Error likely means not running in-cluster panic(fmt.Errorf(\"failed to get in-cluster config: %v\", err)) } fmt.Printf(\"Connected to API Server: %s\\n\", config.Host) fmt.Printf(\"Using Bearer Token: %s...\\n\", config.BearerToken[:10]) // Print first 10 chars // Use this config to create a clientset } Typical In-Cluster Configuration: Host : The address of the Kubernetes API server (e.g., https://10.0.0.1:443 ). BearerToken : The service account token mounted at /var/run/secrets/kubernetes.io/serviceaccount/token . TLSClientConfig : Contains the CA certificate bundle mounted at /var/run/secrets/kubernetes.io/serviceaccount/ca.crt . Insecure is usually false. 2. Out-of-Cluster Configuration (Kubeconfig) When running outside the cluster (e.g., on your local machine), client-go typically loads configuration from a kubeconfig file ( ~/.kube/config by default). package main import ( \"fmt\" \"os\" \"path/filepath\" \"k8s.io/client-go/rest\" \"k8s.io/client-go/tools/clientcmd\" ) func main() { // Default kubeconfig path kubeconfigPath := filepath.Join(os.Getenv(\"HOME\"), \".kube\", \"config\") // If a specific kubeconfig path is provided via an environment variable, use it if kcPath := os.Getenv(\"KUBECONFIG\"); kcPath != \"\" { kubeconfigPath = kcPath } config, err := clientcmd.BuildConfigFromFlags(\"\", kubeconfigPath) if err != nil { panic(fmt.Errorf(\"failed to build kubeconfig: %v\", err)) } fmt.Printf(\"Connected to API Server: %s\\n\", config.Host) // Use this config to create a clientset } Kubeconfig File Structure (Simplified): A kubeconfig file typically defines clusters, users, and contexts. apiVersion: v1 kind: Config clusters: - name: my-cluster cluster: server: https://your-api-server:6443 certificate-authority-data: <base64-encoded-ca-cert> users: - name: my-user user: client-certificate-data: <base64-encoded-client-cert> client-key-data: <base64-encoded-client-key> contexts: - name: my-context context: cluster: my-cluster user: my-user current-context: my-context When clientcmd.BuildConfigFromFlags is used without arguments, it looks for the KUBECONFIG environment variable or defaults to ~/.kube/config . It then selects the context specified by current-context and uses the corresponding cluster and user information to build the rest.Config . Configuration Options and Fields The rest.Config struct offers numerous fields for fine-tuning the connection. Key ones include: Field Description Example Usage (In-Cluster/Kubeconfig) Host URL of the Kubernetes API server. https://10.0.0.1:443 / https://your-api-server:6443 Username Username for Basic Authentication. (Typically empty for in-cluster, set in kubeconfig user section) Password Password for Basic Authentication. (Typically empty for in-cluster, set in kubeconfig user section) TLSClientConfig Contains paths or data for client certificate, key, and CA. CertFile , KeyFile , CAFile (kubeconfig) or CertData , KeyData , CAData (in-cluster) Insecure If true , skips TLS verification. Use with extreme caution. false (default) BearerToken Token for Bearer Token authentication. (Service account token in-cluster) / (User token in kubeconfig) Timeout Default HTTP request timeout. 5 * time.Second QPS Queries Per Second limit. 20.0 Burst API request burst. 30 UserAgent User-Agent string for client requests. my-awesome-controller/v1.0.0 ContentConfig Settings for content negotiation (e.g., JSON vs Protobuf). ContentConfig{GroupVersion: &schema.GroupVersion{...}} RateLimiter Custom rate limiter implementation. (Typically uses default rate limiting based on QPS/Burst) WarningProcessor Handles API warnings. rest.NewirminghamWarningProcessor() Proxy Optional http.Proxy configuration. func(req *http.Request) (*url.URL, error) Transport Layer Configuration The rest.Config influences the underlying HTTP transport. You can further customize this using rest.TransportWrapper or by directly configuring http.Client settings if needed, although this is less common. The client-go library handles much of this automatically based on the rest.Config . // Example of setting QPS and Burst config.QPS = 50.0 config.Burst = 100 // Create clientset with the modified config clientset, err := kubernetes.NewForConfig(config) By correctly configuring rest.Config , you ensure your application can reliably connect to and interact with the Kubernetes API server, whether it's running inside or outside the cluster.","title":"Client-go Configuration"},{"location":"docs/client-go/configuration.html#client-go-configuration","text":"Configuration for client-go primarily revolves around establishing a connection to the Kubernetes API server. This involves specifying the API server's address, authentication credentials, and other transport-related settings. The primary mechanism for this is the rest.Config struct.","title":"Client-go Configuration"},{"location":"docs/client-go/configuration.html#restconfig","text":"The rest.Config struct holds all the necessary information to connect to and authenticate with a Kubernetes cluster. // Example structure of rest.Config (simplified) type Config struct { Host string // The URL for the API Server Username string // Basic Auth Username Password string // Basic Auth Password TLSClientConfig TLSConfig // TLS client configuration BearerToken string // Token for Bearer Token authentication Impersonate ImpersonateConfig // Impersonation settings // ... other fields for timeouts, caching, transport, etc. } type TLSConfig struct { Insecure bool // If true, skip TLS certificate verification CertFile string // Path to the client certificate KeyFile string // Path to the client private key CAFile string // Path to the CA certificate bundle }","title":"rest.Config"},{"location":"docs/client-go/configuration.html#loading-configuration","text":"client-go provides helper functions to load configuration in different environments:","title":"Loading Configuration"},{"location":"docs/client-go/configuration.html#1-in-cluster-configuration","text":"When your application runs inside a Kubernetes cluster (e.g., as a Pod), it can automatically use the service account token mounted into the Pod. package main import ( \"fmt\" \"k8s.io/client-go/rest\" ) func main() { config, err := rest.InClusterConfig() if err != nil { // Error likely means not running in-cluster panic(fmt.Errorf(\"failed to get in-cluster config: %v\", err)) } fmt.Printf(\"Connected to API Server: %s\\n\", config.Host) fmt.Printf(\"Using Bearer Token: %s...\\n\", config.BearerToken[:10]) // Print first 10 chars // Use this config to create a clientset } Typical In-Cluster Configuration: Host : The address of the Kubernetes API server (e.g., https://10.0.0.1:443 ). BearerToken : The service account token mounted at /var/run/secrets/kubernetes.io/serviceaccount/token . TLSClientConfig : Contains the CA certificate bundle mounted at /var/run/secrets/kubernetes.io/serviceaccount/ca.crt . Insecure is usually false.","title":"1. In-Cluster Configuration"},{"location":"docs/client-go/configuration.html#2-out-of-cluster-configuration-kubeconfig","text":"When running outside the cluster (e.g., on your local machine), client-go typically loads configuration from a kubeconfig file ( ~/.kube/config by default). package main import ( \"fmt\" \"os\" \"path/filepath\" \"k8s.io/client-go/rest\" \"k8s.io/client-go/tools/clientcmd\" ) func main() { // Default kubeconfig path kubeconfigPath := filepath.Join(os.Getenv(\"HOME\"), \".kube\", \"config\") // If a specific kubeconfig path is provided via an environment variable, use it if kcPath := os.Getenv(\"KUBECONFIG\"); kcPath != \"\" { kubeconfigPath = kcPath } config, err := clientcmd.BuildConfigFromFlags(\"\", kubeconfigPath) if err != nil { panic(fmt.Errorf(\"failed to build kubeconfig: %v\", err)) } fmt.Printf(\"Connected to API Server: %s\\n\", config.Host) // Use this config to create a clientset } Kubeconfig File Structure (Simplified): A kubeconfig file typically defines clusters, users, and contexts. apiVersion: v1 kind: Config clusters: - name: my-cluster cluster: server: https://your-api-server:6443 certificate-authority-data: <base64-encoded-ca-cert> users: - name: my-user user: client-certificate-data: <base64-encoded-client-cert> client-key-data: <base64-encoded-client-key> contexts: - name: my-context context: cluster: my-cluster user: my-user current-context: my-context When clientcmd.BuildConfigFromFlags is used without arguments, it looks for the KUBECONFIG environment variable or defaults to ~/.kube/config . It then selects the context specified by current-context and uses the corresponding cluster and user information to build the rest.Config .","title":"2. Out-of-Cluster Configuration (Kubeconfig)"},{"location":"docs/client-go/configuration.html#configuration-options-and-fields","text":"The rest.Config struct offers numerous fields for fine-tuning the connection. Key ones include: Field Description Example Usage (In-Cluster/Kubeconfig) Host URL of the Kubernetes API server. https://10.0.0.1:443 / https://your-api-server:6443 Username Username for Basic Authentication. (Typically empty for in-cluster, set in kubeconfig user section) Password Password for Basic Authentication. (Typically empty for in-cluster, set in kubeconfig user section) TLSClientConfig Contains paths or data for client certificate, key, and CA. CertFile , KeyFile , CAFile (kubeconfig) or CertData , KeyData , CAData (in-cluster) Insecure If true , skips TLS verification. Use with extreme caution. false (default) BearerToken Token for Bearer Token authentication. (Service account token in-cluster) / (User token in kubeconfig) Timeout Default HTTP request timeout. 5 * time.Second QPS Queries Per Second limit. 20.0 Burst API request burst. 30 UserAgent User-Agent string for client requests. my-awesome-controller/v1.0.0 ContentConfig Settings for content negotiation (e.g., JSON vs Protobuf). ContentConfig{GroupVersion: &schema.GroupVersion{...}} RateLimiter Custom rate limiter implementation. (Typically uses default rate limiting based on QPS/Burst) WarningProcessor Handles API warnings. rest.NewirminghamWarningProcessor() Proxy Optional http.Proxy configuration. func(req *http.Request) (*url.URL, error)","title":"Configuration Options and Fields"},{"location":"docs/client-go/configuration.html#transport-layer-configuration","text":"The rest.Config influences the underlying HTTP transport. You can further customize this using rest.TransportWrapper or by directly configuring http.Client settings if needed, although this is less common. The client-go library handles much of this automatically based on the rest.Config . // Example of setting QPS and Burst config.QPS = 50.0 config.Burst = 100 // Create clientset with the modified config clientset, err := kubernetes.NewForConfig(config) By correctly configuring rest.Config , you ensure your application can reliably connect to and interact with the Kubernetes API server, whether it's running inside or outside the cluster.","title":"Transport Layer Configuration"},{"location":"docs/kube-apiserver/api_reference.html","text":"Kubernetes API Server API Reference The Kubernetes API server exposes a comprehensive set of RESTful endpoints for managing cluster resources. This document outlines key aspects of the API, including how endpoints are structured, how requests are handled, and the mechanisms for authentication, authorization, and admission control. API Endpoints Structure API endpoints are typically versioned and grouped by API domain. The primary paths include: /api : For core Kubernetes resources (e.g., Pods, Services, Namespaces). Example: /api/v1/pods /apis : For non-core, extended, or custom resources. Example: /apis/apps/v1/deployments Example: /apis/apiextensions.k8s.io/v1/customresourcedefinitions /openapi/v2 or /openapi/v3 : Provides OpenAPI (Swagger) specifications for the API. /swagger.json : Legacy endpoint for Swagger specifications. /version : Returns information about the API server's version. /healthz : Health check endpoint. Request Handling Flow A typical API request follows this sequence: Request Reception : The genericapiserver receives the HTTP request. Authentication : The request's identity is established using configured authenticators (e.g., TLS cert, Bearer Token, Service Account). Authorization : Permissions are checked against the authenticated identity using configured authorizers (e.g., RBAC). Admission Control : The request passes through a chain of admission controllers for validation and mutation. API Handler Execution : The request is routed to the specific handler for the requested resource and action. Storage Interaction : The handler interacts with the storage layer (etcd) to perform the operation. Response Generation : The result is serialized (e.g., to JSON or Protobuf) and returned to the client. Authentication Mechanisms The API server supports multiple authentication methods, configured via flags: Flag Description Example Value Related Code --client-ca-file Path to the CA certificate bundle for verifying client TLS certificates. /etc/kubernetes/pki/ca.crt k8s.io/apiserver/pkg/authentication/authenticator/authenticator.go --requestheader-client-ca-file CA certificate bundle for verifying client certificates provided by a front proxy. /etc/kubernetes/pki/front-proxy-ca.crt k8s.io/apiserver/pkg/authentication/requestheader/requestheader.go --requestheader-allowed-names A list of client certificate common names that will be allowed to pass through the front proxy. [\"front-proxy-client\"] k8s.io/apiserver/pkg/authentication/requestheader/requestheader.go --service-account-key-file Path to the public key file used to verify Service Account tokens (older format). /etc/kubernetes/pki/sa.pub k8s.io/apiserver/pkg/authentication/serviceaccount/serviceaccount.go --service-account-signing-key-file Path to the private key file used to sign Service Account tokens (newer format, preferred). /etc/kubernetes/pki/sa.key k8s.io/apiserver/pkg/authentication/serviceaccount/serviceaccount.go --oidc-issuer-url The URL of the OpenID Connect identity provider. https://accounts.google.com k8s.io/apiserver/pkg/authentication/oidc/oidc.go --oidc-client-id The client ID for the API server when using OIDC authentication. kubernetes k8s.io/apiserver/pkg/authentication/oidc/oidc.go --kubeconfig Path to a kubeconfig file for configuring the client connecting to the API server. ~/.kube/config k8s.io/client-go/tools/clientcmd Service Account Token Authentication Example When a Service Account token is presented, the API server uses the configured public key ( --service-account-key-file or --service-account-signing-key-file ) to verify its signature and extract the service account identity. // Simplified example of Service Account authentication verification func VerifyServiceAccountToken(token string, keyFile string) (*authentication.TokenReviewStatus, error) { // Load public key from keyFile publicKey, err := ioutil.ReadFile(keyFile) // ... handle error ... // Parse token and verify signature using publicKey // ... // Construct TokenReviewStatus based on verification result // ... return status, nil } Authorization Modes Authorization determines if an authenticated user can perform an action. The --authorization-mode flag specifies the strategy. Mode Description Default / Common Use Related Code RBAC Role-Based Access Control. Authorizes based on Roles and ClusterRoles bound to users, groups, or service accounts. Primary mode k8s.io/kubernetes/plugin/pkg/auth/authorizer/rbac Node Allows Kubelets to access only the pods they are responsible for. Used with RBAC k8s.io/kubernetes/plugin/pkg/auth/authorizer/node Webhook Delegates authorization decisions to an external webhook server. Flexible integration k8s.io/apiserver/pkg/authorization/webhook/webhook.go ABAC Attribute-Based Access Control. Authorizes based on attributes in the request and policies defined in a file. Less common k8s.io/apiserver/pkg/authorization/abac/abac.go AlwaysAllow Allows all requests. Do not use in production. Testing only k8s.io/apiserver/pkg/authorization/always_allow/always_allow.go AlwaysDeny Denies all requests. Do not use in production. Testing only k8s.io/apiserver/pkg/authorization/always_deny/always_deny.go RBAC Authorization Example RBAC authorization involves checking Roles (namespaced) or ClusterRoles (cluster-wide) that grant permissions (verbs) on resources, which are then bound to subjects (users, groups, service accounts) via RoleBindings or ClusterRoleBindings. // Conceptual RBAC check for a Pod creation request func isPodCreationAllowed(user, namespace string, rbacAuthorizer Authorizer) (bool, error) { // Create Role-based access check request req := authorizer.NewAttributes( \"create\", // Verb \"pods\", // Resource namespace, // Namespace \"v1\", // API Version \"\", // API Group \"\", // Subresource \"\", // Name metav1.GroupVersionResource{Group: \"\", Version: \"v1\", Resource: \"pods\"}, ) // Perform the authorization check authorized, err := rbacAuthorizer.Authorize(context.TODO(), req) // ... handle error ... return authorized, nil } Admission Controllers Admission controllers intercept requests after authentication and authorization to enforce policies, modify objects, or perform custom validations. Controller Name Description Example Usage Related Code NamespaceLifecycle Manages the lifecycle of namespaces, ensuring resources are not created in deleted namespaces and handling finalizers. Automatically enabled. Ensures resources are correctly associated with their namespace's lifecycle. k8s.io/kubernetes/plugin/pkg/admission/namespacelifecycle LimitRanger Enforces resource limits (CPU, memory) defined in LimitRange objects on Pods and Container specs. Prevents pods from exceeding defined resource limits within a namespace. k8s.io/kubernetes/plugin/pkg/admission/limitranger ServiceAccount Automatically assigns a ServiceAccount to pods that don't specify one, using the default service account in the namespace. Ensures pods have a Service Account for API access. k8s.io/kubernetes/plugin/pkg/admission/serviceaccount ResourceQuota Enforces resource quotas on namespaces, limiting the total amount of compute resources, storage, or object counts. Prevents a namespace from consuming excessive cluster resources. k8s.io/kubernetes/plugin/pkg/admission/resourcequota PodSecurity Enforces Pod Security Standards (e.g., privileged , baseline , restricted ) on pod creation and updates. Ensures pods adhere to defined security contexts. Configurable via --admission-control-config-file . k8s.io/pod-security-admission (external library, integrated via plugin) MutatingAdmissionWebhook Allows external webhooks to intercept and modify (mutate) API objects before they are persisted. Used for automated adjustments like injecting sidecar containers, setting default labels, or modifying resource specs. k8s.io/api/admission/v1 , k8s.io/apiserver/pkg/admission/plugin/mutatingwebhook ValidatingAdmissionWebhook Allows external webhooks to intercept and validate API objects without modifying them. Used for custom validation logic, enforcing organizational policies, or ensuring resource configurations meet specific criteria. k8s.io/api/admission/v1 , k8s.io/apiserver/pkg/admission/plugin/validatingwebhook Mutating and Validating Webhook Example Webhooks are configured via MutatingWebhookConfiguration and ValidatingWebhookConfiguration resources, and their behavior is controlled by the --admission-control-config-file flag pointing to an AdmissionConfiguration resource. // Example: Mutating a Pod spec via a webhook // Client sends a Pod creation request // API Server authenticates and authorizes the request. // API Server sends a MutatingAdmissionReview request to the configured webhook. // Webhook returns a MutatingAdmissionResponse, potentially modifying the Pod spec (e.g., injecting a volume). // API Server applies the mutation and proceeds to the next admission plugin or storage. // Conceptual structure of a MutatingAdmissionReview request sent to a webhook: type MutatingAdmissionReview struct { metav1.TypeMeta `json:\",inline\"` metav1.ObjectMeta `json:\"metadata,omitempty\"` Request *AdmissionRequest `json:\"request,omitempty\"` Response *AdmissionResponse `json:\"response,omitempty\"` } type AdmissionRequest struct { UID types.UID Kind metav1.GroupVersionKind Resource metav1.GroupVersionResource SubResource string Namespace string Operation Operation UserInfo UserInfo Object runtime.RawExtension // The object being admitted // ... other fields } API Reference Tables Detailed API endpoint lists are extensive and dynamically generated. Key resources managed include: Core API Group ( /api/v1 ): Pods, Nodes, Services, Namespaces, Deployments, ReplicaSets, StatefulSets, etc. Apps API Group ( /apis/apps/v1 ): Deployments, StatefulSets, DaemonSets, ReplicaSets. Batch API Group ( /apis/batch/v1 ): Jobs, CronJobs. Networking API Group ( /apis/networking.k8s.io/v1 ): Ingress, NetworkPolicy. Storage API Group ( /apis/storage.k8s.io/v1 ): StorageClass, CSIDriver. Custom Resources: Defined via CustomResourceDefinition API ( /apis/apiextensions.k8s.io/v1 ). Refer to the official Kubernetes API documentation for a complete and up-to-date list of all available API endpoints and their schemas.","title":"Kubernetes API Server API Reference"},{"location":"docs/kube-apiserver/api_reference.html#kubernetes-api-server-api-reference","text":"The Kubernetes API server exposes a comprehensive set of RESTful endpoints for managing cluster resources. This document outlines key aspects of the API, including how endpoints are structured, how requests are handled, and the mechanisms for authentication, authorization, and admission control.","title":"Kubernetes API Server API Reference"},{"location":"docs/kube-apiserver/api_reference.html#api-endpoints-structure","text":"API endpoints are typically versioned and grouped by API domain. The primary paths include: /api : For core Kubernetes resources (e.g., Pods, Services, Namespaces). Example: /api/v1/pods /apis : For non-core, extended, or custom resources. Example: /apis/apps/v1/deployments Example: /apis/apiextensions.k8s.io/v1/customresourcedefinitions /openapi/v2 or /openapi/v3 : Provides OpenAPI (Swagger) specifications for the API. /swagger.json : Legacy endpoint for Swagger specifications. /version : Returns information about the API server's version. /healthz : Health check endpoint.","title":"API Endpoints Structure"},{"location":"docs/kube-apiserver/api_reference.html#request-handling-flow","text":"A typical API request follows this sequence: Request Reception : The genericapiserver receives the HTTP request. Authentication : The request's identity is established using configured authenticators (e.g., TLS cert, Bearer Token, Service Account). Authorization : Permissions are checked against the authenticated identity using configured authorizers (e.g., RBAC). Admission Control : The request passes through a chain of admission controllers for validation and mutation. API Handler Execution : The request is routed to the specific handler for the requested resource and action. Storage Interaction : The handler interacts with the storage layer (etcd) to perform the operation. Response Generation : The result is serialized (e.g., to JSON or Protobuf) and returned to the client.","title":"Request Handling Flow"},{"location":"docs/kube-apiserver/api_reference.html#authentication-mechanisms","text":"The API server supports multiple authentication methods, configured via flags: Flag Description Example Value Related Code --client-ca-file Path to the CA certificate bundle for verifying client TLS certificates. /etc/kubernetes/pki/ca.crt k8s.io/apiserver/pkg/authentication/authenticator/authenticator.go --requestheader-client-ca-file CA certificate bundle for verifying client certificates provided by a front proxy. /etc/kubernetes/pki/front-proxy-ca.crt k8s.io/apiserver/pkg/authentication/requestheader/requestheader.go --requestheader-allowed-names A list of client certificate common names that will be allowed to pass through the front proxy. [\"front-proxy-client\"] k8s.io/apiserver/pkg/authentication/requestheader/requestheader.go --service-account-key-file Path to the public key file used to verify Service Account tokens (older format). /etc/kubernetes/pki/sa.pub k8s.io/apiserver/pkg/authentication/serviceaccount/serviceaccount.go --service-account-signing-key-file Path to the private key file used to sign Service Account tokens (newer format, preferred). /etc/kubernetes/pki/sa.key k8s.io/apiserver/pkg/authentication/serviceaccount/serviceaccount.go --oidc-issuer-url The URL of the OpenID Connect identity provider. https://accounts.google.com k8s.io/apiserver/pkg/authentication/oidc/oidc.go --oidc-client-id The client ID for the API server when using OIDC authentication. kubernetes k8s.io/apiserver/pkg/authentication/oidc/oidc.go --kubeconfig Path to a kubeconfig file for configuring the client connecting to the API server. ~/.kube/config k8s.io/client-go/tools/clientcmd","title":"Authentication Mechanisms"},{"location":"docs/kube-apiserver/api_reference.html#service-account-token-authentication-example","text":"When a Service Account token is presented, the API server uses the configured public key ( --service-account-key-file or --service-account-signing-key-file ) to verify its signature and extract the service account identity. // Simplified example of Service Account authentication verification func VerifyServiceAccountToken(token string, keyFile string) (*authentication.TokenReviewStatus, error) { // Load public key from keyFile publicKey, err := ioutil.ReadFile(keyFile) // ... handle error ... // Parse token and verify signature using publicKey // ... // Construct TokenReviewStatus based on verification result // ... return status, nil }","title":"Service Account Token Authentication Example"},{"location":"docs/kube-apiserver/api_reference.html#authorization-modes","text":"Authorization determines if an authenticated user can perform an action. The --authorization-mode flag specifies the strategy. Mode Description Default / Common Use Related Code RBAC Role-Based Access Control. Authorizes based on Roles and ClusterRoles bound to users, groups, or service accounts. Primary mode k8s.io/kubernetes/plugin/pkg/auth/authorizer/rbac Node Allows Kubelets to access only the pods they are responsible for. Used with RBAC k8s.io/kubernetes/plugin/pkg/auth/authorizer/node Webhook Delegates authorization decisions to an external webhook server. Flexible integration k8s.io/apiserver/pkg/authorization/webhook/webhook.go ABAC Attribute-Based Access Control. Authorizes based on attributes in the request and policies defined in a file. Less common k8s.io/apiserver/pkg/authorization/abac/abac.go AlwaysAllow Allows all requests. Do not use in production. Testing only k8s.io/apiserver/pkg/authorization/always_allow/always_allow.go AlwaysDeny Denies all requests. Do not use in production. Testing only k8s.io/apiserver/pkg/authorization/always_deny/always_deny.go","title":"Authorization Modes"},{"location":"docs/kube-apiserver/api_reference.html#rbac-authorization-example","text":"RBAC authorization involves checking Roles (namespaced) or ClusterRoles (cluster-wide) that grant permissions (verbs) on resources, which are then bound to subjects (users, groups, service accounts) via RoleBindings or ClusterRoleBindings. // Conceptual RBAC check for a Pod creation request func isPodCreationAllowed(user, namespace string, rbacAuthorizer Authorizer) (bool, error) { // Create Role-based access check request req := authorizer.NewAttributes( \"create\", // Verb \"pods\", // Resource namespace, // Namespace \"v1\", // API Version \"\", // API Group \"\", // Subresource \"\", // Name metav1.GroupVersionResource{Group: \"\", Version: \"v1\", Resource: \"pods\"}, ) // Perform the authorization check authorized, err := rbacAuthorizer.Authorize(context.TODO(), req) // ... handle error ... return authorized, nil }","title":"RBAC Authorization Example"},{"location":"docs/kube-apiserver/api_reference.html#admission-controllers","text":"Admission controllers intercept requests after authentication and authorization to enforce policies, modify objects, or perform custom validations. Controller Name Description Example Usage Related Code NamespaceLifecycle Manages the lifecycle of namespaces, ensuring resources are not created in deleted namespaces and handling finalizers. Automatically enabled. Ensures resources are correctly associated with their namespace's lifecycle. k8s.io/kubernetes/plugin/pkg/admission/namespacelifecycle LimitRanger Enforces resource limits (CPU, memory) defined in LimitRange objects on Pods and Container specs. Prevents pods from exceeding defined resource limits within a namespace. k8s.io/kubernetes/plugin/pkg/admission/limitranger ServiceAccount Automatically assigns a ServiceAccount to pods that don't specify one, using the default service account in the namespace. Ensures pods have a Service Account for API access. k8s.io/kubernetes/plugin/pkg/admission/serviceaccount ResourceQuota Enforces resource quotas on namespaces, limiting the total amount of compute resources, storage, or object counts. Prevents a namespace from consuming excessive cluster resources. k8s.io/kubernetes/plugin/pkg/admission/resourcequota PodSecurity Enforces Pod Security Standards (e.g., privileged , baseline , restricted ) on pod creation and updates. Ensures pods adhere to defined security contexts. Configurable via --admission-control-config-file . k8s.io/pod-security-admission (external library, integrated via plugin) MutatingAdmissionWebhook Allows external webhooks to intercept and modify (mutate) API objects before they are persisted. Used for automated adjustments like injecting sidecar containers, setting default labels, or modifying resource specs. k8s.io/api/admission/v1 , k8s.io/apiserver/pkg/admission/plugin/mutatingwebhook ValidatingAdmissionWebhook Allows external webhooks to intercept and validate API objects without modifying them. Used for custom validation logic, enforcing organizational policies, or ensuring resource configurations meet specific criteria. k8s.io/api/admission/v1 , k8s.io/apiserver/pkg/admission/plugin/validatingwebhook","title":"Admission Controllers"},{"location":"docs/kube-apiserver/api_reference.html#mutating-and-validating-webhook-example","text":"Webhooks are configured via MutatingWebhookConfiguration and ValidatingWebhookConfiguration resources, and their behavior is controlled by the --admission-control-config-file flag pointing to an AdmissionConfiguration resource. // Example: Mutating a Pod spec via a webhook // Client sends a Pod creation request // API Server authenticates and authorizes the request. // API Server sends a MutatingAdmissionReview request to the configured webhook. // Webhook returns a MutatingAdmissionResponse, potentially modifying the Pod spec (e.g., injecting a volume). // API Server applies the mutation and proceeds to the next admission plugin or storage. // Conceptual structure of a MutatingAdmissionReview request sent to a webhook: type MutatingAdmissionReview struct { metav1.TypeMeta `json:\",inline\"` metav1.ObjectMeta `json:\"metadata,omitempty\"` Request *AdmissionRequest `json:\"request,omitempty\"` Response *AdmissionResponse `json:\"response,omitempty\"` } type AdmissionRequest struct { UID types.UID Kind metav1.GroupVersionKind Resource metav1.GroupVersionResource SubResource string Namespace string Operation Operation UserInfo UserInfo Object runtime.RawExtension // The object being admitted // ... other fields }","title":"Mutating and Validating Webhook Example"},{"location":"docs/kube-apiserver/api_reference.html#api-reference-tables","text":"Detailed API endpoint lists are extensive and dynamically generated. Key resources managed include: Core API Group ( /api/v1 ): Pods, Nodes, Services, Namespaces, Deployments, ReplicaSets, StatefulSets, etc. Apps API Group ( /apis/apps/v1 ): Deployments, StatefulSets, DaemonSets, ReplicaSets. Batch API Group ( /apis/batch/v1 ): Jobs, CronJobs. Networking API Group ( /apis/networking.k8s.io/v1 ): Ingress, NetworkPolicy. Storage API Group ( /apis/storage.k8s.io/v1 ): StorageClass, CSIDriver. Custom Resources: Defined via CustomResourceDefinition API ( /apis/apiextensions.k8s.io/v1 ). Refer to the official Kubernetes API documentation for a complete and up-to-date list of all available API endpoints and their schemas.","title":"API Reference Tables"},{"location":"docs/kube-apiserver/architecture.html","text":"Kubernetes API Server Architecture The Kubernetes API server ( kube-apiserver ) is a complex component that integrates multiple functionalities provided by various Kubernetes libraries. It acts as the central communication point for the entire cluster. System Context Diagram This diagram illustrates the kube-apiserver 's position within the Kubernetes control plane and its interactions with other components. graph TD subgraph Kubernetes Control Plane KAS[kube-apiserver] SCH[kube-scheduler] CCM[cloud-controller-manager] CTLM[controller-manager] end subgraph Cluster Components KUBELET[kubelet] KMP[kube-proxy] end subgraph External Clients KUBECTL[kubectl] USERS[Users/Admins] APPLICATIONS[Applications] end subgraph Storage ETCD[(etcd)] end subgraph Other API Servers CRD_API[Custom API Server (e.g., metrics, logging)] end KAS -- Watches/Updates --> SCH KAS -- Watches/Updates --> CTLM KAS -- Watches/Updates --> CCM KAS -- API Calls --> KUBELET KAS -- Routes traffic --> KMP KUBECTL -- API Calls --> KAS USERS -- API Calls --> KAS APPLICATIONS -- API Calls --> KAS KAS -- CRUD Operations --> ETCD KAS -- Delegates API Calls --> CRD_API CRD_API -- API Calls --> KAS KAS -- Watches --> KUBECTL KAS -- Watches --> CTLM KAS -- Watches --> CCM KAS -- Watches --> KMP %% Style adjustments classDef controlplane fill:#c9daf8,stroke:#333,stroke-width:2px; class KAS,SCH,CCM,CTLM controlplane; classDef component fill:#cfe2f3,stroke:#333,stroke-width:2px; class KUBELET,KMP component; classDef external fill:#fce5cd,stroke:#333,stroke-width:2px; class KUBECTL,USERS,APPLICATIONS external; classDef storage fill:#d9ead3,stroke:#333,stroke-width:2px; class ETCD storage; classDef customapi fill:#ead1dc,stroke:#333,stroke-width:2px; class CRD_API customapi; Internal Architecture and Request Flow The kube-apiserver is built upon several core libraries and layers: cmd/kube-apiserver/ : The entry point that sets up the command, parses flags ( options.go ), and orchestrates the server creation. k8s.io/apiserver : Provides the generic framework for API servers. It handles: HTTP request routing and multiplexing. TLS configuration and listener management. Standard API endpoints (healthz, version, discovery). Basic authentication and authorization middleware. Admission controller chaining. REST storage abstraction. k8s.io/kubernetes/pkg/controlplane : Implements the specifics of the Kubernetes API, including: Defining Kubernetes resource schemas. Configuring REST storage for each resource type (mapping to etcd via k8s.io/apiserver/pkg/storage ). Initializing API enablement and feature gates. Setting up specific admission controllers and authentication/authorization strategies. k8s.io/kube-aggregator : Responsible for API aggregation. It allows the kube-apiserver to discover and route requests to other registered API servers (e.g., metrics-server, custom API servers). It implements a ServiceResolver to find these aggregated APIs. Request Flow: A typical API request (e.g., GET /api/v1/pods ) follows this path: HTTP Request : Received by the generic API server's listener. Authentication : The request identity is determined (e.g., via TLS client cert, Bearer Token, Service Account Token). Authorization : The authenticated identity is checked against authorization rules (e.g., RBAC). Admission Control : The request is passed through a chain of admission controllers. These can validate the request, mutate the object, or reject it. API Handler : The request is routed to the appropriate handler based on the API group, version, and resource. Storage Operation : The handler interacts with the storage layer to perform the requested operation (e.g., GET , POST , PUT , DELETE ) on the underlying data store (etcd). Response : The result is serialized and sent back to the client. graph LR A[Client Request] --> B(Authentication); B --> C(Authorization); C --> D(Admission Control Chain); D --> E{API Handler}; E --> F[Storage Layer]; F --> G((etcd)); G --> F; F --> E; E --> H[Serialization]; H --> A; %% Adding details to steps subgraph Authentication Methods B1[TLS Client Cert] B2[Bearer Token] B3[Service Account Token] B4[OIDC] end B --> B1 & B2 & B3 & B4; subgraph Admission Controllers D1[NamespaceLifecycle] D2[LimitRanger] D3[ResourceQuota] D4[PodSecurity] D5[Mutating/Validating Webhooks] end D --> D1 & D2 & D3 & D4 & D5; %% Style adjustments classDef component fill:#e0f7fa,stroke:#00796b,stroke-width:2px; class A,B,C,D,E,F,G,H,B1,B2,B3,B4,D1,D2,D3,D4,D5 component; Key Sub-Components and Libraries genericapiserver : Core framework for building API servers. Manages HTTP server, request lifecycle, discovery endpoints. Provides foundational authentication, authorization, and admission middleware. Handles REST storage interfaces. controlplane : Kubernetes-specific API logic. Defines Kubernetes resource REST storage implementations. Manages API version enablement and feature gates. Integrates with specific admission controllers and auth/authz mechanisms. kube-aggregator : Handles API aggregation. Discovers and routes requests to aggregated API servers. Manages APIService objects. apiextensions-apiserver : Manages Custom Resource Definitions (CRDs). Provides endpoints for CustomResourceDefinition resources. Integrates CRDs into the API discovery mechanism. Configuration Aspects API Server Count : The --apiserver-count flag (deprecated in favor of --endpoint-reconciler-type ) and --endpoint-reconciler-type flag manage how multiple API server instances coordinate. Service Discovery : The ServiceResolver (built via buildServiceResolver ) is crucial for how the API server locates other services, including aggregated APIs. Storage : The StorageFactory configures how API objects are stored, typically pointing to etcd. Code Example: Server Chain Creation This function wires together the core API server, API extensions server, and the aggregator. // cmd/kube-apiserver/app/server.go func CreateServerChain(config CompletedConfig) (*aggregatorapiserver.APIAggregator, error) { // Create API extensions server first apiExtensionsServer, err := config.ApiExtensions.New(...) // ... error handling ... // Create the main Kubernetes API server kubeAPIServer, err := config.KubeAPIs.New(apiExtensionsServer.GenericAPIServer) // ... error handling ... // Create the aggregator last, delegating to the kube-apiserver aggregatorServer, err := controlplaneapiserver.CreateAggregatorServer(config.Aggregator, kubeAPIServer.ControlPlane.GenericAPIServer, ...) // ... error handling ... return aggregatorServer, nil } Code Example: Request Authentication Setup The testing/testserver.go file shows how client certificate authentication might be set up for testing proxies. // cmd/kube-apiserver/app/testing/testserver.go (simplified) func StartTestServer(t ktesting.TB, instanceOptions *TestServerInstanceOptions, ...) (TestServer, error) { // ... s := options.NewServerRunOptions() // ... if instanceOptions.EnableCertAuth { reqHeaders := serveroptions.NewDelegatingAuthenticationOptions() s.Authentication.RequestHeader = &reqHeaders.RequestHeader // ... configure ClientCAFile, AllowedNames, ProxyClientKeyFile, ProxyClientCertFile ... } // ... completedOptions, err := s.Complete(ctx) // ... } Code Example: Admission Control Initialization The config.go file shows where the admission control configuration is built. // cmd/kube-apiserver/app/config.go func NewConfig(opts options.CompletedOptions) (*Config, error) { // ... build genericConfig, versionedInformers, storageFactory ... // Create KubeAPIServer config, which includes admission initializers kubeAPIs, serviceResolver, pluginInitializer, err := CreateKubeAPIServerConfig(opts, genericConfig, versionedInformers, storageFactory) // ... error handling ... c.KubeAPIs = kubeAPIs // API extensions server config, also uses pluginInitializer apiExtensions, err := controlplaneapiserver.CreateAPIExtensionsConfig(..., pluginInitializer, ...) // ... error handling ... c.ApiExtensions = apiExtensions // ... } Further Details API Endpoints : The specific API endpoints served are determined by the registered REST storage objects within the controlplane and apiextensions-apiserver configurations. These are typically found in packages like k8s.io/kubernetes/pkg/apis/core/v1 and k8s.io/apiextensions-apiserver/pkg/apis/apiextensions/v1 . Request Handling : Handled by the k8s.io/apiserver/pkg/endpoints package, which provides mechanisms for routing, serialization/deserialization, and response generation. Authentication & Authorization : Detailed logic resides in k8s.io/apiserver/pkg/authentication and k8s.io/apiserver/pkg/authorization , with RBAC implementation in k8s.io/kubernetes/plugin/pkg/auth/authorizer/rbac . Admission Control : Implemented via k8s.io/kubernetes/plugin/pkg/admission and specific controllers, managed by k8s.io/apiserver/pkg/admission .","title":"Kubernetes API Server Architecture"},{"location":"docs/kube-apiserver/architecture.html#kubernetes-api-server-architecture","text":"The Kubernetes API server ( kube-apiserver ) is a complex component that integrates multiple functionalities provided by various Kubernetes libraries. It acts as the central communication point for the entire cluster.","title":"Kubernetes API Server Architecture"},{"location":"docs/kube-apiserver/architecture.html#system-context-diagram","text":"This diagram illustrates the kube-apiserver 's position within the Kubernetes control plane and its interactions with other components. graph TD subgraph Kubernetes Control Plane KAS[kube-apiserver] SCH[kube-scheduler] CCM[cloud-controller-manager] CTLM[controller-manager] end subgraph Cluster Components KUBELET[kubelet] KMP[kube-proxy] end subgraph External Clients KUBECTL[kubectl] USERS[Users/Admins] APPLICATIONS[Applications] end subgraph Storage ETCD[(etcd)] end subgraph Other API Servers CRD_API[Custom API Server (e.g., metrics, logging)] end KAS -- Watches/Updates --> SCH KAS -- Watches/Updates --> CTLM KAS -- Watches/Updates --> CCM KAS -- API Calls --> KUBELET KAS -- Routes traffic --> KMP KUBECTL -- API Calls --> KAS USERS -- API Calls --> KAS APPLICATIONS -- API Calls --> KAS KAS -- CRUD Operations --> ETCD KAS -- Delegates API Calls --> CRD_API CRD_API -- API Calls --> KAS KAS -- Watches --> KUBECTL KAS -- Watches --> CTLM KAS -- Watches --> CCM KAS -- Watches --> KMP %% Style adjustments classDef controlplane fill:#c9daf8,stroke:#333,stroke-width:2px; class KAS,SCH,CCM,CTLM controlplane; classDef component fill:#cfe2f3,stroke:#333,stroke-width:2px; class KUBELET,KMP component; classDef external fill:#fce5cd,stroke:#333,stroke-width:2px; class KUBECTL,USERS,APPLICATIONS external; classDef storage fill:#d9ead3,stroke:#333,stroke-width:2px; class ETCD storage; classDef customapi fill:#ead1dc,stroke:#333,stroke-width:2px; class CRD_API customapi;","title":"System Context Diagram"},{"location":"docs/kube-apiserver/architecture.html#internal-architecture-and-request-flow","text":"The kube-apiserver is built upon several core libraries and layers: cmd/kube-apiserver/ : The entry point that sets up the command, parses flags ( options.go ), and orchestrates the server creation. k8s.io/apiserver : Provides the generic framework for API servers. It handles: HTTP request routing and multiplexing. TLS configuration and listener management. Standard API endpoints (healthz, version, discovery). Basic authentication and authorization middleware. Admission controller chaining. REST storage abstraction. k8s.io/kubernetes/pkg/controlplane : Implements the specifics of the Kubernetes API, including: Defining Kubernetes resource schemas. Configuring REST storage for each resource type (mapping to etcd via k8s.io/apiserver/pkg/storage ). Initializing API enablement and feature gates. Setting up specific admission controllers and authentication/authorization strategies. k8s.io/kube-aggregator : Responsible for API aggregation. It allows the kube-apiserver to discover and route requests to other registered API servers (e.g., metrics-server, custom API servers). It implements a ServiceResolver to find these aggregated APIs. Request Flow: A typical API request (e.g., GET /api/v1/pods ) follows this path: HTTP Request : Received by the generic API server's listener. Authentication : The request identity is determined (e.g., via TLS client cert, Bearer Token, Service Account Token). Authorization : The authenticated identity is checked against authorization rules (e.g., RBAC). Admission Control : The request is passed through a chain of admission controllers. These can validate the request, mutate the object, or reject it. API Handler : The request is routed to the appropriate handler based on the API group, version, and resource. Storage Operation : The handler interacts with the storage layer to perform the requested operation (e.g., GET , POST , PUT , DELETE ) on the underlying data store (etcd). Response : The result is serialized and sent back to the client. graph LR A[Client Request] --> B(Authentication); B --> C(Authorization); C --> D(Admission Control Chain); D --> E{API Handler}; E --> F[Storage Layer]; F --> G((etcd)); G --> F; F --> E; E --> H[Serialization]; H --> A; %% Adding details to steps subgraph Authentication Methods B1[TLS Client Cert] B2[Bearer Token] B3[Service Account Token] B4[OIDC] end B --> B1 & B2 & B3 & B4; subgraph Admission Controllers D1[NamespaceLifecycle] D2[LimitRanger] D3[ResourceQuota] D4[PodSecurity] D5[Mutating/Validating Webhooks] end D --> D1 & D2 & D3 & D4 & D5; %% Style adjustments classDef component fill:#e0f7fa,stroke:#00796b,stroke-width:2px; class A,B,C,D,E,F,G,H,B1,B2,B3,B4,D1,D2,D3,D4,D5 component;","title":"Internal Architecture and Request Flow"},{"location":"docs/kube-apiserver/architecture.html#key-sub-components-and-libraries","text":"genericapiserver : Core framework for building API servers. Manages HTTP server, request lifecycle, discovery endpoints. Provides foundational authentication, authorization, and admission middleware. Handles REST storage interfaces. controlplane : Kubernetes-specific API logic. Defines Kubernetes resource REST storage implementations. Manages API version enablement and feature gates. Integrates with specific admission controllers and auth/authz mechanisms. kube-aggregator : Handles API aggregation. Discovers and routes requests to aggregated API servers. Manages APIService objects. apiextensions-apiserver : Manages Custom Resource Definitions (CRDs). Provides endpoints for CustomResourceDefinition resources. Integrates CRDs into the API discovery mechanism.","title":"Key Sub-Components and Libraries"},{"location":"docs/kube-apiserver/architecture.html#configuration-aspects","text":"API Server Count : The --apiserver-count flag (deprecated in favor of --endpoint-reconciler-type ) and --endpoint-reconciler-type flag manage how multiple API server instances coordinate. Service Discovery : The ServiceResolver (built via buildServiceResolver ) is crucial for how the API server locates other services, including aggregated APIs. Storage : The StorageFactory configures how API objects are stored, typically pointing to etcd.","title":"Configuration Aspects"},{"location":"docs/kube-apiserver/architecture.html#code-example-server-chain-creation","text":"This function wires together the core API server, API extensions server, and the aggregator. // cmd/kube-apiserver/app/server.go func CreateServerChain(config CompletedConfig) (*aggregatorapiserver.APIAggregator, error) { // Create API extensions server first apiExtensionsServer, err := config.ApiExtensions.New(...) // ... error handling ... // Create the main Kubernetes API server kubeAPIServer, err := config.KubeAPIs.New(apiExtensionsServer.GenericAPIServer) // ... error handling ... // Create the aggregator last, delegating to the kube-apiserver aggregatorServer, err := controlplaneapiserver.CreateAggregatorServer(config.Aggregator, kubeAPIServer.ControlPlane.GenericAPIServer, ...) // ... error handling ... return aggregatorServer, nil }","title":"Code Example: Server Chain Creation"},{"location":"docs/kube-apiserver/architecture.html#code-example-request-authentication-setup","text":"The testing/testserver.go file shows how client certificate authentication might be set up for testing proxies. // cmd/kube-apiserver/app/testing/testserver.go (simplified) func StartTestServer(t ktesting.TB, instanceOptions *TestServerInstanceOptions, ...) (TestServer, error) { // ... s := options.NewServerRunOptions() // ... if instanceOptions.EnableCertAuth { reqHeaders := serveroptions.NewDelegatingAuthenticationOptions() s.Authentication.RequestHeader = &reqHeaders.RequestHeader // ... configure ClientCAFile, AllowedNames, ProxyClientKeyFile, ProxyClientCertFile ... } // ... completedOptions, err := s.Complete(ctx) // ... }","title":"Code Example: Request Authentication Setup"},{"location":"docs/kube-apiserver/architecture.html#code-example-admission-control-initialization","text":"The config.go file shows where the admission control configuration is built. // cmd/kube-apiserver/app/config.go func NewConfig(opts options.CompletedOptions) (*Config, error) { // ... build genericConfig, versionedInformers, storageFactory ... // Create KubeAPIServer config, which includes admission initializers kubeAPIs, serviceResolver, pluginInitializer, err := CreateKubeAPIServerConfig(opts, genericConfig, versionedInformers, storageFactory) // ... error handling ... c.KubeAPIs = kubeAPIs // API extensions server config, also uses pluginInitializer apiExtensions, err := controlplaneapiserver.CreateAPIExtensionsConfig(..., pluginInitializer, ...) // ... error handling ... c.ApiExtensions = apiExtensions // ... }","title":"Code Example: Admission Control Initialization"},{"location":"docs/kube-apiserver/architecture.html#further-details","text":"API Endpoints : The specific API endpoints served are determined by the registered REST storage objects within the controlplane and apiextensions-apiserver configurations. These are typically found in packages like k8s.io/kubernetes/pkg/apis/core/v1 and k8s.io/apiextensions-apiserver/pkg/apis/apiextensions/v1 . Request Handling : Handled by the k8s.io/apiserver/pkg/endpoints package, which provides mechanisms for routing, serialization/deserialization, and response generation. Authentication & Authorization : Detailed logic resides in k8s.io/apiserver/pkg/authentication and k8s.io/apiserver/pkg/authorization , with RBAC implementation in k8s.io/kubernetes/plugin/pkg/auth/authorizer/rbac . Admission Control : Implemented via k8s.io/kubernetes/plugin/pkg/admission and specific controllers, managed by k8s.io/apiserver/pkg/admission .","title":"Further Details"},{"location":"docs/kube-apiserver/configuration.html","text":"Kubernetes API Server Configuration The Kubernetes API server ( kube-apiserver ) is highly configurable through a large set of command-line flags. These flags control various aspects of its behavior, including network configuration, authentication, authorization, admission control, storage, and more. The primary configuration structure is defined in cmd/kube-apiserver/app/options/options.go within the ServerRunOptions struct. This struct embeds controlplaneapiserver.Options and includes an Extra struct for additional Kubernetes-specific configurations. Core Configuration Options The following table summarizes key configuration areas and their associated flags. Note that many options are deeply nested and managed by the underlying k8s.io/apiserver and k8s.io/kubernetes/pkg/controlplane packages. Configuration Area Key Options / Flags Description Source File(s) Serving --secure-port , --bind-address , --advertise-address , --cert-dir , --tls-cert-file , --tls-private-key-file Configures the HTTPS port, IP address to bind to, address advertised to peers, and TLS certificates/keys for secure communication. k8s.io/apiserver/pkg/server/options/secure_serving.go Authentication --client-ca-file , --requestheader-client-ca-file , --requestheader-allowed-names , --requestheader-username-headers , --requestheader-group-headers , --requestheader-extra-headers-prefix , --service-account-key-file , --service-account-signing-key-file , --oidc-issuer-url , --oidc-client-id Defines how the API server authenticates requests. Supports client certificates, request header authentication (for proxies/load balancers), service account tokens, and OpenID Connect (OIDC). k8s.io/apiserver/pkg/authentication/options/authentication.go Authorization --authorization-mode Specifies the authorization mode(s) to use. Common values include RBAC , Node , Webhook , ABAC , AlwaysAllow , AlwaysDeny . RBAC is the recommended and most common mode. k8s.io/apiserver/pkg/authorization/options/authorization.go Admission Control --enable-admission-plugins , --disable-admission-plugins , --admission-control-config-file Enables or disables specific admission controllers. This is a critical security and policy enforcement layer. Examples: NamespaceLifecycle , LimitRanger , ServiceAccount , ResourceQuota , PodSecurity , MutatingAdmissionWebhook , ValidatingAdmissionWebhook . k8s.io/kubernetes/pkg/kubeapiserver/admission/options.go API Enablement --runtime-config Allows enabling or disabling specific API versions or groups at runtime. Useful for customizing the API surface. Example: admissionregistration.k8s.io/v1beta1=true . k8s.io/apiserver/pkg/server/options/api_enablement.go Storage --etcd-servers , --etcd-prefix , --etcd-cafile , --etcd-certfile , --etcd-keyfile Configures the connection details for the etcd backend where Kubernetes objects are stored. Includes TLS configuration for secure etcd communication. k8s.io/apiserver/pkg/storage/options/storage_options.go Service IPs --service-cluster-ip-range , --service-node-port-range Defines the IP address ranges for Kubernetes Services ( ClusterIP ) and the port range reserved for Services of type NodePort . cmd/kube-apiserver/app/options/options.go (in Extra struct) Kubelet Communication --kubelet-preferred-address-types , --kubelet-timeout , --kubelet-client-certificate , --kubelet-client-key , --kubelet-certificate-authority Configures how the API server communicates with Kubelets on nodes, including preferred address types, timeouts, and TLS settings. cmd/kube-apiserver/app/options/options.go (in Extra.KubeletConfig ) Feature Gates --feature-gates Enables or disables experimental features. This flag is often managed globally by the component-base library. k8s.io/apiserver/pkg/util/feature/feature_flags.go Logging & Metrics --logtostderr , --v , --logging-format , --advertise-address (for metrics) Controls logging verbosity, format (e.g., JSON), and metrics exposure. k8s.io/component-base/logs/api/v1 Example Configuration Snippets 1. Authentication and Authorization Flags This example shows how to configure basic RBAC authorization and Service Account token authentication. # Enable RBAC authorization and use the default service account key file --authorization-mode=RBAC --service-account-key-file=/etc/kubernetes/pki/sa.pub 2. Admission Control Configuration Enabling specific admission controllers and providing a configuration file for webhooks. # Enable common admission controllers and specify a config file for webhooks --enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,ResourceQuota,PodSecurity,MutatingAdmissionWebhook,ValidatingAdmissionWebhook --admission-control-config-file=/etc/kubernetes/admission-config.yaml Example admission-config.yaml snippet: apiVersion: apiserver.k8s.io/v1 kind: AdmissionConfiguration webhooks: - name: my-validating-webhook.example.com clientConfig: service: name: my-webhook-service namespace: webhook-namespace path: \"/validate\" caBundle: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0t... rules: - operations: [\"CREATE\", \"UPDATE\"] apiGroups: [\"*\"] apiVersions: [\"*\"] resources: [\"pods\", \"deployments\"] when: after: ResourcePolicy 3. Serving Configuration Configuring the secure port and TLS certificates. # Bind to all interfaces on port 6443, use specific cert/key files --secure-port=6443 --bind-address=0.0.0.0 --tls-cert-file=/etc/kubernetes/pki/apiserver.crt --tls-private-key-file=/etc/kubernetes/pki/apiserver.key Default Values Many options have default values defined within the options.go files and are initialized using options.NewServerRunOptions() . For instance, the default service cluster IP range might be set based on the cluster configuration or a sensible default like 10.0.0.0/16 . The k8s.io/kubernetes/cmd/kube-apiserver/app/options/options.go file contains the initialization logic: // cmd/kube-apiserver/app/options/options.go func NewServerRunOptions() *ServerRunOptions { s := ServerRunOptions{ Options: controlplaneapiserver.NewOptions(), // Initializes embedded options Extra: Extra{ // ... default values for Extra struct fields ... ServiceClusterIPRanges: \"10.0.0.0/16\", // Example default KubeletConfig: kubeletclient.KubeletClientConfig{ Port: ports.KubeletPort, // Default Kubelet port // ... other defaults ... }, MasterCount: 1, }, } // ... return &s } Further Reading Kubernetes API Server Documentation : The official Kubernetes documentation provides in-depth explanations of various concepts, including authentication, authorization, and admission control. Source Code : Examining the source files mentioned above provides the most accurate and detailed information on available options and their behavior.","title":"Kubernetes API Server Configuration"},{"location":"docs/kube-apiserver/configuration.html#kubernetes-api-server-configuration","text":"The Kubernetes API server ( kube-apiserver ) is highly configurable through a large set of command-line flags. These flags control various aspects of its behavior, including network configuration, authentication, authorization, admission control, storage, and more. The primary configuration structure is defined in cmd/kube-apiserver/app/options/options.go within the ServerRunOptions struct. This struct embeds controlplaneapiserver.Options and includes an Extra struct for additional Kubernetes-specific configurations.","title":"Kubernetes API Server Configuration"},{"location":"docs/kube-apiserver/configuration.html#core-configuration-options","text":"The following table summarizes key configuration areas and their associated flags. Note that many options are deeply nested and managed by the underlying k8s.io/apiserver and k8s.io/kubernetes/pkg/controlplane packages. Configuration Area Key Options / Flags Description Source File(s) Serving --secure-port , --bind-address , --advertise-address , --cert-dir , --tls-cert-file , --tls-private-key-file Configures the HTTPS port, IP address to bind to, address advertised to peers, and TLS certificates/keys for secure communication. k8s.io/apiserver/pkg/server/options/secure_serving.go Authentication --client-ca-file , --requestheader-client-ca-file , --requestheader-allowed-names , --requestheader-username-headers , --requestheader-group-headers , --requestheader-extra-headers-prefix , --service-account-key-file , --service-account-signing-key-file , --oidc-issuer-url , --oidc-client-id Defines how the API server authenticates requests. Supports client certificates, request header authentication (for proxies/load balancers), service account tokens, and OpenID Connect (OIDC). k8s.io/apiserver/pkg/authentication/options/authentication.go Authorization --authorization-mode Specifies the authorization mode(s) to use. Common values include RBAC , Node , Webhook , ABAC , AlwaysAllow , AlwaysDeny . RBAC is the recommended and most common mode. k8s.io/apiserver/pkg/authorization/options/authorization.go Admission Control --enable-admission-plugins , --disable-admission-plugins , --admission-control-config-file Enables or disables specific admission controllers. This is a critical security and policy enforcement layer. Examples: NamespaceLifecycle , LimitRanger , ServiceAccount , ResourceQuota , PodSecurity , MutatingAdmissionWebhook , ValidatingAdmissionWebhook . k8s.io/kubernetes/pkg/kubeapiserver/admission/options.go API Enablement --runtime-config Allows enabling or disabling specific API versions or groups at runtime. Useful for customizing the API surface. Example: admissionregistration.k8s.io/v1beta1=true . k8s.io/apiserver/pkg/server/options/api_enablement.go Storage --etcd-servers , --etcd-prefix , --etcd-cafile , --etcd-certfile , --etcd-keyfile Configures the connection details for the etcd backend where Kubernetes objects are stored. Includes TLS configuration for secure etcd communication. k8s.io/apiserver/pkg/storage/options/storage_options.go Service IPs --service-cluster-ip-range , --service-node-port-range Defines the IP address ranges for Kubernetes Services ( ClusterIP ) and the port range reserved for Services of type NodePort . cmd/kube-apiserver/app/options/options.go (in Extra struct) Kubelet Communication --kubelet-preferred-address-types , --kubelet-timeout , --kubelet-client-certificate , --kubelet-client-key , --kubelet-certificate-authority Configures how the API server communicates with Kubelets on nodes, including preferred address types, timeouts, and TLS settings. cmd/kube-apiserver/app/options/options.go (in Extra.KubeletConfig ) Feature Gates --feature-gates Enables or disables experimental features. This flag is often managed globally by the component-base library. k8s.io/apiserver/pkg/util/feature/feature_flags.go Logging & Metrics --logtostderr , --v , --logging-format , --advertise-address (for metrics) Controls logging verbosity, format (e.g., JSON), and metrics exposure. k8s.io/component-base/logs/api/v1","title":"Core Configuration Options"},{"location":"docs/kube-apiserver/configuration.html#example-configuration-snippets","text":"","title":"Example Configuration Snippets"},{"location":"docs/kube-apiserver/configuration.html#1-authentication-and-authorization-flags","text":"This example shows how to configure basic RBAC authorization and Service Account token authentication. # Enable RBAC authorization and use the default service account key file --authorization-mode=RBAC --service-account-key-file=/etc/kubernetes/pki/sa.pub","title":"1. Authentication and Authorization Flags"},{"location":"docs/kube-apiserver/configuration.html#2-admission-control-configuration","text":"Enabling specific admission controllers and providing a configuration file for webhooks. # Enable common admission controllers and specify a config file for webhooks --enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,ResourceQuota,PodSecurity,MutatingAdmissionWebhook,ValidatingAdmissionWebhook --admission-control-config-file=/etc/kubernetes/admission-config.yaml Example admission-config.yaml snippet: apiVersion: apiserver.k8s.io/v1 kind: AdmissionConfiguration webhooks: - name: my-validating-webhook.example.com clientConfig: service: name: my-webhook-service namespace: webhook-namespace path: \"/validate\" caBundle: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0t... rules: - operations: [\"CREATE\", \"UPDATE\"] apiGroups: [\"*\"] apiVersions: [\"*\"] resources: [\"pods\", \"deployments\"] when: after: ResourcePolicy","title":"2. Admission Control Configuration"},{"location":"docs/kube-apiserver/configuration.html#3-serving-configuration","text":"Configuring the secure port and TLS certificates. # Bind to all interfaces on port 6443, use specific cert/key files --secure-port=6443 --bind-address=0.0.0.0 --tls-cert-file=/etc/kubernetes/pki/apiserver.crt --tls-private-key-file=/etc/kubernetes/pki/apiserver.key","title":"3. Serving Configuration"},{"location":"docs/kube-apiserver/configuration.html#default-values","text":"Many options have default values defined within the options.go files and are initialized using options.NewServerRunOptions() . For instance, the default service cluster IP range might be set based on the cluster configuration or a sensible default like 10.0.0.0/16 . The k8s.io/kubernetes/cmd/kube-apiserver/app/options/options.go file contains the initialization logic: // cmd/kube-apiserver/app/options/options.go func NewServerRunOptions() *ServerRunOptions { s := ServerRunOptions{ Options: controlplaneapiserver.NewOptions(), // Initializes embedded options Extra: Extra{ // ... default values for Extra struct fields ... ServiceClusterIPRanges: \"10.0.0.0/16\", // Example default KubeletConfig: kubeletclient.KubeletClientConfig{ Port: ports.KubeletPort, // Default Kubelet port // ... other defaults ... }, MasterCount: 1, }, } // ... return &s }","title":"Default Values"},{"location":"docs/kube-apiserver/configuration.html#further-reading","text":"Kubernetes API Server Documentation : The official Kubernetes documentation provides in-depth explanations of various concepts, including authentication, authorization, and admission control. Source Code : Examining the source files mentioned above provides the most accurate and detailed information on available options and their behavior.","title":"Further Reading"},{"location":"docs/kube-controller-manager/index.html","text":"kube-controller-manager The kube-controller-manager is a core component of Kubernetes that runs controllers. These controllers watch the state of the cluster and make changes attempting to move the current state towards the desired state. Table of Contents Overview Core Controllers Control Loop Logic Key Concepts Sub-components Architecture API Reference Configuration Overview The kube-controller-manager binary provides a control plane component that runs controllers. It is a single binary that includes all the core controllers, which are logically grouped. It is responsible for a variety of control loops that manage Kubernetes resources. Core Controllers The kube-controller-manager manages several core controllers, including: - Node Controller - Replication Controller - Endpoint Controller - Namespace Controller - ServiceAccount Controller - PersistentVolume Controller - PersistentVolumeClaim Controller - Route Controller - ResourceQuota Controller - Deployment Controller - StatefulSet Controller - DaemonSet Controller - Job Controller - CronJob Controller - ... (and many others) Control Loop Logic Each controller runs a control loop that continuously monitors the state of the cluster and attempts to reconcile the desired state with the current state. This involves: 1. Watching for changes to specific Kubernetes resources. 2. Comparing the current state with the desired state. 3. Taking actions to correct any discrepancies. Key Concepts Desired State : The target state of the cluster as defined by the user (e.g., a Deployment with 3 replicas). Current State : The actual state of the cluster as observed by the controllers. Reconciliation : The process by which controllers try to bring the current state in line with the desired state. Informers : A mechanism used by controllers to efficiently watch for changes to resources without constantly polling the API server. Sub-components The kube-controller-manager is composed of numerous individual controllers, each responsible for a specific resource type or functionality. Some of the key sub-components include: - Node Controller ( pkg/controller/node/nodecontroller.go ) - Replication Controller ( pkg/controller/replication/replicationcontroller.go ) - Endpoint Controller ( pkg/controller/endpoint/endpointcontroller.go ) - Namespace Controller ( pkg/controller/namespace/namespacecontroller.go ) - ServiceAccount Controller ( pkg/controller/serviceaccount/serviceaccountcontroller.go ) - PersistentVolume Controller ( pkg/controller/volume/persistentvolume/persistentvolumecontroller.go ) - ... (and many others) Architecture (Details to be added in architecture.md ) API Reference (Details to be added in api_reference.md ) Configuration (Details to be added in configuration.md )","title":"kube-controller-manager"},{"location":"docs/kube-controller-manager/index.html#kube-controller-manager","text":"The kube-controller-manager is a core component of Kubernetes that runs controllers. These controllers watch the state of the cluster and make changes attempting to move the current state towards the desired state.","title":"kube-controller-manager"},{"location":"docs/kube-controller-manager/index.html#table-of-contents","text":"Overview Core Controllers Control Loop Logic Key Concepts Sub-components Architecture API Reference Configuration","title":"Table of Contents"},{"location":"docs/kube-controller-manager/index.html#overview","text":"The kube-controller-manager binary provides a control plane component that runs controllers. It is a single binary that includes all the core controllers, which are logically grouped. It is responsible for a variety of control loops that manage Kubernetes resources.","title":"Overview"},{"location":"docs/kube-controller-manager/index.html#core-controllers","text":"The kube-controller-manager manages several core controllers, including: - Node Controller - Replication Controller - Endpoint Controller - Namespace Controller - ServiceAccount Controller - PersistentVolume Controller - PersistentVolumeClaim Controller - Route Controller - ResourceQuota Controller - Deployment Controller - StatefulSet Controller - DaemonSet Controller - Job Controller - CronJob Controller - ... (and many others)","title":"Core Controllers"},{"location":"docs/kube-controller-manager/index.html#control-loop-logic","text":"Each controller runs a control loop that continuously monitors the state of the cluster and attempts to reconcile the desired state with the current state. This involves: 1. Watching for changes to specific Kubernetes resources. 2. Comparing the current state with the desired state. 3. Taking actions to correct any discrepancies.","title":"Control Loop Logic"},{"location":"docs/kube-controller-manager/index.html#key-concepts","text":"Desired State : The target state of the cluster as defined by the user (e.g., a Deployment with 3 replicas). Current State : The actual state of the cluster as observed by the controllers. Reconciliation : The process by which controllers try to bring the current state in line with the desired state. Informers : A mechanism used by controllers to efficiently watch for changes to resources without constantly polling the API server.","title":"Key Concepts"},{"location":"docs/kube-controller-manager/index.html#sub-components","text":"The kube-controller-manager is composed of numerous individual controllers, each responsible for a specific resource type or functionality. Some of the key sub-components include: - Node Controller ( pkg/controller/node/nodecontroller.go ) - Replication Controller ( pkg/controller/replication/replicationcontroller.go ) - Endpoint Controller ( pkg/controller/endpoint/endpointcontroller.go ) - Namespace Controller ( pkg/controller/namespace/namespacecontroller.go ) - ServiceAccount Controller ( pkg/controller/serviceaccount/serviceaccountcontroller.go ) - PersistentVolume Controller ( pkg/controller/volume/persistentvolume/persistentvolumecontroller.go ) - ... (and many others)","title":"Sub-components"},{"location":"docs/kube-controller-manager/index.html#architecture","text":"(Details to be added in architecture.md )","title":"Architecture"},{"location":"docs/kube-controller-manager/index.html#api-reference","text":"(Details to be added in api_reference.md )","title":"API Reference"},{"location":"docs/kube-controller-manager/index.html#configuration","text":"(Details to be added in configuration.md )","title":"Configuration"},{"location":"docs/kube-controller-manager/api_reference.html","text":"API Reference The kube-controller-manager primarily interacts with the Kubernetes API Server. It does not expose its own distinct API endpoint for external consumption. Instead, its functionality is defined by the controllers it runs and their interactions with the Kubernetes API. Internal Interfaces and Interactions Controllers within the kube-controller-manager utilize client libraries to interact with the Kubernetes API Server. These interactions typically involve: Watching resources: Controllers use informers to get efficient, cached, and event-driven views of Kubernetes resources. Reading resources: Fetching detailed information about specific resources. Creating resources: e.g., creating Pods, Services, Endpoints. Updating resources: Modifying existing resources, such as updating Pod status. Deleting resources: Removing resources from the cluster. Key Client Interactions The controllers interact with the API Server through various Kubernetes client-go packages. For example: Core Clients : For interacting with core API objects like Pods, Services, Namespaces, etc. Informers : A crucial component for efficiently watching resource changes without constant polling. Control Loop API Interactions (Example: Node Controller) When the Node Controller detects a new Node, it might: 1. Watch for Node events via the API Server. 2. Create a Node object in the cluster if it's a new node. 3. Update the Node's status based on heartbeats or other conditions. 4. Perform garbage collection of resources associated with deleted nodes. Code Examples 1. Example of using Informers (Conceptual) ```go // This is a conceptual example. Actual implementation uses controller-runtime or client-go. import ( \"k8s.io/client-go/informers\" \"k8s.io/client-go/kubernetes\" // ... other imports ) func NewNodeController(client kubernetes.Interface, nodeInformer informers.NodeInformer) *NodeController { // ... controller setup nodeInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{ AddFunc: onAdd, UpdateFunc: onUpdate, DeleteFunc: onDelete, }) // ... return &NodeController{client: client} } func (c NodeController) onAdd(obj interface{}) { node := obj.( corev1.Node) // Logic to handle new node } // ... other event handlers ``` 2. Creating a Namespace (Conceptual) ```go // This is a conceptual example. import ( \"context\" corev1 \"k8s.io/api/core/v1\" metav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\" \"k8s.io/client-go/kubernetes\" ) func createNamespace(client kubernetes.Clientset, name string) (*corev1.Namespace, error) { ns := &corev1.Namespace{ ObjectMeta: metav1.ObjectMeta{ Name: name, }, } return client.CoreV1().Namespaces().Create(context.TODO(), ns, metav1.CreateOptions{}) } ``` 3. Updating Pod Status (Conceptual) ```go // This is a conceptual example. import ( \"context\" corev1 \"k8s.io/api/core/v1\" metav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\" \"k8s.io/client-go/kubernetes\" \"k8s.io/apimachinery/pkg/types\" ) func updatePodStatus(client kubernetes.Clientset, namespace, podName string, status corev1.PodStatus) (*corev1.Pod, error) { // In a real scenario, you'd likely use a strategic merge patch or JSON merge patch // to update specific fields like status.conditions or status.phase. // For simplicity, this example shows a full object update which is less common for status. // Fetch the latest pod to ensure we have the correct resourceVersion for optimistic locking pod, err := client.CoreV1().Pods(namespace).Get(context.TODO(), podName, metav1.GetOptions{}) if err != nil { return nil, err } pod.Status = status // Ensure ResourceVersion is set for optimistic concurrency control // status.ObservedGeneration is also important to set correctly return client.CoreV1().Pods(namespace).UpdateStatus(context.TODO(), pod, metav1.UpdateOptions{}) } ``` Table of Operations Operation Description Associated Resources Watch/List Monitor and retrieve resources. Pods, Nodes, Services, etc. Get Retrieve a single specific resource. Pods, Nodes, Services, etc. Create Create new resources. Pods, Deployments, Services, etc. Update Modify existing resources. Pods, Deployments, Services, etc. Delete Remove resources. Pods, Deployments, Services, etc. UpdateStatus Update the status subresource of a resource. Pods, Nodes, Deployments, etc. Patch Partially update a resource. All Source Files Controller implementations are expected to be in packages like pkg/controller/ and its subdirectories. Client interactions are typically within controller logic files.","title":"API Reference"},{"location":"docs/kube-controller-manager/api_reference.html#api-reference","text":"The kube-controller-manager primarily interacts with the Kubernetes API Server. It does not expose its own distinct API endpoint for external consumption. Instead, its functionality is defined by the controllers it runs and their interactions with the Kubernetes API.","title":"API Reference"},{"location":"docs/kube-controller-manager/api_reference.html#internal-interfaces-and-interactions","text":"Controllers within the kube-controller-manager utilize client libraries to interact with the Kubernetes API Server. These interactions typically involve: Watching resources: Controllers use informers to get efficient, cached, and event-driven views of Kubernetes resources. Reading resources: Fetching detailed information about specific resources. Creating resources: e.g., creating Pods, Services, Endpoints. Updating resources: Modifying existing resources, such as updating Pod status. Deleting resources: Removing resources from the cluster.","title":"Internal Interfaces and Interactions"},{"location":"docs/kube-controller-manager/api_reference.html#key-client-interactions","text":"The controllers interact with the API Server through various Kubernetes client-go packages. For example: Core Clients : For interacting with core API objects like Pods, Services, Namespaces, etc. Informers : A crucial component for efficiently watching resource changes without constant polling.","title":"Key Client Interactions"},{"location":"docs/kube-controller-manager/api_reference.html#control-loop-api-interactions-example-node-controller","text":"When the Node Controller detects a new Node, it might: 1. Watch for Node events via the API Server. 2. Create a Node object in the cluster if it's a new node. 3. Update the Node's status based on heartbeats or other conditions. 4. Perform garbage collection of resources associated with deleted nodes.","title":"Control Loop API Interactions (Example: Node Controller)"},{"location":"docs/kube-controller-manager/api_reference.html#code-examples","text":"","title":"Code Examples"},{"location":"docs/kube-controller-manager/api_reference.html#1-example-of-using-informers-conceptual","text":"```go // This is a conceptual example. Actual implementation uses controller-runtime or client-go. import ( \"k8s.io/client-go/informers\" \"k8s.io/client-go/kubernetes\" // ... other imports ) func NewNodeController(client kubernetes.Interface, nodeInformer informers.NodeInformer) *NodeController { // ... controller setup nodeInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{ AddFunc: onAdd, UpdateFunc: onUpdate, DeleteFunc: onDelete, }) // ... return &NodeController{client: client} } func (c NodeController) onAdd(obj interface{}) { node := obj.( corev1.Node) // Logic to handle new node } // ... other event handlers ```","title":"1. Example of using Informers (Conceptual)"},{"location":"docs/kube-controller-manager/api_reference.html#2-creating-a-namespace-conceptual","text":"```go // This is a conceptual example. import ( \"context\" corev1 \"k8s.io/api/core/v1\" metav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\" \"k8s.io/client-go/kubernetes\" ) func createNamespace(client kubernetes.Clientset, name string) (*corev1.Namespace, error) { ns := &corev1.Namespace{ ObjectMeta: metav1.ObjectMeta{ Name: name, }, } return client.CoreV1().Namespaces().Create(context.TODO(), ns, metav1.CreateOptions{}) } ```","title":"2. Creating a Namespace (Conceptual)"},{"location":"docs/kube-controller-manager/api_reference.html#3-updating-pod-status-conceptual","text":"```go // This is a conceptual example. import ( \"context\" corev1 \"k8s.io/api/core/v1\" metav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\" \"k8s.io/client-go/kubernetes\" \"k8s.io/apimachinery/pkg/types\" ) func updatePodStatus(client kubernetes.Clientset, namespace, podName string, status corev1.PodStatus) (*corev1.Pod, error) { // In a real scenario, you'd likely use a strategic merge patch or JSON merge patch // to update specific fields like status.conditions or status.phase. // For simplicity, this example shows a full object update which is less common for status. // Fetch the latest pod to ensure we have the correct resourceVersion for optimistic locking pod, err := client.CoreV1().Pods(namespace).Get(context.TODO(), podName, metav1.GetOptions{}) if err != nil { return nil, err } pod.Status = status // Ensure ResourceVersion is set for optimistic concurrency control // status.ObservedGeneration is also important to set correctly return client.CoreV1().Pods(namespace).UpdateStatus(context.TODO(), pod, metav1.UpdateOptions{}) } ```","title":"3. Updating Pod Status (Conceptual)"},{"location":"docs/kube-controller-manager/api_reference.html#table-of-operations","text":"Operation Description Associated Resources Watch/List Monitor and retrieve resources. Pods, Nodes, Services, etc. Get Retrieve a single specific resource. Pods, Nodes, Services, etc. Create Create new resources. Pods, Deployments, Services, etc. Update Modify existing resources. Pods, Deployments, Services, etc. Delete Remove resources. Pods, Deployments, Services, etc. UpdateStatus Update the status subresource of a resource. Pods, Nodes, Deployments, etc. Patch Partially update a resource. All","title":"Table of Operations"},{"location":"docs/kube-controller-manager/api_reference.html#source-files","text":"Controller implementations are expected to be in packages like pkg/controller/ and its subdirectories. Client interactions are typically within controller logic files.","title":"Source Files"},{"location":"docs/kube-controller-manager/architecture.html","text":"Architecture System Context Diagram ```mermaid graph TD A[kube-controller-manager] --> B(API Server); A --> C(etcd); B --> C; D[Kubelet] --> B; E[User] --> B; F[Other Controllers] --> B; ``` Internal Architecture Diagram ```mermaid graph TD subgraph kube-controller-manager N[Node Controller] --> APIServer; R[Replication Controller] --> APIServer; E[Endpoint Controller] --> APIServer; NS[Namespace Controller] --> APIServer; SA[ServiceAccount Controller] --> APIServer; PV[PersistentVolume Controller] --> APIServer; PVC[PersistentVolumeClaim Controller] --> APIServer; Other[Other Controllers] --> APIServer; end APIServer[API Server]; ``` Control Flow Example: Pod Creation by Replication Controller Replication Controller watches for ReplicationController objects. When a ReplicationController is created/updated, it defines a desired number of pod replicas. The controller compares the desired replica count with the actual running pods matching the selector. If there's a discrepancy, the controller creates/deletes pods via the API Server . API Server updates etcd . Kubelet on worker nodes watches for Pods assigned to it via the API Server . Kubelet creates/manages the pods. Component Interactions kube-controller-manager interacts with the API Server to watch, get, create, update, and delete resources. Controllers within the manager use informers to efficiently track resource changes. The API Server persists state changes to etcd . Diagrams System Context Diagram (provided above) Internal Controller Orchestration Diagram (provided above) Control Loop Diagram for a specific controller (e.g., Node Controller lifecycle) Code Examples (Examples to be added) Source Files Main entry point: cmd/kube-controller-manager/controller-manager.go Controller implementations are expected to be in packages like pkg/controller/ and its subdirectories.","title":"Architecture"},{"location":"docs/kube-controller-manager/architecture.html#architecture","text":"","title":"Architecture"},{"location":"docs/kube-controller-manager/architecture.html#system-context-diagram","text":"```mermaid graph TD A[kube-controller-manager] --> B(API Server); A --> C(etcd); B --> C; D[Kubelet] --> B; E[User] --> B; F[Other Controllers] --> B; ```","title":"System Context Diagram"},{"location":"docs/kube-controller-manager/architecture.html#internal-architecture-diagram","text":"```mermaid graph TD subgraph kube-controller-manager N[Node Controller] --> APIServer; R[Replication Controller] --> APIServer; E[Endpoint Controller] --> APIServer; NS[Namespace Controller] --> APIServer; SA[ServiceAccount Controller] --> APIServer; PV[PersistentVolume Controller] --> APIServer; PVC[PersistentVolumeClaim Controller] --> APIServer; Other[Other Controllers] --> APIServer; end APIServer[API Server]; ```","title":"Internal Architecture Diagram"},{"location":"docs/kube-controller-manager/architecture.html#control-flow-example-pod-creation-by-replication-controller","text":"Replication Controller watches for ReplicationController objects. When a ReplicationController is created/updated, it defines a desired number of pod replicas. The controller compares the desired replica count with the actual running pods matching the selector. If there's a discrepancy, the controller creates/deletes pods via the API Server . API Server updates etcd . Kubelet on worker nodes watches for Pods assigned to it via the API Server . Kubelet creates/manages the pods.","title":"Control Flow Example: Pod Creation by Replication Controller"},{"location":"docs/kube-controller-manager/architecture.html#component-interactions","text":"kube-controller-manager interacts with the API Server to watch, get, create, update, and delete resources. Controllers within the manager use informers to efficiently track resource changes. The API Server persists state changes to etcd .","title":"Component Interactions"},{"location":"docs/kube-controller-manager/architecture.html#diagrams","text":"System Context Diagram (provided above) Internal Controller Orchestration Diagram (provided above) Control Loop Diagram for a specific controller (e.g., Node Controller lifecycle)","title":"Diagrams"},{"location":"docs/kube-controller-manager/architecture.html#code-examples","text":"(Examples to be added)","title":"Code Examples"},{"location":"docs/kube-controller-manager/architecture.html#source-files","text":"Main entry point: cmd/kube-controller-manager/controller-manager.go Controller implementations are expected to be in packages like pkg/controller/ and its subdirectories.","title":"Source Files"},{"location":"docs/kube-controller-manager/configuration.html","text":"Configuration The kube-controller-manager is primarily configured using command-line flags. These flags control various aspects of its behavior, including the controllers it runs, leader election, and connection details for the API server. Command-line Flags Below is a table of common configuration flags. For a comprehensive list, refer to the Kubernetes source code or use the --help flag on a running instance. Flag Name Type Default Value Description --kubeconfig string ~/.kube/config Path to the kubeconfig file. Used to connect to the Kubernetes API server. --cluster-name string kubernetes The name of the cluster. Used for leader election. --controllers comma-separated list * Comma-separated list of controllers to run. * runs all. Use !controllerName to disable specific controllers. --enable-leader-election bool true Enables leader election for controllers. Ensures only one instance is active at a time. --leader-election-lease-duration duration 15s The duration that non-leader candidates will wait to retry leader election. --leader-election-renew-deadline duration 10s The interval on which the leader renews its lease. --leader-election-retry-period duration 2s The duration the clients need to wait to start leader election again. --node-monitor-period duration 5s The interval at which the NodeController monitors nodes. --node-monitor-grace-period duration 40s The amount of time the NodeController will continue to monitor a node before marking it as unhealthy. --pod-eviction-timeout duration 5m The grace period for deleting pods on a node that is terminating. --service-account-private-key-file string Path to the private key file used to sign service account tokens. --root-ca-file string If set, this root certificate authority will be used for verifying the API server during connection. --enable-garbage-collector bool true Whether to enable the garbage collector. --term-container-health-check-timeout duration 1m Timeout for container health checks during termination. Example Configuration (Command-line) ```bash kube-controller-manager \\ --kubeconfig=/etc/kubernetes/controller-manager.conf \\ --cluster-name=my-k8s-cluster \\ --controllers=* \\ --leader-election-lease-duration=15s \\ --node-monitor-period=5s \\ --node-monitor-grace-period=40s \\ --service-account-private-key-file=/etc/kubernetes/pki/sa.key \\ --root-ca-file=/etc/kubernetes/pki/ca.crt \\ --enable-leader-election=true ``` Environment Variables While primarily configured via flags, some underlying libraries or components might respect environment variables for configuration (e.g., KUBECONFIG ). Source Files Flag definitions are typically found in the cmd/kube-controller-manager/ directory. The logic for applying these configurations is within the main controller-manager.go file and related setup functions.","title":"Configuration"},{"location":"docs/kube-controller-manager/configuration.html#configuration","text":"The kube-controller-manager is primarily configured using command-line flags. These flags control various aspects of its behavior, including the controllers it runs, leader election, and connection details for the API server.","title":"Configuration"},{"location":"docs/kube-controller-manager/configuration.html#command-line-flags","text":"Below is a table of common configuration flags. For a comprehensive list, refer to the Kubernetes source code or use the --help flag on a running instance. Flag Name Type Default Value Description --kubeconfig string ~/.kube/config Path to the kubeconfig file. Used to connect to the Kubernetes API server. --cluster-name string kubernetes The name of the cluster. Used for leader election. --controllers comma-separated list * Comma-separated list of controllers to run. * runs all. Use !controllerName to disable specific controllers. --enable-leader-election bool true Enables leader election for controllers. Ensures only one instance is active at a time. --leader-election-lease-duration duration 15s The duration that non-leader candidates will wait to retry leader election. --leader-election-renew-deadline duration 10s The interval on which the leader renews its lease. --leader-election-retry-period duration 2s The duration the clients need to wait to start leader election again. --node-monitor-period duration 5s The interval at which the NodeController monitors nodes. --node-monitor-grace-period duration 40s The amount of time the NodeController will continue to monitor a node before marking it as unhealthy. --pod-eviction-timeout duration 5m The grace period for deleting pods on a node that is terminating. --service-account-private-key-file string Path to the private key file used to sign service account tokens. --root-ca-file string If set, this root certificate authority will be used for verifying the API server during connection. --enable-garbage-collector bool true Whether to enable the garbage collector. --term-container-health-check-timeout duration 1m Timeout for container health checks during termination.","title":"Command-line Flags"},{"location":"docs/kube-controller-manager/configuration.html#example-configuration-command-line","text":"```bash kube-controller-manager \\ --kubeconfig=/etc/kubernetes/controller-manager.conf \\ --cluster-name=my-k8s-cluster \\ --controllers=* \\ --leader-election-lease-duration=15s \\ --node-monitor-period=5s \\ --node-monitor-grace-period=40s \\ --service-account-private-key-file=/etc/kubernetes/pki/sa.key \\ --root-ca-file=/etc/kubernetes/pki/ca.crt \\ --enable-leader-election=true ```","title":"Example Configuration (Command-line)"},{"location":"docs/kube-controller-manager/configuration.html#environment-variables","text":"While primarily configured via flags, some underlying libraries or components might respect environment variables for configuration (e.g., KUBECONFIG ).","title":"Environment Variables"},{"location":"docs/kube-controller-manager/configuration.html#source-files","text":"Flag definitions are typically found in the cmd/kube-controller-manager/ directory. The logic for applying these configurations is within the main controller-manager.go file and related setup functions.","title":"Source Files"},{"location":"docs/kube-proxy/index.html","text":"Kube-Proxy Component Documentation Overview Kube-proxy is a crucial network proxy that runs on each node in a Kubernetes cluster. Its primary responsibility is to handle network rules on nodes, allowing network communication to your Services from inside or outside of your cluster. Kube-proxy maintains network rules on nodes, which perform connection forwarding. These rules allow network communication to your Services from inside or outside of your cluster. Kube-proxy has a number of features: Service Proxying : It intercepts traffic destined for Service IP addresses and ports and forwards it to the appropriate backing Pod. Load Balancing : It distributes traffic across multiple Pods that back a Service. Network Rule Management : It configures network rules on the node using different strategies like iptables, IPVS, or eBPF. Service Discovery : It watches for changes in Service and EndpointSlices/Endpoints objects and updates the network rules accordingly. Getting Started To get started with understanding kube-proxy, refer to the following sections: Architecture Configuration Sub-components Key Concepts Services Kubernetes Services provide a stable IP address and DNS name for a set of Pods. Kube-proxy ensures that traffic directed to a Service IP is correctly routed to one of its backing Pods. Endpoints/EndpointSlices These objects represent the network endpoints (IP addresses and ports) of the Pods that back a Service. Kube-proxy watches these objects to know where to forward traffic. Network Strategies Kube-proxy supports various strategies for managing network rules: - iptables : A widely used Linux packet filtering framework. - IPVS : (IP Virtual Server) A more performant and scalable load-balancing solution built into the Linux kernel. - eBPF : (Extended Berkeley Packet Filter) A newer technology that allows running sandboxed programs in the Linux kernel, offering high performance and flexibility for networking tasks. Sub-components Kube-proxy is composed of several key sub-components, each with a specific role: apis : Handles communication with the Kubernetes API server to watch for Service and Endpoint/EndpointSlice changes. config : Manages the configuration of kube-proxy, including the chosen network strategy. conntrack : Manages connection tracking, crucial for stateful network traffic. iptables : Implements the iptables-based network rule management. ipvs : Implements the IPVS-based network rule management. nftables : Implements the nftables-based network rule management. metrics : Exposes metrics about kube-proxy's performance and behavior. runner : Manages the lifecycle of kube-proxy. servicechangetracker : Tracks changes to Services. endpointschangetracker : Tracks changes to EndpointSlices. util : Utility functions used across different parts of kube-proxy. winkernel : Windows-specific kernel implementations (if applicable). Architecture Link to Architecture Documentation Configuration Link to Configuration Documentation Code Examples Watching Services and Endpoints The following snippet demonstrates how kube-proxy might watch for Service and Endpoint changes using the Kubernetes API. // Simplified example of watching Services svcClient, err := clientset.CoreV1().Services(namespace).Watch(ctx, metav1.ListOptions{}) if err != nil { // Handle error } for event := range svcClient.ResultChan() { // Process Service event } // Simplified example of watching EndpointSlices epSliceClient, err := clientset.DiscoveryV1().EndpointSlices(namespace).Watch(ctx, metav1.ListOptions{}) if err != nil { // Handle error } for event := range epSliceClient.ResultChan() { // Process EndpointSlice event } iptables Rule Management An example of how kube-proxy might configure iptables rules for a Service. # Example iptables rule for a Service # This rule DNATs traffic destined for the Service IP to a Pod IP iptables -t nat -A KUBE-SERVICES -m comment --comment \"kube-service: default/my-service\" -m set --match-set KUBE_MARKERS src -j KUBE-REDIRECT IPVS Load Balancing An example of how kube-proxy might configure IPVS for a Service. // Simplified example of adding an IPVS service and real servers iface := \"eth0\" // Example interface serviceIP := net.ParseIP(\"10.0.0.1\") servicePort := uint16(80) realServerIPs := []net.IP{net.ParseIP(\"192.168.1.10\"), net.ParseIP(\"192.168.1.11\")} // Add IPVS service err := ipvs.NewService(serviceIP, servicePort, socket.ProtocolTCP).AddService() if err != nil { // Handle error } // Add real servers to the service for _, rsIP := range realServerIPs { err := ipvs.NewService(serviceIP, servicePort, socket.ProtocolTCP).AddRealServer(rsIP, 10) // 10 is weight if err != nil { // Handle error } }","title":"Kube-Proxy Component Documentation"},{"location":"docs/kube-proxy/index.html#kube-proxy-component-documentation","text":"","title":"Kube-Proxy Component Documentation"},{"location":"docs/kube-proxy/index.html#overview","text":"Kube-proxy is a crucial network proxy that runs on each node in a Kubernetes cluster. Its primary responsibility is to handle network rules on nodes, allowing network communication to your Services from inside or outside of your cluster. Kube-proxy maintains network rules on nodes, which perform connection forwarding. These rules allow network communication to your Services from inside or outside of your cluster. Kube-proxy has a number of features: Service Proxying : It intercepts traffic destined for Service IP addresses and ports and forwards it to the appropriate backing Pod. Load Balancing : It distributes traffic across multiple Pods that back a Service. Network Rule Management : It configures network rules on the node using different strategies like iptables, IPVS, or eBPF. Service Discovery : It watches for changes in Service and EndpointSlices/Endpoints objects and updates the network rules accordingly.","title":"Overview"},{"location":"docs/kube-proxy/index.html#getting-started","text":"To get started with understanding kube-proxy, refer to the following sections: Architecture Configuration Sub-components","title":"Getting Started"},{"location":"docs/kube-proxy/index.html#key-concepts","text":"","title":"Key Concepts"},{"location":"docs/kube-proxy/index.html#services","text":"Kubernetes Services provide a stable IP address and DNS name for a set of Pods. Kube-proxy ensures that traffic directed to a Service IP is correctly routed to one of its backing Pods.","title":"Services"},{"location":"docs/kube-proxy/index.html#endpointsendpointslices","text":"These objects represent the network endpoints (IP addresses and ports) of the Pods that back a Service. Kube-proxy watches these objects to know where to forward traffic.","title":"Endpoints/EndpointSlices"},{"location":"docs/kube-proxy/index.html#network-strategies","text":"Kube-proxy supports various strategies for managing network rules: - iptables : A widely used Linux packet filtering framework. - IPVS : (IP Virtual Server) A more performant and scalable load-balancing solution built into the Linux kernel. - eBPF : (Extended Berkeley Packet Filter) A newer technology that allows running sandboxed programs in the Linux kernel, offering high performance and flexibility for networking tasks.","title":"Network Strategies"},{"location":"docs/kube-proxy/index.html#sub-components","text":"Kube-proxy is composed of several key sub-components, each with a specific role: apis : Handles communication with the Kubernetes API server to watch for Service and Endpoint/EndpointSlice changes. config : Manages the configuration of kube-proxy, including the chosen network strategy. conntrack : Manages connection tracking, crucial for stateful network traffic. iptables : Implements the iptables-based network rule management. ipvs : Implements the IPVS-based network rule management. nftables : Implements the nftables-based network rule management. metrics : Exposes metrics about kube-proxy's performance and behavior. runner : Manages the lifecycle of kube-proxy. servicechangetracker : Tracks changes to Services. endpointschangetracker : Tracks changes to EndpointSlices. util : Utility functions used across different parts of kube-proxy. winkernel : Windows-specific kernel implementations (if applicable).","title":"Sub-components"},{"location":"docs/kube-proxy/index.html#architecture","text":"Link to Architecture Documentation","title":"Architecture"},{"location":"docs/kube-proxy/index.html#configuration","text":"Link to Configuration Documentation","title":"Configuration"},{"location":"docs/kube-proxy/index.html#code-examples","text":"","title":"Code Examples"},{"location":"docs/kube-proxy/index.html#watching-services-and-endpoints","text":"The following snippet demonstrates how kube-proxy might watch for Service and Endpoint changes using the Kubernetes API. // Simplified example of watching Services svcClient, err := clientset.CoreV1().Services(namespace).Watch(ctx, metav1.ListOptions{}) if err != nil { // Handle error } for event := range svcClient.ResultChan() { // Process Service event } // Simplified example of watching EndpointSlices epSliceClient, err := clientset.DiscoveryV1().EndpointSlices(namespace).Watch(ctx, metav1.ListOptions{}) if err != nil { // Handle error } for event := range epSliceClient.ResultChan() { // Process EndpointSlice event }","title":"Watching Services and Endpoints"},{"location":"docs/kube-proxy/index.html#iptables-rule-management","text":"An example of how kube-proxy might configure iptables rules for a Service. # Example iptables rule for a Service # This rule DNATs traffic destined for the Service IP to a Pod IP iptables -t nat -A KUBE-SERVICES -m comment --comment \"kube-service: default/my-service\" -m set --match-set KUBE_MARKERS src -j KUBE-REDIRECT","title":"iptables Rule Management"},{"location":"docs/kube-proxy/index.html#ipvs-load-balancing","text":"An example of how kube-proxy might configure IPVS for a Service. // Simplified example of adding an IPVS service and real servers iface := \"eth0\" // Example interface serviceIP := net.ParseIP(\"10.0.0.1\") servicePort := uint16(80) realServerIPs := []net.IP{net.ParseIP(\"192.168.1.10\"), net.ParseIP(\"192.168.1.11\")} // Add IPVS service err := ipvs.NewService(serviceIP, servicePort, socket.ProtocolTCP).AddService() if err != nil { // Handle error } // Add real servers to the service for _, rsIP := range realServerIPs { err := ipvs.NewService(serviceIP, servicePort, socket.ProtocolTCP).AddRealServer(rsIP, 10) // 10 is weight if err != nil { // Handle error } }","title":"IPVS Load Balancing"},{"location":"docs/kube-proxy/api_reference.html","text":"Kube-Proxy API Reference Overview Kube-proxy does not expose a direct API for external consumption in the same way a typical web service does. Instead, its \"API\" refers to the Kubernetes API resources it interacts with and the internal interfaces it uses for its different network proxying modes. Kubernetes API Resources Kube-proxy primarily interacts with the following Kubernetes API resources: Services : Defines a logical set of Pods and a policy by which to access them. Kube-proxy watches these to understand the desired state of network services. Endpoints / EndpointSlices : Lists the actual network endpoints (IP addresses and ports) of the Pods backing a Service. Kube-proxy uses this information to program the correct network rules. You can interact with these resources using kubectl or the Kubernetes client libraries. Internal Interfaces Kube-proxy defines internal interfaces for its various proxying modes to allow for flexibility and testing. The most notable ones are: Proxy Interface : Defines the core Sync method responsible for synchronizing the state of Services and Endpoints with the node's network rules. Proxier Interface : A more specific interface, often implemented by iptables.Proxy or ipvs.Proxy , which handles the direct manipulation of network rules. Interaction with Kubernetes API Kube-proxy uses the standard Kubernetes client-go libraries to watch and list Services and EndpointSlices. Watching Services Kube-proxy uses clientset.CoreV1().Services(namespace).Watch(...) to get real-time updates. // Example using client-go to watch Services import ( metav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\" \"k8s.io/client-go/kubernetes\" \"k8s.io/client-go/tools/clientcmd\" // ... other imports ) func watchServices(ctx context.Context, clientset kubernetes.Interface) { serviceWatcher, err := clientset.CoreV1().Services(\"default\").Watch(ctx, metav1.ListOptions{}) if err != nil { // Handle error return } defer serviceWatcher.Stop() for event := range serviceWatcher.ResultChan() { // event.Type will be Added, Modified, or Deleted // event.Object will be of type *v1.Service // Process the event fmt.Printf(\"Received Service event: %s for %s/%s\\n\", event.Type, event.Object.(*v1.Service).Namespace, event.Object.(*v1.Service).Name) } } Watching EndpointSlices Similarly, it watches EndpointSlices using clientset.DiscoveryV1().EndpointSlices(namespace).Watch(...) . // Example using client-go to watch EndpointSlices import ( metav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\" \"k8s.io/client-go/kubernetes\" // ... other imports ) func watchEndpointSlices(ctx context.Context, clientset kubernetes.Interface) { epSliceWatcher, err := clientset.DiscoveryV1().EndpointSlices(\"default\").Watch(ctx, metav1.ListOptions{}) if err != nil { // Handle error return } defer epSliceWatcher.Stop() for event := range epSliceWatcher.ResultChan() { // event.Type will be Added, Modified, or Deleted // event.Object will be of type *discoveryv1.EndpointSlice // Process the event fmt.Printf(\"Received EndpointSlice event: %s for %s/%s\\n\", event.Type, event.Object.(*discoveryv1.EndpointSlice).Namespace, event.Object.(*discoveryv1.EndpointSlice).Name) } } Source Files Core Proxy Logic : pkg/proxy/proxy.go (contains the main Proxy interface and implementation) iptables Implementation : pkg/proxy/iptables/iptables.go IPVS Implementation : pkg/proxy/ipvs/ipvs.go EndpointSlice Handling : pkg/proxy/endpointslicecache/endpointslicecache.go Service/Endpoint Change Tracking : pkg/proxy/servicechangetracker.go , pkg/proxy/endpointschangetracker.go Code Example: Syncing Rules A simplified representation of the Sync method, which is the core of kube-proxy's operation. // Inside pkg/proxy/proxier.go (conceptual) type Proxier interface { // Sync is called periodically and upon receipt of Service/Endpoint changes. // The Proxier should not block. Sync(services []*v1.Service, endpoints []*v1.Endpoints) // ... other methods like OnServiceAdded, OnServiceDeleted, etc. } func (ipt *iptablesProxy) Sync(services []*v1.Service, endpoints []*v1.Endpoints) { // 1. Create a map of endpoints keyed by Service name. // 2. Iterate through services. // 3. For each service, generate the necessary iptables rules. // 4. Apply the rules to the system. // 5. Clean up stale rules. fmt.Printf(\"Syncing %d services and %d endpoints\\n\", len(services), len(endpoints)) // ... implementation details ... }","title":"Kube-Proxy API Reference"},{"location":"docs/kube-proxy/api_reference.html#kube-proxy-api-reference","text":"","title":"Kube-Proxy API Reference"},{"location":"docs/kube-proxy/api_reference.html#overview","text":"Kube-proxy does not expose a direct API for external consumption in the same way a typical web service does. Instead, its \"API\" refers to the Kubernetes API resources it interacts with and the internal interfaces it uses for its different network proxying modes.","title":"Overview"},{"location":"docs/kube-proxy/api_reference.html#kubernetes-api-resources","text":"Kube-proxy primarily interacts with the following Kubernetes API resources: Services : Defines a logical set of Pods and a policy by which to access them. Kube-proxy watches these to understand the desired state of network services. Endpoints / EndpointSlices : Lists the actual network endpoints (IP addresses and ports) of the Pods backing a Service. Kube-proxy uses this information to program the correct network rules. You can interact with these resources using kubectl or the Kubernetes client libraries.","title":"Kubernetes API Resources"},{"location":"docs/kube-proxy/api_reference.html#internal-interfaces","text":"Kube-proxy defines internal interfaces for its various proxying modes to allow for flexibility and testing. The most notable ones are: Proxy Interface : Defines the core Sync method responsible for synchronizing the state of Services and Endpoints with the node's network rules. Proxier Interface : A more specific interface, often implemented by iptables.Proxy or ipvs.Proxy , which handles the direct manipulation of network rules.","title":"Internal Interfaces"},{"location":"docs/kube-proxy/api_reference.html#interaction-with-kubernetes-api","text":"Kube-proxy uses the standard Kubernetes client-go libraries to watch and list Services and EndpointSlices.","title":"Interaction with Kubernetes API"},{"location":"docs/kube-proxy/api_reference.html#watching-services","text":"Kube-proxy uses clientset.CoreV1().Services(namespace).Watch(...) to get real-time updates. // Example using client-go to watch Services import ( metav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\" \"k8s.io/client-go/kubernetes\" \"k8s.io/client-go/tools/clientcmd\" // ... other imports ) func watchServices(ctx context.Context, clientset kubernetes.Interface) { serviceWatcher, err := clientset.CoreV1().Services(\"default\").Watch(ctx, metav1.ListOptions{}) if err != nil { // Handle error return } defer serviceWatcher.Stop() for event := range serviceWatcher.ResultChan() { // event.Type will be Added, Modified, or Deleted // event.Object will be of type *v1.Service // Process the event fmt.Printf(\"Received Service event: %s for %s/%s\\n\", event.Type, event.Object.(*v1.Service).Namespace, event.Object.(*v1.Service).Name) } }","title":"Watching Services"},{"location":"docs/kube-proxy/api_reference.html#watching-endpointslices","text":"Similarly, it watches EndpointSlices using clientset.DiscoveryV1().EndpointSlices(namespace).Watch(...) . // Example using client-go to watch EndpointSlices import ( metav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\" \"k8s.io/client-go/kubernetes\" // ... other imports ) func watchEndpointSlices(ctx context.Context, clientset kubernetes.Interface) { epSliceWatcher, err := clientset.DiscoveryV1().EndpointSlices(\"default\").Watch(ctx, metav1.ListOptions{}) if err != nil { // Handle error return } defer epSliceWatcher.Stop() for event := range epSliceWatcher.ResultChan() { // event.Type will be Added, Modified, or Deleted // event.Object will be of type *discoveryv1.EndpointSlice // Process the event fmt.Printf(\"Received EndpointSlice event: %s for %s/%s\\n\", event.Type, event.Object.(*discoveryv1.EndpointSlice).Namespace, event.Object.(*discoveryv1.EndpointSlice).Name) } }","title":"Watching EndpointSlices"},{"location":"docs/kube-proxy/api_reference.html#source-files","text":"Core Proxy Logic : pkg/proxy/proxy.go (contains the main Proxy interface and implementation) iptables Implementation : pkg/proxy/iptables/iptables.go IPVS Implementation : pkg/proxy/ipvs/ipvs.go EndpointSlice Handling : pkg/proxy/endpointslicecache/endpointslicecache.go Service/Endpoint Change Tracking : pkg/proxy/servicechangetracker.go , pkg/proxy/endpointschangetracker.go","title":"Source Files"},{"location":"docs/kube-proxy/api_reference.html#code-example-syncing-rules","text":"A simplified representation of the Sync method, which is the core of kube-proxy's operation. // Inside pkg/proxy/proxier.go (conceptual) type Proxier interface { // Sync is called periodically and upon receipt of Service/Endpoint changes. // The Proxier should not block. Sync(services []*v1.Service, endpoints []*v1.Endpoints) // ... other methods like OnServiceAdded, OnServiceDeleted, etc. } func (ipt *iptablesProxy) Sync(services []*v1.Service, endpoints []*v1.Endpoints) { // 1. Create a map of endpoints keyed by Service name. // 2. Iterate through services. // 3. For each service, generate the necessary iptables rules. // 4. Apply the rules to the system. // 5. Clean up stale rules. fmt.Printf(\"Syncing %d services and %d endpoints\\n\", len(services), len(endpoints)) // ... implementation details ... }","title":"Code Example: Syncing Rules"},{"location":"docs/kube-proxy/architecture.html","text":"Kube-Proxy Architecture System Context Kube-proxy operates as a daemon on each Kubernetes node, interacting with the node's network stack and the Kubernetes control plane. It is essential for enabling Service abstraction within the cluster. graph TD A[Kubelet] --> B(Kube-Proxy on Node); C[API Server] --> D{Kube-Proxy Watchers}; D --> B; B --> E{Node Network Stack (iptables/IPVS)}; F[External Client] --> E; G[Internal Client] --> E; E --> H[Service]; H --> I[Pod]; Internal Architecture Kube-proxy's internal architecture is modular, with different components handling various aspects of its operation. The core logic revolves around watching for changes in Services and Endpoints/EndpointSlices and then programming the node's network rules accordingly. graph TD subgraph Kube-Proxy Process A[API Watchers] --> B(Core Logic); B --> C{Network Strategy}; C -- iptables --> D[iptables Manager]; C -- IPVS --> E[IPVS Manager]; C -- nftables --> F[nftables Manager]; B --> G[Metrics Exporter]; B --> H[Connection Tracking]; end subgraph Node Network Stack D --> I(iptables Rules); E --> J(IPVS Rules); F --> K(nftables Rules); end L[Kubernetes API Server] --> A; M[Service Object] --> A; N[EndpointSlice Object] --> A; I --> O{Traffic Forwarding}; J --> O; K --> O; O --> P[Target Pod]; Data Flow API Watchers : Kube-proxy starts watchers to monitor Service and EndpointSlice resources in the Kubernetes API. Update Detection : When a Service or EndpointSlice changes, the watchers detect the event. Core Logic : The core logic processes these changes, determining the necessary updates to network rules. Network Strategy : Based on the configured strategy (e.g., iptables , ipvs ), the corresponding manager is invoked. Rule Programming : The manager programs the node's network rules (e.g., adds/removes iptables rules or configures IPVS entries). Traffic Forwarding : The node's network stack, now programmed by kube-proxy, intercepts traffic destined for Service IPs and forwards it to the appropriate Pods. Component Interactions Kubelet : While not directly interacting with kube-proxy's core logic, Kubelet is responsible for running kube-proxy as a daemon on each node. Kubernetes API Server : Kube-proxy heavily relies on the API server to watch for Service and EndpointSlice updates. Node Network Stack : This is the primary target of kube-proxy's operations. Kube-proxy configures iptables , IPVS , or nftables on the node. Network Rule Management Kube-proxy supports several network rule management backends: iptables Pros : Ubiquitous, well-understood. Cons : Can become slow with a large number of Services/rules. Source : pkg/proxy/iptables/ IPVS Pros : Higher performance and scalability compared to iptables, especially for load balancing. Cons : Requires IPVS kernel modules to be loaded. Source : pkg/proxy/ipvs/ nftables Pros : Modern, more powerful, and efficient replacement for iptables. Cons : Newer, adoption might be less widespread than iptables. Source : pkg/proxy/nftables/ Load Balancing Kube-proxy implements load balancing for Services. The chosen network strategy dictates how this is achieved: - iptables : Uses various iptables mechanisms (e.g., statistic module with random or sh ) to select a backend Pod. - IPVS : Offers robust load-balancing algorithms (e.g., round-robin, least connection) natively. Code Examples Example: iptables rule creation (conceptual) This shows a high-level view of how an iptables rule might be generated. // Inside pkg/proxy/iptables/rules.go (conceptual) func (t *iptablesProxy) syncProxyRules() { // ... get services and endpoints ... for _, svc := range services { // ... determine rule spec ... rule := createServiceRule(svc) t.executables.NewRule().Append(\"nat\", \"PREROUTING\", rule.Spec...) // ... add rules for load balancing targets ... } // ... flush outdated rules ... } Example: IPVS configuration (conceptual) Illustrates adding a virtual service and real servers in IPVS. // Inside pkg/proxy/ipvs/conntrack.go (conceptual) func (p *IPVSProxy) Sync(services []*v1.Service, endpoints []*v1.Endpoints) error { // ... iterate over services and endpoints ... for _, svc := range services { // ... get service details (IP, port) ... // Add or update IPVS service entry err := ipvs.NewService(svcIP, uint16(svcPort), socket.ProtocolTCP).AddService() if err != nil { return err } // Add or update real server entries for each endpoint for _, ep := range endpoints { // ... get endpoint IP and port ... err := ipvs.NewService(svcIP, uint16(svcPort), socket.ProtocolTCP).AddRealServer(epIP, weight) if err != nil { return err } } } // ... remove stale entries ... return nil } Example: EndpointSlice Cache Kube-proxy utilizes caches to efficiently track EndpointSlices. // Inside pkg/proxy/endpointslicecache/endpointslicecache.go (conceptual) type EndpointSliceCache struct { sync.RWMutex cache map[string]*discoveryv1.EndpointSlice // ... other fields ... } func (e *EndpointSliceCache) Add(obj interface{}) { // ... add EndpointSlice to cache ... } func (e *EndpointSliceCache) Update(oldObj, newObj interface{}) { // ... update EndpointSlice in cache ... } func (e *EndpointSliceCache) Delete(obj interface{}) { // ... remove EndpointSlice from cache ... }","title":"Kube-Proxy Architecture"},{"location":"docs/kube-proxy/architecture.html#kube-proxy-architecture","text":"","title":"Kube-Proxy Architecture"},{"location":"docs/kube-proxy/architecture.html#system-context","text":"Kube-proxy operates as a daemon on each Kubernetes node, interacting with the node's network stack and the Kubernetes control plane. It is essential for enabling Service abstraction within the cluster. graph TD A[Kubelet] --> B(Kube-Proxy on Node); C[API Server] --> D{Kube-Proxy Watchers}; D --> B; B --> E{Node Network Stack (iptables/IPVS)}; F[External Client] --> E; G[Internal Client] --> E; E --> H[Service]; H --> I[Pod];","title":"System Context"},{"location":"docs/kube-proxy/architecture.html#internal-architecture","text":"Kube-proxy's internal architecture is modular, with different components handling various aspects of its operation. The core logic revolves around watching for changes in Services and Endpoints/EndpointSlices and then programming the node's network rules accordingly. graph TD subgraph Kube-Proxy Process A[API Watchers] --> B(Core Logic); B --> C{Network Strategy}; C -- iptables --> D[iptables Manager]; C -- IPVS --> E[IPVS Manager]; C -- nftables --> F[nftables Manager]; B --> G[Metrics Exporter]; B --> H[Connection Tracking]; end subgraph Node Network Stack D --> I(iptables Rules); E --> J(IPVS Rules); F --> K(nftables Rules); end L[Kubernetes API Server] --> A; M[Service Object] --> A; N[EndpointSlice Object] --> A; I --> O{Traffic Forwarding}; J --> O; K --> O; O --> P[Target Pod];","title":"Internal Architecture"},{"location":"docs/kube-proxy/architecture.html#data-flow","text":"API Watchers : Kube-proxy starts watchers to monitor Service and EndpointSlice resources in the Kubernetes API. Update Detection : When a Service or EndpointSlice changes, the watchers detect the event. Core Logic : The core logic processes these changes, determining the necessary updates to network rules. Network Strategy : Based on the configured strategy (e.g., iptables , ipvs ), the corresponding manager is invoked. Rule Programming : The manager programs the node's network rules (e.g., adds/removes iptables rules or configures IPVS entries). Traffic Forwarding : The node's network stack, now programmed by kube-proxy, intercepts traffic destined for Service IPs and forwards it to the appropriate Pods.","title":"Data Flow"},{"location":"docs/kube-proxy/architecture.html#component-interactions","text":"Kubelet : While not directly interacting with kube-proxy's core logic, Kubelet is responsible for running kube-proxy as a daemon on each node. Kubernetes API Server : Kube-proxy heavily relies on the API server to watch for Service and EndpointSlice updates. Node Network Stack : This is the primary target of kube-proxy's operations. Kube-proxy configures iptables , IPVS , or nftables on the node.","title":"Component Interactions"},{"location":"docs/kube-proxy/architecture.html#network-rule-management","text":"Kube-proxy supports several network rule management backends:","title":"Network Rule Management"},{"location":"docs/kube-proxy/architecture.html#iptables","text":"Pros : Ubiquitous, well-understood. Cons : Can become slow with a large number of Services/rules. Source : pkg/proxy/iptables/","title":"iptables"},{"location":"docs/kube-proxy/architecture.html#ipvs","text":"Pros : Higher performance and scalability compared to iptables, especially for load balancing. Cons : Requires IPVS kernel modules to be loaded. Source : pkg/proxy/ipvs/","title":"IPVS"},{"location":"docs/kube-proxy/architecture.html#nftables","text":"Pros : Modern, more powerful, and efficient replacement for iptables. Cons : Newer, adoption might be less widespread than iptables. Source : pkg/proxy/nftables/","title":"nftables"},{"location":"docs/kube-proxy/architecture.html#load-balancing","text":"Kube-proxy implements load balancing for Services. The chosen network strategy dictates how this is achieved: - iptables : Uses various iptables mechanisms (e.g., statistic module with random or sh ) to select a backend Pod. - IPVS : Offers robust load-balancing algorithms (e.g., round-robin, least connection) natively.","title":"Load Balancing"},{"location":"docs/kube-proxy/architecture.html#code-examples","text":"","title":"Code Examples"},{"location":"docs/kube-proxy/architecture.html#example-iptables-rule-creation-conceptual","text":"This shows a high-level view of how an iptables rule might be generated. // Inside pkg/proxy/iptables/rules.go (conceptual) func (t *iptablesProxy) syncProxyRules() { // ... get services and endpoints ... for _, svc := range services { // ... determine rule spec ... rule := createServiceRule(svc) t.executables.NewRule().Append(\"nat\", \"PREROUTING\", rule.Spec...) // ... add rules for load balancing targets ... } // ... flush outdated rules ... }","title":"Example: iptables rule creation (conceptual)"},{"location":"docs/kube-proxy/architecture.html#example-ipvs-configuration-conceptual","text":"Illustrates adding a virtual service and real servers in IPVS. // Inside pkg/proxy/ipvs/conntrack.go (conceptual) func (p *IPVSProxy) Sync(services []*v1.Service, endpoints []*v1.Endpoints) error { // ... iterate over services and endpoints ... for _, svc := range services { // ... get service details (IP, port) ... // Add or update IPVS service entry err := ipvs.NewService(svcIP, uint16(svcPort), socket.ProtocolTCP).AddService() if err != nil { return err } // Add or update real server entries for each endpoint for _, ep := range endpoints { // ... get endpoint IP and port ... err := ipvs.NewService(svcIP, uint16(svcPort), socket.ProtocolTCP).AddRealServer(epIP, weight) if err != nil { return err } } } // ... remove stale entries ... return nil }","title":"Example: IPVS configuration (conceptual)"},{"location":"docs/kube-proxy/architecture.html#example-endpointslice-cache","text":"Kube-proxy utilizes caches to efficiently track EndpointSlices. // Inside pkg/proxy/endpointslicecache/endpointslicecache.go (conceptual) type EndpointSliceCache struct { sync.RWMutex cache map[string]*discoveryv1.EndpointSlice // ... other fields ... } func (e *EndpointSliceCache) Add(obj interface{}) { // ... add EndpointSlice to cache ... } func (e *EndpointSliceCache) Update(oldObj, newObj interface{}) { // ... update EndpointSlice in cache ... } func (e *EndpointSliceCache) Delete(obj interface{}) { // ... remove EndpointSlice from cache ... }","title":"Example: EndpointSlice Cache"},{"location":"docs/kube-proxy/configuration.html","text":"Kube-Proxy Configuration Overview Kube-proxy's behavior is primarily controlled by command-line flags and a configuration file. The configuration allows you to specify the network proxying mode, logging verbosity, and other operational parameters. Configuration File Kube-proxy can be configured using a YAML file. The path to this file is typically passed as a command-line argument ( --config ). Example kube-proxy-config.yaml : apiVersion: kubeproxy.config.k8s.io/v1alpha1 kind: KubeProxyConfiguration bindAddress: 0.0.0.0 clusterCIDR: 10.244.0.0/16 healthzBindAddress: 127.0.0.1:10256 kubeconfig: \"\" mode: \"iptables\" metricsBindAddress: 0.0.0.0:10249 clientConnection: kubeconfig: \"\" acceptContentTypes: \"\" contentType: \"application/vnd.kubernetes.protobuf\" qps: 50 burst: 100 logging: verbosity: 0 flushFrequency: 5s options: json: false prettyText: false nodePortAddresses: - 192.168.0.0/16 - 10.0.0.0/8 featureGates: {} Configuration Options Option Type Default Description Source File(s) apiVersion string v1alpha1 API version of the KubeProxyConfiguration object. pkg/proxy/config/types.go kind string KubeProxyConfiguration Kind of the configuration object. pkg/proxy/config/types.go bindAddress string 0.0.0.0 The IP address that the local ports (e.g. health check, metrics) will be bound to. pkg/proxy/config/types.go clusterCIDR string The primary config source for cluster-wide IP addresses, e.g. 10.244.0.0/16 . Used to differentiate cluster traffic from external traffic. pkg/proxy/config/types.go healthzBindAddress string 127.0.0.1:10256 The IP address that the health check server will be bound to. Set to 0.0.0.0 to expose on all interfaces. pkg/proxy/config/types.go kubeconfig string \"\" Path to a kubeconfig file. If empty, will load default config or use in-cluster config. pkg/proxy/config/types.go mode string iptables The mode kube-proxy will use for networking rules. Options: iptables , ipvs , kernelproxy , userspace (deprecated). pkg/proxy/config/types.go metricsBindAddress string 0.0.0.0:10249 The IP address that the metric server will be bound to. Set to 0.0.0.0 to expose on all interfaces. pkg/proxy/config/types.go clientConnection object Configuration for the connection to the Kubernetes API server. pkg/proxy/config/types.go clientConnection.qps float64 50 Queries per second limit for the client connection. pkg/proxy/config/types.go clientConnection.burst float64 100 Burst limit for the client connection. pkg/proxy/config/types.go nodePortAddresses []string List of CIDRs that contain IP addresses representing the node, used for NodePort Services. pkg/proxy/config/types.go featureGates map {} A map of feature gates to enable/disable features. pkg/proxy/config/types.go Command-line Flags In addition to the configuration file, kube-proxy accepts several command-line flags that can override or supplement the configuration file settings. Common flags include: --config <path> : Path to the kube-proxy configuration file. --cluster-cidr=<cidr> : The cluster CIDR. --healthz-bind-address=<ip:port> : The address to bind the healthz server to. --kubeconfig=<path> : Path to the kubeconfig file. --logtostderr : Log to stderr instead of files. --v=<level> : Set the verbosity level of logs. --proxy-mode=<mode> : The network proxying mode ( iptables , ipvs , userspace ). Code Examples Example 1: Specifying Configuration File Running kube-proxy with a specific configuration file: kube-proxy --config=/path/to/your/kube-proxy-config.yaml Example 2: Overriding Mode via Flag Running kube-proxy with the IPVS mode, overriding any setting in a config file: kube-proxy --proxy-mode=ipvs Example 3: Setting Log Verbosity Running kube-proxy and logging debug information (level 4): kube-proxy --v=4 --logtostderr Sub-components and Configuration The pkg/proxy/config/types.go file defines the structure of the KubeProxyConfiguration and related types. The pkg/proxy/util/kernel_linux.go and pkg/proxy/util/userspace.go files contain utilities related to kernel and userspace interactions, which are influenced by the configuration. The choice of mode (e.g., iptables , ipvs ) dictates which specific proxy implementation ( pkg/proxy/iptables/ or pkg/proxy/ipvs/ ) is used, significantly impacting how network rules are managed.","title":"Kube-Proxy Configuration"},{"location":"docs/kube-proxy/configuration.html#kube-proxy-configuration","text":"","title":"Kube-Proxy Configuration"},{"location":"docs/kube-proxy/configuration.html#overview","text":"Kube-proxy's behavior is primarily controlled by command-line flags and a configuration file. The configuration allows you to specify the network proxying mode, logging verbosity, and other operational parameters.","title":"Overview"},{"location":"docs/kube-proxy/configuration.html#configuration-file","text":"Kube-proxy can be configured using a YAML file. The path to this file is typically passed as a command-line argument ( --config ). Example kube-proxy-config.yaml : apiVersion: kubeproxy.config.k8s.io/v1alpha1 kind: KubeProxyConfiguration bindAddress: 0.0.0.0 clusterCIDR: 10.244.0.0/16 healthzBindAddress: 127.0.0.1:10256 kubeconfig: \"\" mode: \"iptables\" metricsBindAddress: 0.0.0.0:10249 clientConnection: kubeconfig: \"\" acceptContentTypes: \"\" contentType: \"application/vnd.kubernetes.protobuf\" qps: 50 burst: 100 logging: verbosity: 0 flushFrequency: 5s options: json: false prettyText: false nodePortAddresses: - 192.168.0.0/16 - 10.0.0.0/8 featureGates: {}","title":"Configuration File"},{"location":"docs/kube-proxy/configuration.html#configuration-options","text":"Option Type Default Description Source File(s) apiVersion string v1alpha1 API version of the KubeProxyConfiguration object. pkg/proxy/config/types.go kind string KubeProxyConfiguration Kind of the configuration object. pkg/proxy/config/types.go bindAddress string 0.0.0.0 The IP address that the local ports (e.g. health check, metrics) will be bound to. pkg/proxy/config/types.go clusterCIDR string The primary config source for cluster-wide IP addresses, e.g. 10.244.0.0/16 . Used to differentiate cluster traffic from external traffic. pkg/proxy/config/types.go healthzBindAddress string 127.0.0.1:10256 The IP address that the health check server will be bound to. Set to 0.0.0.0 to expose on all interfaces. pkg/proxy/config/types.go kubeconfig string \"\" Path to a kubeconfig file. If empty, will load default config or use in-cluster config. pkg/proxy/config/types.go mode string iptables The mode kube-proxy will use for networking rules. Options: iptables , ipvs , kernelproxy , userspace (deprecated). pkg/proxy/config/types.go metricsBindAddress string 0.0.0.0:10249 The IP address that the metric server will be bound to. Set to 0.0.0.0 to expose on all interfaces. pkg/proxy/config/types.go clientConnection object Configuration for the connection to the Kubernetes API server. pkg/proxy/config/types.go clientConnection.qps float64 50 Queries per second limit for the client connection. pkg/proxy/config/types.go clientConnection.burst float64 100 Burst limit for the client connection. pkg/proxy/config/types.go nodePortAddresses []string List of CIDRs that contain IP addresses representing the node, used for NodePort Services. pkg/proxy/config/types.go featureGates map {} A map of feature gates to enable/disable features. pkg/proxy/config/types.go","title":"Configuration Options"},{"location":"docs/kube-proxy/configuration.html#command-line-flags","text":"In addition to the configuration file, kube-proxy accepts several command-line flags that can override or supplement the configuration file settings. Common flags include: --config <path> : Path to the kube-proxy configuration file. --cluster-cidr=<cidr> : The cluster CIDR. --healthz-bind-address=<ip:port> : The address to bind the healthz server to. --kubeconfig=<path> : Path to the kubeconfig file. --logtostderr : Log to stderr instead of files. --v=<level> : Set the verbosity level of logs. --proxy-mode=<mode> : The network proxying mode ( iptables , ipvs , userspace ).","title":"Command-line Flags"},{"location":"docs/kube-proxy/configuration.html#code-examples","text":"","title":"Code Examples"},{"location":"docs/kube-proxy/configuration.html#example-1-specifying-configuration-file","text":"Running kube-proxy with a specific configuration file: kube-proxy --config=/path/to/your/kube-proxy-config.yaml","title":"Example 1: Specifying Configuration File"},{"location":"docs/kube-proxy/configuration.html#example-2-overriding-mode-via-flag","text":"Running kube-proxy with the IPVS mode, overriding any setting in a config file: kube-proxy --proxy-mode=ipvs","title":"Example 2: Overriding Mode via Flag"},{"location":"docs/kube-proxy/configuration.html#example-3-setting-log-verbosity","text":"Running kube-proxy and logging debug information (level 4): kube-proxy --v=4 --logtostderr","title":"Example 3: Setting Log Verbosity"},{"location":"docs/kube-proxy/configuration.html#sub-components-and-configuration","text":"The pkg/proxy/config/types.go file defines the structure of the KubeProxyConfiguration and related types. The pkg/proxy/util/kernel_linux.go and pkg/proxy/util/userspace.go files contain utilities related to kernel and userspace interactions, which are influenced by the configuration. The choice of mode (e.g., iptables , ipvs ) dictates which specific proxy implementation ( pkg/proxy/iptables/ or pkg/proxy/ipvs/ ) is used, significantly impacting how network rules are managed.","title":"Sub-components and Configuration"},{"location":"docs/kube-scheduler/index.html","text":"Kubernetes Scheduler (kube-scheduler) Overview The Kubernetes Scheduler ( kube-scheduler ) is a control plane component that watches for newly created Pods that have no Node assigned and selects a Node for them to run on. The scheduler runs a number of plugins which are responsible for filtering out unsuitable Nodes and for ranking the remaining Nodes. The scheduler makes a decision on which Node is best for a Pod based on a combination of its requirements and available resources. Key Concepts Predicates: Predicate functions are used to filter out Nodes that are not suitable for running a Pod. A Pod can only be scheduled on a Node if it passes all predicate checks. Examples include checking for sufficient resources, port availability, and storage availability. Priorities: Priority functions are used to rank the Nodes that have passed the predicate filters. Each priority function assigns a score to a Node, and the scheduler selects the Node with the highest total score. This allows for nuanced placement decisions based on factors like data locality, taints/tolerations, and affinity rules. Node Selection: The final decision of which Node to schedule a Pod on is made after applying both predicates (filtering) and priorities (ranking). Leader Election: To ensure high availability and prevent multiple scheduler instances from making conflicting decisions, kube-scheduler employs a leader election mechanism. Only the elected leader instance is active and performs scheduling. Sub-components The pkg/scheduler/ directory contains the following main sub-components and files: framework : Core scheduling framework, defines interfaces for plugins (predicates, priorities, etc.) and manages their lifecycle. Source: pkg/scheduler/framework/ apis : Contains API definitions and configurations for the scheduler, including scheduling profiles and extender configurations. Source: pkg/scheduler/apis/ extender : Functionality for interacting with external scheduling extenders. Source: pkg/scheduler/extender.go metrics : Exposes scheduler-related metrics for monitoring and observability. Source: pkg/scheduler/metrics/metrics.go profile : Manages different scheduling profiles, allowing for customized plugin configurations. Source: pkg/scheduler/profile/ eventhandlers : Handles events related to pods and nodes to update the scheduler's internal state. Source: pkg/scheduler/eventhandlers.go schedule_one : Contains the logic for scheduling a single pod. Source: pkg/scheduler/schedule_one.go scheduler : The main entry point and core logic for the scheduler, including leader election and the main scheduling loop. Source: pkg/scheduler/scheduler.go util : Utility functions used throughout the scheduler. Source: pkg/scheduler/util/ testing : Utilities for testing scheduler components. Source: pkg/scheduler/testing/ backend : Contains the default scheduler profiles and configurations. Source: pkg/scheduler/backend/ Source Code Links Core Scheduler: pkg/scheduler/scheduler.go Scheduling Framework: pkg/scheduler/framework/ Configuration: pkg/scheduler/apis/config/ Diagrams High-Level Architecture This diagram illustrates the kube-scheduler 's position within the Kubernetes control plane. ```mermaid graph TD subgraph Kubernetes Control Plane API_Server[(API Server)] ETCD[(etcd)] Controller_Manager[Controller Manager] Scheduler[Kube-Scheduler] end subgraph Worker Nodes Kubelet Container_Runtime end API_Server -- Watches/Updates --> ETCD Controller_Manager -- Manages Resources --> API_Server Scheduler -- Watches Pods --> API_Server Scheduler -- Assigns Pods to Nodes --> API_Server API_Server -- Instructs --> Kubelet Kubelet -- Manages Containers --> Container_Runtime Scheduler -- Leader Election --> API_Server ``` Code Examples Starting the Scheduler (Simplified): ```go // Simplified example of scheduler initialization // (Refer to cmd/kube-scheduler/app/server.go for full implementation) import ( \"k8s.io/kubernetes/pkg/scheduler\" // ... other imports ) func RunScheduler(ctx context.Context, / ... /) error { // ... schedulerConfig, err := scheduler.NewConfig( / ... / ) if err != nil { return err } s, err := scheduler.New(ctx, / ... /, schedulerConfig) if err != nil { return err } s.Run() // Starts the main scheduling loop // ... return nil } ``` Pod Scheduling Decision Flow (Conceptual): ```go // Conceptual flow within scheduleOnePod func scheduleOnePod(pod v1.Pod, / ... /) { // 1. Predicates: Filter out unsuitable nodes feasibleNodes, err := p.findNodes(pod, / ... */) if err != nil { // Handle error: node predicates failed return } // 2. Priorities: Score the feasible nodes scores, err := p.prioritizeNodes(pod, feasibleNodes, /* ... */) if err != nil { // Handle error: node priorities failed return } // 3. Node Selection: Choose the highest-scoring node bestNode, err := p.selectBestNode(scores, /* ... */) if err != nil { // Handle error: could not select best node return } // 4. Bind Pod to the selected node err = p.bind(pod, bestNode, /* ... */) if err != nil { // Handle error: failed to bind pod return } } ``` Leader Election (Conceptual): The scheduler uses Kubernetes's built-in leader election mechanism, typically configured via a Lease object in the kube-system namespace. go // Excerpt from pkg/scheduler/scheduler.go related to leader election setup // (Actual implementation involves controllers and Lease objects) func (s *Scheduler) Run(ctx context.Context) { // ... // Leader election is typically configured and managed externally, // but the scheduler listens for the leader transition. // If s.election is configured, it will block until leader is elected. // ... go leaderelection.RunOrDie(ctx, leaderelection.LeaderElectionConfig{ // ... Leader election configuration ... Callbacks: leaderelection.LeaderCallbacks{ OnStartedLeading: func(ctx context.Context) { // Start the main scheduling loop when elected leader s.component.(*schedulerapp.Scheduler).client, _ = clientset.NewForConfig(s.config.ClientConfig) go s.schedule(ctx) // ... }, OnStoppedLeading: func() { // Stop the scheduling loop when leader is lost // ... }, }, }) // ... } Configuration Example (YAML): ```yaml apiVersion: kubescheduler.config.k8s.io/v1beta3 kind: KubeSchedulerConfiguration leaderElection: leaderElect: true renewDeadline: \"15s\" retryPeriod: \"2s\" leaderKey: \"/registry/master/scheduler\" profiles: schedulerName: default-scheduler plugins: multiPoint: enabled: name: \"*\" name: \"PodFitsHostPorts\" weight: 2 name: \"PodFitsHost\" weight: 2 # ... other plugin configurations ``` Scheduling a Pod (Conceptual CLI): While you don't directly \"run\" the scheduler via CLI for a specific pod, you create a pod and the scheduler picks it up. ```bash # 1. Define a pod manifest (e.g., my-pod.yaml) cat < my-pod.yaml apiVersion: v1 kind: Pod metadata: name: my-pod spec: containers: name: nginx image: nginx EOF 2. Apply the pod manifest to the cluster kubectl apply -f my-pod.yaml 3. The kube-scheduler will assign a node to my-pod. You can verify this by describing the pod: kubectl describe pod my-pod ``` Additional Resources Kubernetes Scheduler Documentation: https://kubernetes.io/docs/concepts/scheduling-eviction/scheduling-overview/ Kube-Scheduler Configuration: https://kubernetes.io/docs/reference/config-api/kube-scheduler-config.v1beta3/ Future Work Detailed documentation of each predicate and priority plugin. In-depth analysis of the extender API and its usage. Explanation of the default profiles and their plugin configurations. Mermaid diagrams for specific scheduling flows (e.g., PodFitsHostPorts predicate).","title":"Kubernetes Scheduler (kube-scheduler)"},{"location":"docs/kube-scheduler/index.html#kubernetes-scheduler-kube-scheduler","text":"","title":"Kubernetes Scheduler (kube-scheduler)"},{"location":"docs/kube-scheduler/index.html#overview","text":"The Kubernetes Scheduler ( kube-scheduler ) is a control plane component that watches for newly created Pods that have no Node assigned and selects a Node for them to run on. The scheduler runs a number of plugins which are responsible for filtering out unsuitable Nodes and for ranking the remaining Nodes. The scheduler makes a decision on which Node is best for a Pod based on a combination of its requirements and available resources.","title":"Overview"},{"location":"docs/kube-scheduler/index.html#key-concepts","text":"Predicates: Predicate functions are used to filter out Nodes that are not suitable for running a Pod. A Pod can only be scheduled on a Node if it passes all predicate checks. Examples include checking for sufficient resources, port availability, and storage availability. Priorities: Priority functions are used to rank the Nodes that have passed the predicate filters. Each priority function assigns a score to a Node, and the scheduler selects the Node with the highest total score. This allows for nuanced placement decisions based on factors like data locality, taints/tolerations, and affinity rules. Node Selection: The final decision of which Node to schedule a Pod on is made after applying both predicates (filtering) and priorities (ranking). Leader Election: To ensure high availability and prevent multiple scheduler instances from making conflicting decisions, kube-scheduler employs a leader election mechanism. Only the elected leader instance is active and performs scheduling.","title":"Key Concepts"},{"location":"docs/kube-scheduler/index.html#sub-components","text":"The pkg/scheduler/ directory contains the following main sub-components and files: framework : Core scheduling framework, defines interfaces for plugins (predicates, priorities, etc.) and manages their lifecycle. Source: pkg/scheduler/framework/ apis : Contains API definitions and configurations for the scheduler, including scheduling profiles and extender configurations. Source: pkg/scheduler/apis/ extender : Functionality for interacting with external scheduling extenders. Source: pkg/scheduler/extender.go metrics : Exposes scheduler-related metrics for monitoring and observability. Source: pkg/scheduler/metrics/metrics.go profile : Manages different scheduling profiles, allowing for customized plugin configurations. Source: pkg/scheduler/profile/ eventhandlers : Handles events related to pods and nodes to update the scheduler's internal state. Source: pkg/scheduler/eventhandlers.go schedule_one : Contains the logic for scheduling a single pod. Source: pkg/scheduler/schedule_one.go scheduler : The main entry point and core logic for the scheduler, including leader election and the main scheduling loop. Source: pkg/scheduler/scheduler.go util : Utility functions used throughout the scheduler. Source: pkg/scheduler/util/ testing : Utilities for testing scheduler components. Source: pkg/scheduler/testing/ backend : Contains the default scheduler profiles and configurations. Source: pkg/scheduler/backend/","title":"Sub-components"},{"location":"docs/kube-scheduler/index.html#source-code-links","text":"Core Scheduler: pkg/scheduler/scheduler.go Scheduling Framework: pkg/scheduler/framework/ Configuration: pkg/scheduler/apis/config/","title":"Source Code Links"},{"location":"docs/kube-scheduler/index.html#diagrams","text":"","title":"Diagrams"},{"location":"docs/kube-scheduler/index.html#high-level-architecture","text":"This diagram illustrates the kube-scheduler 's position within the Kubernetes control plane. ```mermaid graph TD subgraph Kubernetes Control Plane API_Server[(API Server)] ETCD[(etcd)] Controller_Manager[Controller Manager] Scheduler[Kube-Scheduler] end subgraph Worker Nodes Kubelet Container_Runtime end API_Server -- Watches/Updates --> ETCD Controller_Manager -- Manages Resources --> API_Server Scheduler -- Watches Pods --> API_Server Scheduler -- Assigns Pods to Nodes --> API_Server API_Server -- Instructs --> Kubelet Kubelet -- Manages Containers --> Container_Runtime Scheduler -- Leader Election --> API_Server ```","title":"High-Level Architecture"},{"location":"docs/kube-scheduler/index.html#code-examples","text":"Starting the Scheduler (Simplified): ```go // Simplified example of scheduler initialization // (Refer to cmd/kube-scheduler/app/server.go for full implementation) import ( \"k8s.io/kubernetes/pkg/scheduler\" // ... other imports ) func RunScheduler(ctx context.Context, / ... /) error { // ... schedulerConfig, err := scheduler.NewConfig( / ... / ) if err != nil { return err } s, err := scheduler.New(ctx, / ... /, schedulerConfig) if err != nil { return err } s.Run() // Starts the main scheduling loop // ... return nil } ``` Pod Scheduling Decision Flow (Conceptual): ```go // Conceptual flow within scheduleOnePod func scheduleOnePod(pod v1.Pod, / ... /) { // 1. Predicates: Filter out unsuitable nodes feasibleNodes, err := p.findNodes(pod, / ... */) if err != nil { // Handle error: node predicates failed return } // 2. Priorities: Score the feasible nodes scores, err := p.prioritizeNodes(pod, feasibleNodes, /* ... */) if err != nil { // Handle error: node priorities failed return } // 3. Node Selection: Choose the highest-scoring node bestNode, err := p.selectBestNode(scores, /* ... */) if err != nil { // Handle error: could not select best node return } // 4. Bind Pod to the selected node err = p.bind(pod, bestNode, /* ... */) if err != nil { // Handle error: failed to bind pod return } } ``` Leader Election (Conceptual): The scheduler uses Kubernetes's built-in leader election mechanism, typically configured via a Lease object in the kube-system namespace. go // Excerpt from pkg/scheduler/scheduler.go related to leader election setup // (Actual implementation involves controllers and Lease objects) func (s *Scheduler) Run(ctx context.Context) { // ... // Leader election is typically configured and managed externally, // but the scheduler listens for the leader transition. // If s.election is configured, it will block until leader is elected. // ... go leaderelection.RunOrDie(ctx, leaderelection.LeaderElectionConfig{ // ... Leader election configuration ... Callbacks: leaderelection.LeaderCallbacks{ OnStartedLeading: func(ctx context.Context) { // Start the main scheduling loop when elected leader s.component.(*schedulerapp.Scheduler).client, _ = clientset.NewForConfig(s.config.ClientConfig) go s.schedule(ctx) // ... }, OnStoppedLeading: func() { // Stop the scheduling loop when leader is lost // ... }, }, }) // ... } Configuration Example (YAML): ```yaml apiVersion: kubescheduler.config.k8s.io/v1beta3 kind: KubeSchedulerConfiguration leaderElection: leaderElect: true renewDeadline: \"15s\" retryPeriod: \"2s\" leaderKey: \"/registry/master/scheduler\" profiles: schedulerName: default-scheduler plugins: multiPoint: enabled: name: \"*\" name: \"PodFitsHostPorts\" weight: 2 name: \"PodFitsHost\" weight: 2 # ... other plugin configurations ``` Scheduling a Pod (Conceptual CLI): While you don't directly \"run\" the scheduler via CLI for a specific pod, you create a pod and the scheduler picks it up. ```bash # 1. Define a pod manifest (e.g., my-pod.yaml) cat < my-pod.yaml apiVersion: v1 kind: Pod metadata: name: my-pod spec: containers: name: nginx image: nginx EOF","title":"Code Examples"},{"location":"docs/kube-scheduler/index.html#2-apply-the-pod-manifest-to-the-cluster","text":"kubectl apply -f my-pod.yaml","title":"2. Apply the pod manifest to the cluster"},{"location":"docs/kube-scheduler/index.html#3-the-kube-scheduler-will-assign-a-node-to-my-pod","text":"","title":"3. The kube-scheduler will assign a node to my-pod."},{"location":"docs/kube-scheduler/index.html#you-can-verify-this-by-describing-the-pod","text":"kubectl describe pod my-pod ```","title":"You can verify this by describing the pod:"},{"location":"docs/kube-scheduler/index.html#additional-resources","text":"Kubernetes Scheduler Documentation: https://kubernetes.io/docs/concepts/scheduling-eviction/scheduling-overview/ Kube-Scheduler Configuration: https://kubernetes.io/docs/reference/config-api/kube-scheduler-config.v1beta3/","title":"Additional Resources"},{"location":"docs/kube-scheduler/index.html#future-work","text":"Detailed documentation of each predicate and priority plugin. In-depth analysis of the extender API and its usage. Explanation of the default profiles and their plugin configurations. Mermaid diagrams for specific scheduling flows (e.g., PodFitsHostPorts predicate).","title":"Future Work"},{"location":"docs/kube-scheduler/api_reference.html","text":"kube-scheduler API Reference Configuration API The kube-scheduler 's behavior is configured via a KubeSchedulerConfiguration object. This configuration is typically provided as a YAML file to the scheduler process. The configuration API is versioned, with v1beta3 being a common version. Key Sections of the Configuration: kind: KubeSchedulerConfiguration : The root object. apiVersion: kubescheduler.config.k8s.io/v1beta3 : Specifies the API version. leaderElection : Configuration for the leader election mechanism. clientConnection : Settings for the API server client connection. profiles : Defines one or more scheduling profiles. Each profile contains: schedulerName : The name used by Pods to select this scheduler. plugins : Configuration for the scheduling plugins (predicates, priorities, etc.). multiPoint : Plugin configurations, including ordering and weights. Individual plugin settings. pluginConfig : Specific configurations for individual plugins. health : Health check configuration. enableProfiling : Enables profiling endpoints. Example Configuration Snippet ```yaml apiVersion: kubescheduler.config.k8s.io/v1beta3 kind: KubeSchedulerConfiguration clientConnection: kubeconfig: \"\" # Path to kubeconfig, or leave empty for in-cluster qps: 50 burst: 100 leaderElection: leaderElect: true leaseDuration: \"15s\" renewDeadline: \"10s\" retryPeriod: \"2s\" leaderKey: \"/kubernetes/leader-election/kube-scheduler\" profiles: - schedulerName: default-scheduler # The \" \" symbol can be used to enable all plugins that are not explicitly configured. # Plugins are enabled in the order they appear in the profile. plugins: multiPoint: enabled: - name: \" \" # Example of enabling a specific plugin with a weight - name: \"PodFitsHostPorts\" weight: 2 # Example of disabling a specific plugin - name: \"PodFitsOnNode\" plugin: \"NodeAffinity\" # This is an example, NodeAffinity is a plugin name enabled: false pluginConfig: - name: MaintainPodsThroughLease args: leaseDuration: \"15s\" ... more profile and plugin configurations ``` Metrics The kube-scheduler exposes metrics via an HTTP endpoint (typically /metrics on port 10259). These metrics are crucial for monitoring the scheduler's performance and health. Metric Name Type Description Example Usage scheduler_schedule_attempts Counter Number of Pod scheduling attempts. rate(scheduler_schedule_attempts[5m]) scheduler_pod_scheduling_duration_seconds Histogram Latency of Pod scheduling operations. histogram_quantile(0.99, rate(scheduler_pod_scheduling_duration_seconds_bucket[5m])) scheduler_e2e_scheduling_latency_seconds Histogram End-to-end latency from Pod creation to scheduling completion. rate(scheduler_e2e_scheduling_latency_seconds_sum[5m]) / rate(scheduler_e2e_scheduling_latency_seconds_count[5m]) scheduler_queue_unschedulable_pods Gauge Number of Pods in the queue that are currently unschedulable. scheduler_queue_unschedulable_pods scheduler_predicates_evaluation_duration_seconds Histogram Duration of predicate evaluations. rate(scheduler_predicates_evaluation_duration_seconds_avg[5m]) scheduler_priorities_evaluation_duration_seconds Histogram Duration of priority evaluations. rate(scheduler_priorities_evaluation_duration_seconds_avg[5m]) scheduler_binding_duration_seconds Histogram Duration of the Pod binding operation. rate(scheduler_binding_duration_seconds_bucket[5m]) scheduler_scheduling_algorithm_duration_seconds Histogram Total time spent in the scheduling algorithm (predicates + priorities). rate(scheduler_scheduling_algorithm_duration_seconds_avg[5m]) Example Metrics Query (Prometheus) To get the average scheduling latency over the last 5 minutes: ```promql rate(scheduler_pod_scheduling_duration_seconds_sum[5m]) / rate(scheduler_pod_scheduling_duration_seconds_count[5m]) ``` Source Code Links Configuration API Types: pkg/scheduler/apis/config/v1beta3/types.go Default Configuration: pkg/scheduler/apis/config/v1beta3/defaults.go Metrics Definitions: pkg/scheduler/metrics/metrics.go API Server Client Connection: pkg/scheduler/scheduler.go (within NewConfig function) Future Work Detailed documentation of each configuration field. Specific examples for configuring each major scheduling plugin. Explanation of how to use schedulerName in Pod specifications. More in-depth examples of Prometheus queries for scheduler metrics.","title":"kube-scheduler API Reference"},{"location":"docs/kube-scheduler/api_reference.html#kube-scheduler-api-reference","text":"","title":"kube-scheduler API Reference"},{"location":"docs/kube-scheduler/api_reference.html#configuration-api","text":"The kube-scheduler 's behavior is configured via a KubeSchedulerConfiguration object. This configuration is typically provided as a YAML file to the scheduler process. The configuration API is versioned, with v1beta3 being a common version. Key Sections of the Configuration: kind: KubeSchedulerConfiguration : The root object. apiVersion: kubescheduler.config.k8s.io/v1beta3 : Specifies the API version. leaderElection : Configuration for the leader election mechanism. clientConnection : Settings for the API server client connection. profiles : Defines one or more scheduling profiles. Each profile contains: schedulerName : The name used by Pods to select this scheduler. plugins : Configuration for the scheduling plugins (predicates, priorities, etc.). multiPoint : Plugin configurations, including ordering and weights. Individual plugin settings. pluginConfig : Specific configurations for individual plugins. health : Health check configuration. enableProfiling : Enables profiling endpoints.","title":"Configuration API"},{"location":"docs/kube-scheduler/api_reference.html#example-configuration-snippet","text":"```yaml apiVersion: kubescheduler.config.k8s.io/v1beta3 kind: KubeSchedulerConfiguration clientConnection: kubeconfig: \"\" # Path to kubeconfig, or leave empty for in-cluster qps: 50 burst: 100 leaderElection: leaderElect: true leaseDuration: \"15s\" renewDeadline: \"10s\" retryPeriod: \"2s\" leaderKey: \"/kubernetes/leader-election/kube-scheduler\" profiles: - schedulerName: default-scheduler # The \" \" symbol can be used to enable all plugins that are not explicitly configured. # Plugins are enabled in the order they appear in the profile. plugins: multiPoint: enabled: - name: \" \" # Example of enabling a specific plugin with a weight - name: \"PodFitsHostPorts\" weight: 2 # Example of disabling a specific plugin - name: \"PodFitsOnNode\" plugin: \"NodeAffinity\" # This is an example, NodeAffinity is a plugin name enabled: false pluginConfig: - name: MaintainPodsThroughLease args: leaseDuration: \"15s\"","title":"Example Configuration Snippet"},{"location":"docs/kube-scheduler/api_reference.html#more-profile-and-plugin-configurations","text":"```","title":"... more profile and plugin configurations"},{"location":"docs/kube-scheduler/api_reference.html#metrics","text":"The kube-scheduler exposes metrics via an HTTP endpoint (typically /metrics on port 10259). These metrics are crucial for monitoring the scheduler's performance and health. Metric Name Type Description Example Usage scheduler_schedule_attempts Counter Number of Pod scheduling attempts. rate(scheduler_schedule_attempts[5m]) scheduler_pod_scheduling_duration_seconds Histogram Latency of Pod scheduling operations. histogram_quantile(0.99, rate(scheduler_pod_scheduling_duration_seconds_bucket[5m])) scheduler_e2e_scheduling_latency_seconds Histogram End-to-end latency from Pod creation to scheduling completion. rate(scheduler_e2e_scheduling_latency_seconds_sum[5m]) / rate(scheduler_e2e_scheduling_latency_seconds_count[5m]) scheduler_queue_unschedulable_pods Gauge Number of Pods in the queue that are currently unschedulable. scheduler_queue_unschedulable_pods scheduler_predicates_evaluation_duration_seconds Histogram Duration of predicate evaluations. rate(scheduler_predicates_evaluation_duration_seconds_avg[5m]) scheduler_priorities_evaluation_duration_seconds Histogram Duration of priority evaluations. rate(scheduler_priorities_evaluation_duration_seconds_avg[5m]) scheduler_binding_duration_seconds Histogram Duration of the Pod binding operation. rate(scheduler_binding_duration_seconds_bucket[5m]) scheduler_scheduling_algorithm_duration_seconds Histogram Total time spent in the scheduling algorithm (predicates + priorities). rate(scheduler_scheduling_algorithm_duration_seconds_avg[5m])","title":"Metrics"},{"location":"docs/kube-scheduler/api_reference.html#example-metrics-query-prometheus","text":"To get the average scheduling latency over the last 5 minutes: ```promql rate(scheduler_pod_scheduling_duration_seconds_sum[5m]) / rate(scheduler_pod_scheduling_duration_seconds_count[5m]) ```","title":"Example Metrics Query (Prometheus)"},{"location":"docs/kube-scheduler/api_reference.html#source-code-links","text":"Configuration API Types: pkg/scheduler/apis/config/v1beta3/types.go Default Configuration: pkg/scheduler/apis/config/v1beta3/defaults.go Metrics Definitions: pkg/scheduler/metrics/metrics.go API Server Client Connection: pkg/scheduler/scheduler.go (within NewConfig function)","title":"Source Code Links"},{"location":"docs/kube-scheduler/api_reference.html#future-work","text":"Detailed documentation of each configuration field. Specific examples for configuring each major scheduling plugin. Explanation of how to use schedulerName in Pod specifications. More in-depth examples of Prometheus queries for scheduler metrics.","title":"Future Work"},{"location":"docs/kube-scheduler/architecture.html","text":"kube-scheduler Architecture System Context The kube-scheduler is a critical component of the Kubernetes control plane. It interacts primarily with the API Server to watch for unscheduled Pods and to update their .spec.nodeName field once a suitable Node is selected. It also participates in leader election, typically using a Lease object managed by the API Server. Internal Architecture The kube-scheduler operates as a distributed system, with potentially multiple instances running, but only one is active as the leader at any given time due to leader election. The core logic revolves around a scheduling loop that processes Pods from a work queue. Scheduling Pipeline Pod Event Handling: The scheduler watches for Pod creation events via the API Server. New or unscheduled Pods are added to an internal work queue. Pod Selection: The scheduler dequeues a Pod and begins the selection process. Node Filtering (Predicates): A series of predicate functions are run against all available Nodes. Nodes that fail any predicate are removed from the list of potential placements for the current Pod. Common predicates include: PodFitsHostPorts : Checks if Pod's hostPort is available on the Node. PodFitsOnNode : Checks if the Node has sufficient resources (CPU, memory, ephemeral-storage). NoVolumeZoneConflict : Ensures persistent volume zone requirements are met. PodFitsResources : Verifies resource requests of the Pod against Node capacity. MatchNodeSelector : Checks if the Node labels match the Pod's nodeSelector or nodeAffinity rules. CheckNodeUnschedulable : Ensures the Pod doesn't violate the node.kubernetes.io/unschedulable taint. TaintTolerationMatches : Checks if the Pod tolerates taints on the Node. Node Scoring (Priorities): Nodes that pass the predicate filters are then scored by priority functions. Each priority function assigns a numerical score to a Node based on predefined criteria. The scheduler sums these scores for each Node. Common priorities include: ImageLocalityPriority : Favors Nodes that have the Pod's required container images already cached. TaintTolerationPriority : Gives higher scores to Nodes whose taints the Pod tolerates. NodeAffinityPriority : Assigns higher scores to Nodes that match the Pod's nodeAffinity rules. PodTopologySpreadPriority : Scores Nodes based on Kubernetes's Pod Topology Spread Constraints. BalancedResourceAllocation : Favors Nodes with a more balanced resource utilization. Node Selection: The Node with the highest combined priority score is selected. If multiple Nodes have the same highest score, one is chosen arbitrarily (or based on internal tie-breaking). Pod Binding: The scheduler updates the Pod object via the API Server, setting the .spec.nodeName field to the chosen Node. The Kubelet on that Node then takes over Pod lifecycle management. Plugin Management The scheduling framework ( pkg/scheduler/framework/ ) provides an extensible interface for plugins. Predicates and priorities are implemented as plugins. The configuration ( pkg/scheduler/apis/config/ ) defines which plugins are enabled, their order, and their weights. Data Flow Diagram This diagram illustrates the flow of a Pod through the scheduling process. ```mermaid graph TD A[Pod Created] --> B{API Server Watch}; B --> C[Add Pod to Queue]; C --> D{Dequeue Pod}; D --> E{Find Nodes (Predicates)}; E -- Filtered Nodes --> F{Score Nodes (Priorities)}; F -- Scored Nodes --> G{Select Best Node}; G --> H{Bind Pod to Node}; H --> I[Update Pod Spec via API Server]; I --> J[Pod Assigned to Node]; subgraph Predicate Phase E end subgraph Priority Phase F end style E fill:#f9f,stroke:#333,stroke-width:2px style F fill:#ccf,stroke:#333,stroke-width:2px ``` Leader Election The kube-scheduler uses a Lease-based leader election mechanism. Initialization: When a scheduler starts, it attempts to acquire a leader lock (typically a Lease object in the kube-system namespace). Acquisition: If no leader is currently active, the instance acquires the lock and becomes the leader. It starts the main scheduling loop. It periodically renews its lease to maintain leadership. Contention: If another instance is already the leader, the new instance waits and periodically retries acquiring the lock. Loss of Leadership: If the leader fails to renew its lease (e.g., due to network issues or crashing), the lock expires. Another instance can then acquire the lock and become the new leader. Callback: The leader election component triggers OnStartedLeading and OnStoppedLeading callbacks, allowing the scheduler to start or stop its core work. Leader Election Diagram ```mermaid graph LR subgraph Scheduler Instances S1[Instance 1] S2[Instance 2] S3[Instance 3] end subgraph Lease Object L(kube-system/scheduler-leader-election) end S1 -- Tries to acquire lease --> L S2 -- Tries to acquire lease --> L S3 -- Tries to acquire lease --> L L -- Grants lease to --> S1 S1 -- Becomes Leader --> A[Starts Scheduling Loop] S1 -- Renews Lease Periodically --> L S2 -- Waits / Retries --> L S3 -- Waits / Retries --> L L -- Lease Expires / Lost --> S1 S1 -- Stops Scheduling Loop --> B[Resumes waiting] S2 -- Acquires Lease --> L S2 -- Becomes Leader --> C[Starts Scheduling Loop] ``` Source Code Links Main Scheduler: pkg/scheduler/scheduler.go Scheduling Framework: pkg/scheduler/framework/ Configuration Types: pkg/scheduler/apis/config/v1beta3/types.go Predicate Logic: [ pkg/scheduler/framework/plugins/ directory contains various predicate implementations.] Priority Logic: [ pkg/scheduler/framework/plugins/ directory contains various priority implementations.] Leader Election: Utilizes Kubernetes client-go's leader election package. Future Work Detailed breakdown of each predicate and priority plugin with specific code examples. Analysis of the extender mechanism and its configuration. Explanation of how scheduling profiles are defined and utilized. Diagrams illustrating the interaction between specific plugins. Performance considerations and tuning. Extensibility points within the scheduling framework.","title":"kube-scheduler Architecture"},{"location":"docs/kube-scheduler/architecture.html#kube-scheduler-architecture","text":"","title":"kube-scheduler Architecture"},{"location":"docs/kube-scheduler/architecture.html#system-context","text":"The kube-scheduler is a critical component of the Kubernetes control plane. It interacts primarily with the API Server to watch for unscheduled Pods and to update their .spec.nodeName field once a suitable Node is selected. It also participates in leader election, typically using a Lease object managed by the API Server.","title":"System Context"},{"location":"docs/kube-scheduler/architecture.html#internal-architecture","text":"The kube-scheduler operates as a distributed system, with potentially multiple instances running, but only one is active as the leader at any given time due to leader election. The core logic revolves around a scheduling loop that processes Pods from a work queue.","title":"Internal Architecture"},{"location":"docs/kube-scheduler/architecture.html#scheduling-pipeline","text":"Pod Event Handling: The scheduler watches for Pod creation events via the API Server. New or unscheduled Pods are added to an internal work queue. Pod Selection: The scheduler dequeues a Pod and begins the selection process. Node Filtering (Predicates): A series of predicate functions are run against all available Nodes. Nodes that fail any predicate are removed from the list of potential placements for the current Pod. Common predicates include: PodFitsHostPorts : Checks if Pod's hostPort is available on the Node. PodFitsOnNode : Checks if the Node has sufficient resources (CPU, memory, ephemeral-storage). NoVolumeZoneConflict : Ensures persistent volume zone requirements are met. PodFitsResources : Verifies resource requests of the Pod against Node capacity. MatchNodeSelector : Checks if the Node labels match the Pod's nodeSelector or nodeAffinity rules. CheckNodeUnschedulable : Ensures the Pod doesn't violate the node.kubernetes.io/unschedulable taint. TaintTolerationMatches : Checks if the Pod tolerates taints on the Node. Node Scoring (Priorities): Nodes that pass the predicate filters are then scored by priority functions. Each priority function assigns a numerical score to a Node based on predefined criteria. The scheduler sums these scores for each Node. Common priorities include: ImageLocalityPriority : Favors Nodes that have the Pod's required container images already cached. TaintTolerationPriority : Gives higher scores to Nodes whose taints the Pod tolerates. NodeAffinityPriority : Assigns higher scores to Nodes that match the Pod's nodeAffinity rules. PodTopologySpreadPriority : Scores Nodes based on Kubernetes's Pod Topology Spread Constraints. BalancedResourceAllocation : Favors Nodes with a more balanced resource utilization. Node Selection: The Node with the highest combined priority score is selected. If multiple Nodes have the same highest score, one is chosen arbitrarily (or based on internal tie-breaking). Pod Binding: The scheduler updates the Pod object via the API Server, setting the .spec.nodeName field to the chosen Node. The Kubelet on that Node then takes over Pod lifecycle management.","title":"Scheduling Pipeline"},{"location":"docs/kube-scheduler/architecture.html#plugin-management","text":"The scheduling framework ( pkg/scheduler/framework/ ) provides an extensible interface for plugins. Predicates and priorities are implemented as plugins. The configuration ( pkg/scheduler/apis/config/ ) defines which plugins are enabled, their order, and their weights.","title":"Plugin Management"},{"location":"docs/kube-scheduler/architecture.html#data-flow-diagram","text":"This diagram illustrates the flow of a Pod through the scheduling process. ```mermaid graph TD A[Pod Created] --> B{API Server Watch}; B --> C[Add Pod to Queue]; C --> D{Dequeue Pod}; D --> E{Find Nodes (Predicates)}; E -- Filtered Nodes --> F{Score Nodes (Priorities)}; F -- Scored Nodes --> G{Select Best Node}; G --> H{Bind Pod to Node}; H --> I[Update Pod Spec via API Server]; I --> J[Pod Assigned to Node]; subgraph Predicate Phase E end subgraph Priority Phase F end style E fill:#f9f,stroke:#333,stroke-width:2px style F fill:#ccf,stroke:#333,stroke-width:2px ```","title":"Data Flow Diagram"},{"location":"docs/kube-scheduler/architecture.html#leader-election","text":"The kube-scheduler uses a Lease-based leader election mechanism. Initialization: When a scheduler starts, it attempts to acquire a leader lock (typically a Lease object in the kube-system namespace). Acquisition: If no leader is currently active, the instance acquires the lock and becomes the leader. It starts the main scheduling loop. It periodically renews its lease to maintain leadership. Contention: If another instance is already the leader, the new instance waits and periodically retries acquiring the lock. Loss of Leadership: If the leader fails to renew its lease (e.g., due to network issues or crashing), the lock expires. Another instance can then acquire the lock and become the new leader. Callback: The leader election component triggers OnStartedLeading and OnStoppedLeading callbacks, allowing the scheduler to start or stop its core work.","title":"Leader Election"},{"location":"docs/kube-scheduler/architecture.html#leader-election-diagram","text":"```mermaid graph LR subgraph Scheduler Instances S1[Instance 1] S2[Instance 2] S3[Instance 3] end subgraph Lease Object L(kube-system/scheduler-leader-election) end S1 -- Tries to acquire lease --> L S2 -- Tries to acquire lease --> L S3 -- Tries to acquire lease --> L L -- Grants lease to --> S1 S1 -- Becomes Leader --> A[Starts Scheduling Loop] S1 -- Renews Lease Periodically --> L S2 -- Waits / Retries --> L S3 -- Waits / Retries --> L L -- Lease Expires / Lost --> S1 S1 -- Stops Scheduling Loop --> B[Resumes waiting] S2 -- Acquires Lease --> L S2 -- Becomes Leader --> C[Starts Scheduling Loop] ```","title":"Leader Election Diagram"},{"location":"docs/kube-scheduler/architecture.html#source-code-links","text":"Main Scheduler: pkg/scheduler/scheduler.go Scheduling Framework: pkg/scheduler/framework/ Configuration Types: pkg/scheduler/apis/config/v1beta3/types.go Predicate Logic: [ pkg/scheduler/framework/plugins/ directory contains various predicate implementations.] Priority Logic: [ pkg/scheduler/framework/plugins/ directory contains various priority implementations.] Leader Election: Utilizes Kubernetes client-go's leader election package.","title":"Source Code Links"},{"location":"docs/kube-scheduler/architecture.html#future-work","text":"Detailed breakdown of each predicate and priority plugin with specific code examples. Analysis of the extender mechanism and its configuration. Explanation of how scheduling profiles are defined and utilized. Diagrams illustrating the interaction between specific plugins. Performance considerations and tuning. Extensibility points within the scheduling framework.","title":"Future Work"},{"location":"docs/kube-scheduler/configuration.html","text":"kube-scheduler Configuration The kube-scheduler 's behavior is highly configurable, allowing fine-tuning of its scheduling decisions, plugin enablement, and operational parameters. Configuration is primarily managed through a KubeSchedulerConfiguration object, typically supplied as a YAML file. Configuration Options The following table details key configuration options. Note that this is not exhaustive, and specific plugin configurations can add many more parameters. Parameter Name Type Description Default Value Source File(s) apiVersion string Specifies the API version of the configuration file. kubescheduler.config.k8s.io/v1beta3 - kind string Must be KubeSchedulerConfiguration . KubeSchedulerConfiguration - clientConnection ClientConnectionConfiguration Settings for connecting to the Kubernetes API server. Varies (e.g., QPS: 50 , Burst: 100 ) pkg/scheduler/apis/config/v1beta3/defaults.go leaderElection LeaderElectionConfiguration Configuration for the leader election mechanism. leaderElect: true , leaseDuration: \"15s\" , etc. pkg/scheduler/apis/config/v1beta3/defaults.go profiles []Policy A list of scheduling profiles. Each profile defines a set of plugins and their configurations. Default profile with standard plugins pkg/scheduler/apis/config/v1beta3/defaults.go enableProfiling bool Enables the profiling endpoint for performance analysis. false pkg/scheduler/apis/config/v1beta3/defaults.go health HealthScheduleConfiguration Configuration for the health checker. Varies pkg/scheduler/apis/config/v1beta3/defaults.go percentageOfNodesToScore int32 Fraction of nodes that the scheduler tries to find the best fit for a pod. 0 means all nodes. 0 (all nodes) pkg/scheduler/apis/config/v1beta3/defaults.go schedulerName string The name to use for the scheduler instance. Pods can target this scheduler using spec.schedulerName . \"default-scheduler\" pkg/scheduler/apis/config/v1beta3/defaults.go shardAware bool Whether the scheduler should be shard-aware. false pkg/scheduler/apis/config/v1beta3/defaults.go skipPodPendingTimeout bool If true, the scheduler will not attempt to schedule pods that have been pending for a long time. false pkg/scheduler/apis/config/v1beta3/defaults.go hardPodAffinitySymmetric bool If true, hard pod affinity/anti-affinity rules are symmetric. false pkg/scheduler/apis/config/v1beta3/defaults.go disablePreemption bool If true, preemption is disabled. false pkg/scheduler/apis/config/v1beta3/defaults.go profiles (Plugin Configuration) Within each profile, the plugins.multiPoint.enabled list defines the scheduling plugins to use, their order, and their weights. name: \"*\" : Enables all default plugins not explicitly listed. name: \"PluginName\" : Enables a specific plugin. weight : An integer used in priority calculations. Higher weights mean the plugin's score has more impact. pluginConfig This section allows for detailed configuration of specific plugins, overriding defaults or providing necessary arguments. name : The name of the plugin to configure. args : Plugin-specific arguments (e.g., leaseDuration for MaintainPodsThroughLease ). Code Examples Minimal Scheduler Configuration: A basic configuration enabling the default scheduler and leader election. ```yaml apiVersion: kubescheduler.config.k8s.io/v1beta3 kind: KubeSchedulerConfiguration leaderElection: leaderElect: true profiles: schedulerName: default-scheduler plugins: multiPoint: enabled: name: \"*\" ``` Customizing Plugin Weights: This example increases the weight of PodFitsHostPorts and ImageLocalityPriority . ```yaml apiVersion: kubescheduler.config.k8s.io/v1beta3 kind: KubeSchedulerConfiguration leaderElection: leaderElect: true profiles: schedulerName: default-scheduler plugins: multiPoint: enabled: name: \"*\" name: \"PodFitsHostPorts\" weight: 3 # Increased weight name: \"ImageLocalityPriority\" weight: 5 # Increased weight ``` Running Scheduler with a Custom Configuration File: This command demonstrates how to start the kube-scheduler binary, pointing it to a custom configuration file. ```bash kube-scheduler \\ --config=/path/to/your/custom-scheduler-config.yaml \\ --leader-elect=true \\ --v=2 ``` (Note: Actual command and flags might vary slightly based on the Kubernetes version and deployment method.) Source Code Links Default Configuration Values: pkg/scheduler/apis/config/v1beta3/defaults.go Configuration API Types: pkg/scheduler/apis/config/v1beta3/types.go Main Scheduler Entrypoint (CLI Flags): cmd/kube-scheduler/app/server.go Future Work Detailed explanation of each plugin's configuration arguments. Examples of advanced configurations, such as custom profiles or extender settings. How to dynamically update scheduler configuration (if supported). Impact of different configuration choices on scheduling performance.","title":"kube-scheduler Configuration"},{"location":"docs/kube-scheduler/configuration.html#kube-scheduler-configuration","text":"The kube-scheduler 's behavior is highly configurable, allowing fine-tuning of its scheduling decisions, plugin enablement, and operational parameters. Configuration is primarily managed through a KubeSchedulerConfiguration object, typically supplied as a YAML file.","title":"kube-scheduler Configuration"},{"location":"docs/kube-scheduler/configuration.html#configuration-options","text":"The following table details key configuration options. Note that this is not exhaustive, and specific plugin configurations can add many more parameters. Parameter Name Type Description Default Value Source File(s) apiVersion string Specifies the API version of the configuration file. kubescheduler.config.k8s.io/v1beta3 - kind string Must be KubeSchedulerConfiguration . KubeSchedulerConfiguration - clientConnection ClientConnectionConfiguration Settings for connecting to the Kubernetes API server. Varies (e.g., QPS: 50 , Burst: 100 ) pkg/scheduler/apis/config/v1beta3/defaults.go leaderElection LeaderElectionConfiguration Configuration for the leader election mechanism. leaderElect: true , leaseDuration: \"15s\" , etc. pkg/scheduler/apis/config/v1beta3/defaults.go profiles []Policy A list of scheduling profiles. Each profile defines a set of plugins and their configurations. Default profile with standard plugins pkg/scheduler/apis/config/v1beta3/defaults.go enableProfiling bool Enables the profiling endpoint for performance analysis. false pkg/scheduler/apis/config/v1beta3/defaults.go health HealthScheduleConfiguration Configuration for the health checker. Varies pkg/scheduler/apis/config/v1beta3/defaults.go percentageOfNodesToScore int32 Fraction of nodes that the scheduler tries to find the best fit for a pod. 0 means all nodes. 0 (all nodes) pkg/scheduler/apis/config/v1beta3/defaults.go schedulerName string The name to use for the scheduler instance. Pods can target this scheduler using spec.schedulerName . \"default-scheduler\" pkg/scheduler/apis/config/v1beta3/defaults.go shardAware bool Whether the scheduler should be shard-aware. false pkg/scheduler/apis/config/v1beta3/defaults.go skipPodPendingTimeout bool If true, the scheduler will not attempt to schedule pods that have been pending for a long time. false pkg/scheduler/apis/config/v1beta3/defaults.go hardPodAffinitySymmetric bool If true, hard pod affinity/anti-affinity rules are symmetric. false pkg/scheduler/apis/config/v1beta3/defaults.go disablePreemption bool If true, preemption is disabled. false pkg/scheduler/apis/config/v1beta3/defaults.go","title":"Configuration Options"},{"location":"docs/kube-scheduler/configuration.html#profiles-plugin-configuration","text":"Within each profile, the plugins.multiPoint.enabled list defines the scheduling plugins to use, their order, and their weights. name: \"*\" : Enables all default plugins not explicitly listed. name: \"PluginName\" : Enables a specific plugin. weight : An integer used in priority calculations. Higher weights mean the plugin's score has more impact.","title":"profiles (Plugin Configuration)"},{"location":"docs/kube-scheduler/configuration.html#pluginconfig","text":"This section allows for detailed configuration of specific plugins, overriding defaults or providing necessary arguments. name : The name of the plugin to configure. args : Plugin-specific arguments (e.g., leaseDuration for MaintainPodsThroughLease ).","title":"pluginConfig"},{"location":"docs/kube-scheduler/configuration.html#code-examples","text":"Minimal Scheduler Configuration: A basic configuration enabling the default scheduler and leader election. ```yaml apiVersion: kubescheduler.config.k8s.io/v1beta3 kind: KubeSchedulerConfiguration leaderElection: leaderElect: true profiles: schedulerName: default-scheduler plugins: multiPoint: enabled: name: \"*\" ``` Customizing Plugin Weights: This example increases the weight of PodFitsHostPorts and ImageLocalityPriority . ```yaml apiVersion: kubescheduler.config.k8s.io/v1beta3 kind: KubeSchedulerConfiguration leaderElection: leaderElect: true profiles: schedulerName: default-scheduler plugins: multiPoint: enabled: name: \"*\" name: \"PodFitsHostPorts\" weight: 3 # Increased weight name: \"ImageLocalityPriority\" weight: 5 # Increased weight ``` Running Scheduler with a Custom Configuration File: This command demonstrates how to start the kube-scheduler binary, pointing it to a custom configuration file. ```bash kube-scheduler \\ --config=/path/to/your/custom-scheduler-config.yaml \\ --leader-elect=true \\ --v=2 ``` (Note: Actual command and flags might vary slightly based on the Kubernetes version and deployment method.)","title":"Code Examples"},{"location":"docs/kube-scheduler/configuration.html#source-code-links","text":"Default Configuration Values: pkg/scheduler/apis/config/v1beta3/defaults.go Configuration API Types: pkg/scheduler/apis/config/v1beta3/types.go Main Scheduler Entrypoint (CLI Flags): cmd/kube-scheduler/app/server.go","title":"Source Code Links"},{"location":"docs/kube-scheduler/configuration.html#future-work","text":"Detailed explanation of each plugin's configuration arguments. Examples of advanced configurations, such as custom profiles or extender settings. How to dynamically update scheduler configuration (if supported). Impact of different configuration choices on scheduling performance.","title":"Future Work"},{"location":"docs/kubectl/index.html","text":"Kubectl Overview The Kubernetes command-line interface, kubectl, is the primary tool for interacting with a Kubernetes cluster. It allows users to perform a wide range of operations, from deploying applications to inspecting and managing cluster resources. Key Concepts Commands : Kubectl operates through a system of commands, each designed for a specific task (e.g., get , create , delete , apply ). Resources : These are the objects within Kubernetes that kubectl manages, such as Pods, Services, Deployments, and ConfigMaps. Flags and Arguments : Commands can be modified with flags (e.g., -n <namespace> ) and arguments (e.g., <resource-type>/<resource-name> ) to refine their behavior. Configuration : Kubectl uses a configuration file (typically ~/.kube/config ) to manage connections to multiple clusters and authenticate. Sub-components The pkg/kubectl/ directory contains the core logic for the kubectl client. Key sub-components include: Command Package ( cmd/ ) : This package defines the structure and implementation of kubectl commands. It handles argument parsing, command execution, and output formatting. cmd/convert : A sub-package within cmd/ that provides functionality for converting resource configurations between different API versions. Core Logic : The main kubectl/ directory contains foundational logic for API interaction, client creation, and command execution flow. Getting Started Installation : Ensure kubectl is installed and configured to connect to your Kubernetes cluster. Basic Commands : kubectl get pods : List all pods in the current namespace. kubectl get services -A : List all services across all namespaces. kubectl describe pod <pod-name> : Get detailed information about a specific pod. Applying Configurations : kubectl apply -f <filename>.yaml : Create or update resources defined in a YAML file. Architecture (Mermaid diagram to be added) API Interaction Kubectl interacts with the Kubernetes API server to manage cluster resources. It constructs API requests based on user commands and flags, and then processes the API responses. Output Formatting Kubectl supports various output formats, including: Human-readable (default) : Formatted for easy understanding. JSON : kubectl get pods -o json YAML : kubectl get pods -o yaml Wide : kubectl get pods -o wide (shows additional columns like IP and Node) Further Reading Kubernetes Documentation: kubectl Overview Table of Contents Key Concepts Sub-components Getting Started Architecture API Interaction Output Formatting Further Reading","title":"Kubectl Overview"},{"location":"docs/kubectl/index.html#kubectl-overview","text":"The Kubernetes command-line interface, kubectl, is the primary tool for interacting with a Kubernetes cluster. It allows users to perform a wide range of operations, from deploying applications to inspecting and managing cluster resources.","title":"Kubectl Overview"},{"location":"docs/kubectl/index.html#key-concepts","text":"Commands : Kubectl operates through a system of commands, each designed for a specific task (e.g., get , create , delete , apply ). Resources : These are the objects within Kubernetes that kubectl manages, such as Pods, Services, Deployments, and ConfigMaps. Flags and Arguments : Commands can be modified with flags (e.g., -n <namespace> ) and arguments (e.g., <resource-type>/<resource-name> ) to refine their behavior. Configuration : Kubectl uses a configuration file (typically ~/.kube/config ) to manage connections to multiple clusters and authenticate.","title":"Key Concepts"},{"location":"docs/kubectl/index.html#sub-components","text":"The pkg/kubectl/ directory contains the core logic for the kubectl client. Key sub-components include: Command Package ( cmd/ ) : This package defines the structure and implementation of kubectl commands. It handles argument parsing, command execution, and output formatting. cmd/convert : A sub-package within cmd/ that provides functionality for converting resource configurations between different API versions. Core Logic : The main kubectl/ directory contains foundational logic for API interaction, client creation, and command execution flow.","title":"Sub-components"},{"location":"docs/kubectl/index.html#getting-started","text":"Installation : Ensure kubectl is installed and configured to connect to your Kubernetes cluster. Basic Commands : kubectl get pods : List all pods in the current namespace. kubectl get services -A : List all services across all namespaces. kubectl describe pod <pod-name> : Get detailed information about a specific pod. Applying Configurations : kubectl apply -f <filename>.yaml : Create or update resources defined in a YAML file.","title":"Getting Started"},{"location":"docs/kubectl/index.html#architecture","text":"(Mermaid diagram to be added)","title":"Architecture"},{"location":"docs/kubectl/index.html#api-interaction","text":"Kubectl interacts with the Kubernetes API server to manage cluster resources. It constructs API requests based on user commands and flags, and then processes the API responses.","title":"API Interaction"},{"location":"docs/kubectl/index.html#output-formatting","text":"Kubectl supports various output formats, including: Human-readable (default) : Formatted for easy understanding. JSON : kubectl get pods -o json YAML : kubectl get pods -o yaml Wide : kubectl get pods -o wide (shows additional columns like IP and Node)","title":"Output Formatting"},{"location":"docs/kubectl/index.html#further-reading","text":"Kubernetes Documentation: kubectl Overview","title":"Further Reading"},{"location":"docs/kubectl/index.html#table-of-contents","text":"Key Concepts Sub-components Getting Started Architecture API Interaction Output Formatting Further Reading","title":"Table of Contents"},{"location":"docs/kubectl/api_reference.html","text":"Kubectl API Reference Kubectl itself is a command-line tool and does not expose a traditional API in the same way a server does. However, it interacts extensively with the Kubernetes API server. This document outlines the primary user-facing commands and their typical interactions with the API. Core Commands and API Interactions Kubectl commands generally map to Kubernetes API operations (GET, POST, PUT, DELETE) on specific resources. Kubectl Command HTTP Method API Path (Example) Description get <resource> GET /api/v1/pods Retrieves one or more resources. create -f <file> POST /api/v1/namespaces/<namespace>/pods Creates a new resource defined in a file. apply -f <file> POST/PUT /api/v1/namespaces/<namespace>/pods Creates or updates resources based on a declarative configuration. delete <resource>/<name> DELETE /api/v1/namespaces/<namespace>/pods/<name> Deletes a specific resource. describe <resource>/<name> GET /api/v1/namespaces/<namespace>/pods/<name> Retrieves detailed information about a specific resource. logs <pod-name> GET /api/v1/namespaces/<namespace>/pods/<name>/log Fetches logs from a pod. exec <pod-name> -- <command> POST /api/v1/namespaces/<namespace>/pods/<name>/exec Executes a command within a container in a pod. version GET /version Retrieves version information for the client and the API server. convert N/A N/A Converts resource definitions between API versions (client-side operation). Command Structure and Flags Most kubectl commands follow the pattern: kubectl <command> <resource-type> [resource-name] [flags] Common Flags: -n , --namespace <namespace> : Specify the namespace for the operation. -o , --output <format> : Specify the output format (e.g., json , yaml , wide , name , custom-columns ). -A , --all-namespaces : List or operate on resources in all namespaces. -l , --selector <label-selector> : Filter resources by label selectors. --field-selector <field-selector> : Filter resources by field selectors. -f , --filename <file> : Read resources from a file. Code Examples Getting Pods in JSON format : bash kubectl get pods -n default -o json This command retrieves all pods in the default namespace and outputs them in JSON format, which is then sent back by the Kubernetes API server. Applying a Deployment from a YAML file : bash kubectl apply -f my-deployment.yaml This command reads the my-deployment.yaml file. Kubectl parses the YAML, determines the resource type (e.g., Deployment), and sends a POST or PUT request to the appropriate API endpoint (e.g., /apis/apps/v1/namespaces/<namespace>/deployments ) to create or update the resource on the server. Executing a command in a pod : bash kubectl exec my-pod -- ls -l /app This command initiates a request to the API server to execute the ls -l /app command within the my-pod . The API server facilitates this by instructing the Kubelet on the pod's node to run the command inside the container and stream the output back. Sub-components cmd/ : This package is responsible for defining and executing all kubectl commands. It includes sub-packages for specific command groups like cmd/convert . cmd/convert : Handles the logic for converting Kubernetes resource definitions between different API versions. This is primarily a client-side operation. Core Client Logic : This encompasses the packages responsible for: Reading and interpreting the kubeconfig file ( ~/.kube/config ). Establishing connections to the Kubernetes API server using rest.Config . Creating typed clientsets (e.g., clientset.CoreV1().Pods(...) ) for interacting with specific API groups and versions. Resources Kubernetes API Reference kubectl Cheat Sheet","title":"Kubectl API Reference"},{"location":"docs/kubectl/api_reference.html#kubectl-api-reference","text":"Kubectl itself is a command-line tool and does not expose a traditional API in the same way a server does. However, it interacts extensively with the Kubernetes API server. This document outlines the primary user-facing commands and their typical interactions with the API.","title":"Kubectl API Reference"},{"location":"docs/kubectl/api_reference.html#core-commands-and-api-interactions","text":"Kubectl commands generally map to Kubernetes API operations (GET, POST, PUT, DELETE) on specific resources. Kubectl Command HTTP Method API Path (Example) Description get <resource> GET /api/v1/pods Retrieves one or more resources. create -f <file> POST /api/v1/namespaces/<namespace>/pods Creates a new resource defined in a file. apply -f <file> POST/PUT /api/v1/namespaces/<namespace>/pods Creates or updates resources based on a declarative configuration. delete <resource>/<name> DELETE /api/v1/namespaces/<namespace>/pods/<name> Deletes a specific resource. describe <resource>/<name> GET /api/v1/namespaces/<namespace>/pods/<name> Retrieves detailed information about a specific resource. logs <pod-name> GET /api/v1/namespaces/<namespace>/pods/<name>/log Fetches logs from a pod. exec <pod-name> -- <command> POST /api/v1/namespaces/<namespace>/pods/<name>/exec Executes a command within a container in a pod. version GET /version Retrieves version information for the client and the API server. convert N/A N/A Converts resource definitions between API versions (client-side operation).","title":"Core Commands and API Interactions"},{"location":"docs/kubectl/api_reference.html#command-structure-and-flags","text":"Most kubectl commands follow the pattern: kubectl <command> <resource-type> [resource-name] [flags]","title":"Command Structure and Flags"},{"location":"docs/kubectl/api_reference.html#common-flags","text":"-n , --namespace <namespace> : Specify the namespace for the operation. -o , --output <format> : Specify the output format (e.g., json , yaml , wide , name , custom-columns ). -A , --all-namespaces : List or operate on resources in all namespaces. -l , --selector <label-selector> : Filter resources by label selectors. --field-selector <field-selector> : Filter resources by field selectors. -f , --filename <file> : Read resources from a file.","title":"Common Flags:"},{"location":"docs/kubectl/api_reference.html#code-examples","text":"Getting Pods in JSON format : bash kubectl get pods -n default -o json This command retrieves all pods in the default namespace and outputs them in JSON format, which is then sent back by the Kubernetes API server. Applying a Deployment from a YAML file : bash kubectl apply -f my-deployment.yaml This command reads the my-deployment.yaml file. Kubectl parses the YAML, determines the resource type (e.g., Deployment), and sends a POST or PUT request to the appropriate API endpoint (e.g., /apis/apps/v1/namespaces/<namespace>/deployments ) to create or update the resource on the server. Executing a command in a pod : bash kubectl exec my-pod -- ls -l /app This command initiates a request to the API server to execute the ls -l /app command within the my-pod . The API server facilitates this by instructing the Kubelet on the pod's node to run the command inside the container and stream the output back.","title":"Code Examples"},{"location":"docs/kubectl/api_reference.html#sub-components","text":"cmd/ : This package is responsible for defining and executing all kubectl commands. It includes sub-packages for specific command groups like cmd/convert . cmd/convert : Handles the logic for converting Kubernetes resource definitions between different API versions. This is primarily a client-side operation. Core Client Logic : This encompasses the packages responsible for: Reading and interpreting the kubeconfig file ( ~/.kube/config ). Establishing connections to the Kubernetes API server using rest.Config . Creating typed clientsets (e.g., clientset.CoreV1().Pods(...) ) for interacting with specific API groups and versions.","title":"Sub-components"},{"location":"docs/kubectl/api_reference.html#resources","text":"Kubernetes API Reference kubectl Cheat Sheet","title":"Resources"},{"location":"docs/kubectl/architecture.html","text":"Kubectl Architecture System Context Kubectl is the client-side application used to interact with the Kubernetes API server. It sits outside the cluster and sends requests to the API server to manage cluster resources. graph LR User -- Interacts with --> Kubectl Kubectl -- Sends API requests --> APIServer(Kubernetes API Server) APIServer -- Manages --> etcd APIServer -- Schedules --> Kubelet(Kubelet on Nodes) Kubelet -- Manages --> Pods Internal Architecture The core of kubectl resides in the pkg/kubectl/ directory. Key components include: Command Package ( pkg/kubectl/cmd/ ) : Parses user commands and flags. Orchestrates the execution of kubectl operations. Handles different command types (e.g., get , create , apply , delete , convert ). cmd/convert : Specifically handles the conversion of Kubernetes resources between different API versions. It ensures that configurations are compatible with the target API server. pkg/kubectl/cmd/convert/convert.go : Implements the core conversion logic. pkg/kubectl/cmd/convert/import_known_versions.go : Manages the known API versions for conversion. Core Client Logic : Responsible for creating REST clients that communicate with the API server. Handles authentication and configuration loading (from ~/.kube/config ). Processes API responses and formats them for the user. graph TD A[User Command] --> B{Command Parser}; B -- Parsed Command --> C{Command Executor}; C -- API Request --> D[REST Client]; D -- Sends to --> APIServer(Kubernetes API Server); APIServer -- Response --> D; D -- Processed Response --> C; C -- Formatted Output --> E[User Output]; subgraph pkg/kubectl/cmd B C end subgraph Core Client Logic D end %% Specific handler for convert command C -- Convert Command --> ConvertCmd[Convert Executor]; ConvertCmd --> ConvertLogic[Convert Logic]; ConvertLogic --> APIServer; subgraph pkg/kubectl/cmd/convert ConvertCmd ConvertLogic end Data Flow Example (kubectl get pods) User Input : kubectl get pods -n kube-system Command Parsing : The command and flags are parsed. The get command and the pods resource are identified, along with the kube-system namespace. API Client : A REST client is configured using the ~/.kube/config file. API Request : An HTTP GET request is sent to the API server endpoint for pods in the kube-system namespace (e.g., /api/v1/namespaces/kube-system/pods ). API Server : The API server processes the request, retrieves the pod information from etcd, and returns a JSON or YAML response. Response Processing : Kubectl receives the response, deserializes it, and formats it for display. Output : The list of pods is displayed to the user. Sub-components cmd/ : Contains implementations for all kubectl commands. cmd/convert/ : Handles resource version conversion. convert.go : Main implementation for the convert command. import_known_versions.go : Utility for managing known API versions. Core Client : Handles overall client setup, configuration, and API interaction. Code Examples Creating a REST client (simplified) : ```go import ( \"k8s.io/client-go/rest\" // ... other imports ) // Load configuration from ~/.kube/config config, err := clientcmd.NewNonInteractiveDeferredLoadingClientConfig( clientcmd.NewDefaultClientConfigLoadingRules(), &clientcmd.ConfigOverrides{}, ).ClientConfig() if err != nil { // handle error } // Create the clientset clientset, err := kubernetes.NewForConfig(config) if err != nil { // handle error } // Now you can use clientset to interact with the API pods, err := clientset.CoreV1().Pods(\"default\").List(context.TODO(), metav1.ListOptions{}) ``` cmd/convert/convert.go snippet (conceptual) : ```go // Function to convert resources func RunConvert(cmd *cobra.Command, args []string) error { // ... parse flags for input/output versions and resources ... // Read resource definition (e.g., from file or stdin) // ... // Use API machinery to convert the object to the target version // convertedObject, err := scheme.Convert(originalObject, targetVersion, \"\") // ... // Print the converted object // ... return nil } ``` Command definition example ( cmd/get.go - conceptual) : go var getCmd = &cobra.Command{ Use: \"get RESOURCE [NAME | -l label] [flags]\", Short: \"Display resources\", Aliases: []string{\"g\", \"sp\"}, Run: func(cmd *cobra.Command, args []string) { // ... logic to fetch and display resources ... }, }","title":"Kubectl Architecture"},{"location":"docs/kubectl/architecture.html#kubectl-architecture","text":"","title":"Kubectl Architecture"},{"location":"docs/kubectl/architecture.html#system-context","text":"Kubectl is the client-side application used to interact with the Kubernetes API server. It sits outside the cluster and sends requests to the API server to manage cluster resources. graph LR User -- Interacts with --> Kubectl Kubectl -- Sends API requests --> APIServer(Kubernetes API Server) APIServer -- Manages --> etcd APIServer -- Schedules --> Kubelet(Kubelet on Nodes) Kubelet -- Manages --> Pods","title":"System Context"},{"location":"docs/kubectl/architecture.html#internal-architecture","text":"The core of kubectl resides in the pkg/kubectl/ directory. Key components include: Command Package ( pkg/kubectl/cmd/ ) : Parses user commands and flags. Orchestrates the execution of kubectl operations. Handles different command types (e.g., get , create , apply , delete , convert ). cmd/convert : Specifically handles the conversion of Kubernetes resources between different API versions. It ensures that configurations are compatible with the target API server. pkg/kubectl/cmd/convert/convert.go : Implements the core conversion logic. pkg/kubectl/cmd/convert/import_known_versions.go : Manages the known API versions for conversion. Core Client Logic : Responsible for creating REST clients that communicate with the API server. Handles authentication and configuration loading (from ~/.kube/config ). Processes API responses and formats them for the user. graph TD A[User Command] --> B{Command Parser}; B -- Parsed Command --> C{Command Executor}; C -- API Request --> D[REST Client]; D -- Sends to --> APIServer(Kubernetes API Server); APIServer -- Response --> D; D -- Processed Response --> C; C -- Formatted Output --> E[User Output]; subgraph pkg/kubectl/cmd B C end subgraph Core Client Logic D end %% Specific handler for convert command C -- Convert Command --> ConvertCmd[Convert Executor]; ConvertCmd --> ConvertLogic[Convert Logic]; ConvertLogic --> APIServer; subgraph pkg/kubectl/cmd/convert ConvertCmd ConvertLogic end","title":"Internal Architecture"},{"location":"docs/kubectl/architecture.html#data-flow-example-kubectl-get-pods","text":"User Input : kubectl get pods -n kube-system Command Parsing : The command and flags are parsed. The get command and the pods resource are identified, along with the kube-system namespace. API Client : A REST client is configured using the ~/.kube/config file. API Request : An HTTP GET request is sent to the API server endpoint for pods in the kube-system namespace (e.g., /api/v1/namespaces/kube-system/pods ). API Server : The API server processes the request, retrieves the pod information from etcd, and returns a JSON or YAML response. Response Processing : Kubectl receives the response, deserializes it, and formats it for display. Output : The list of pods is displayed to the user.","title":"Data Flow Example (kubectl get pods)"},{"location":"docs/kubectl/architecture.html#sub-components","text":"cmd/ : Contains implementations for all kubectl commands. cmd/convert/ : Handles resource version conversion. convert.go : Main implementation for the convert command. import_known_versions.go : Utility for managing known API versions. Core Client : Handles overall client setup, configuration, and API interaction.","title":"Sub-components"},{"location":"docs/kubectl/architecture.html#code-examples","text":"Creating a REST client (simplified) : ```go import ( \"k8s.io/client-go/rest\" // ... other imports ) // Load configuration from ~/.kube/config config, err := clientcmd.NewNonInteractiveDeferredLoadingClientConfig( clientcmd.NewDefaultClientConfigLoadingRules(), &clientcmd.ConfigOverrides{}, ).ClientConfig() if err != nil { // handle error } // Create the clientset clientset, err := kubernetes.NewForConfig(config) if err != nil { // handle error } // Now you can use clientset to interact with the API pods, err := clientset.CoreV1().Pods(\"default\").List(context.TODO(), metav1.ListOptions{}) ``` cmd/convert/convert.go snippet (conceptual) : ```go // Function to convert resources func RunConvert(cmd *cobra.Command, args []string) error { // ... parse flags for input/output versions and resources ... // Read resource definition (e.g., from file or stdin) // ... // Use API machinery to convert the object to the target version // convertedObject, err := scheme.Convert(originalObject, targetVersion, \"\") // ... // Print the converted object // ... return nil } ``` Command definition example ( cmd/get.go - conceptual) : go var getCmd = &cobra.Command{ Use: \"get RESOURCE [NAME | -l label] [flags]\", Short: \"Display resources\", Aliases: []string{\"g\", \"sp\"}, Run: func(cmd *cobra.Command, args []string) { // ... logic to fetch and display resources ... }, }","title":"Code Examples"},{"location":"docs/kubectl/configuration.html","text":"Kubectl Configuration Kubectl's behavior and target cluster are primarily controlled through a configuration file, commonly known as kubeconfig . Kubeconfig File ( ~/.kube/config ) The kubeconfig file stores information about clusters, users, and contexts. Clusters : Define the connection details for your Kubernetes clusters, including the API server endpoint and certificate authority data. Users : Specify authentication credentials, such as client certificates, tokens, or basic authentication details. Contexts : Link a cluster, a user, and an optional namespace to define the operating environment. Example kubeconfig structure: apiVersion: v1 kind: Config clusters: - name: my-cluster cluster: server: https://192.168.1.100:6443 certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0F... users: - name: my-user user: client-certificate-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0F... client-key-data: LS0tLS1CRUdJTiBFVlJ... contexts: - name: my-context context: cluster: my-cluster user: my-user namespace: default # Optional: defaults to 'default' if not specified current-context: my-context Environment Variables Kubectl respects the following environment variables for configuration: KUBECONFIG : A colon-separated list of file paths to use as the kubeconfig file. If not set, it defaults to ~/.kube/config . KUBE_EDITOR : The editor command to use for interactive-style commands (e.g., kubectl edit ). KUBECTL_COMMAND_HEADERS : If set to true , kubectl will print HTTP headers for all requests it makes to the API server. Command-line Flags for Configuration Many configuration options can be overridden or specified directly via command-line flags: --kubeconfig <path> : Path to the kubeconfig file. --context <context-name> : The name of the context to use. --cluster <cluster-name> : The name of the cluster to use. --user <user-name> : The name of the user to use. --namespace <namespace-name> : The namespace to use for the request. Sub-components The configuration handling is primarily managed by the Kubernetes client-go library, specifically within packages that deal with client configuration and REST clients. Client Configuration Loading : Logic exists within pkg/kubectl/ (and underlying client-go libraries) to load configurations from: Default locations ( ~/.kube/config ). Paths specified by the KUBECONFIG environment variable. Command-line flags ( --kubeconfig , --context , etc.). REST Client : Once a configuration ( rest.Config ) is loaded, it's used to create a rest.RESTClient or a typed kubernetes.Clientset , which then performs the actual API interactions. Code Examples Loading kubeconfig (conceptual Go snippet) : ```go import ( \"k8s.io/client-go/tools/clientcmd\" // ... other imports ) // Load default config rules clientConfigLoadingRules := clientcmd.NewDefaultClientConfigLoadingRules() // Allow override via KUBECONFIG env var or --kubeconfig flag clientConfig := clientcmd.NewNonInteractiveDeferredLoadingClientConfig( clientConfigLoadingRules, &clientcmd.ConfigOverrides{}, // Override with context, namespace flags if needed ) // Get REST configuration restConfig, err := clientConfig.ClientConfig() if err != nil { // Handle error } // Use restConfig to create a clientset or RESTClient ``` Setting the current context via command line : bash kubectl config use-context my-other-context This command modifies the current-context field in your active kubeconfig file, ensuring subsequent kubectl commands use the specified context. Viewing the current configuration : bash kubectl config view This command displays the currently effective kubeconfig, merging settings from the file and environment variables. Listing available contexts : bash kubectl config get-contexts This command lists all contexts defined in your kubeconfig file, indicating the current one with an asterisk.","title":"Kubectl Configuration"},{"location":"docs/kubectl/configuration.html#kubectl-configuration","text":"Kubectl's behavior and target cluster are primarily controlled through a configuration file, commonly known as kubeconfig .","title":"Kubectl Configuration"},{"location":"docs/kubectl/configuration.html#kubeconfig-file-kubeconfig","text":"The kubeconfig file stores information about clusters, users, and contexts. Clusters : Define the connection details for your Kubernetes clusters, including the API server endpoint and certificate authority data. Users : Specify authentication credentials, such as client certificates, tokens, or basic authentication details. Contexts : Link a cluster, a user, and an optional namespace to define the operating environment.","title":"Kubeconfig File (~/.kube/config)"},{"location":"docs/kubectl/configuration.html#example-kubeconfig-structure","text":"apiVersion: v1 kind: Config clusters: - name: my-cluster cluster: server: https://192.168.1.100:6443 certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0F... users: - name: my-user user: client-certificate-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0F... client-key-data: LS0tLS1CRUdJTiBFVlJ... contexts: - name: my-context context: cluster: my-cluster user: my-user namespace: default # Optional: defaults to 'default' if not specified current-context: my-context","title":"Example kubeconfig structure:"},{"location":"docs/kubectl/configuration.html#environment-variables","text":"Kubectl respects the following environment variables for configuration: KUBECONFIG : A colon-separated list of file paths to use as the kubeconfig file. If not set, it defaults to ~/.kube/config . KUBE_EDITOR : The editor command to use for interactive-style commands (e.g., kubectl edit ). KUBECTL_COMMAND_HEADERS : If set to true , kubectl will print HTTP headers for all requests it makes to the API server.","title":"Environment Variables"},{"location":"docs/kubectl/configuration.html#command-line-flags-for-configuration","text":"Many configuration options can be overridden or specified directly via command-line flags: --kubeconfig <path> : Path to the kubeconfig file. --context <context-name> : The name of the context to use. --cluster <cluster-name> : The name of the cluster to use. --user <user-name> : The name of the user to use. --namespace <namespace-name> : The namespace to use for the request.","title":"Command-line Flags for Configuration"},{"location":"docs/kubectl/configuration.html#sub-components","text":"The configuration handling is primarily managed by the Kubernetes client-go library, specifically within packages that deal with client configuration and REST clients. Client Configuration Loading : Logic exists within pkg/kubectl/ (and underlying client-go libraries) to load configurations from: Default locations ( ~/.kube/config ). Paths specified by the KUBECONFIG environment variable. Command-line flags ( --kubeconfig , --context , etc.). REST Client : Once a configuration ( rest.Config ) is loaded, it's used to create a rest.RESTClient or a typed kubernetes.Clientset , which then performs the actual API interactions.","title":"Sub-components"},{"location":"docs/kubectl/configuration.html#code-examples","text":"Loading kubeconfig (conceptual Go snippet) : ```go import ( \"k8s.io/client-go/tools/clientcmd\" // ... other imports ) // Load default config rules clientConfigLoadingRules := clientcmd.NewDefaultClientConfigLoadingRules() // Allow override via KUBECONFIG env var or --kubeconfig flag clientConfig := clientcmd.NewNonInteractiveDeferredLoadingClientConfig( clientConfigLoadingRules, &clientcmd.ConfigOverrides{}, // Override with context, namespace flags if needed ) // Get REST configuration restConfig, err := clientConfig.ClientConfig() if err != nil { // Handle error } // Use restConfig to create a clientset or RESTClient ``` Setting the current context via command line : bash kubectl config use-context my-other-context This command modifies the current-context field in your active kubeconfig file, ensuring subsequent kubectl commands use the specified context. Viewing the current configuration : bash kubectl config view This command displays the currently effective kubeconfig, merging settings from the file and environment variables. Listing available contexts : bash kubectl config get-contexts This command lists all contexts defined in your kubeconfig file, indicating the current one with an asterisk.","title":"Code Examples"}]}